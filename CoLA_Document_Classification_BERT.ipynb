{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLA_Document_Classification_BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yi-juho/-K_MHAS-_HuggingFace_dataset_MultiBERT_shared/blob/main/CoLA_Document_Classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Classification BERT [CoLA]\n"
      ],
      "metadata": {
        "id": "SGcOaw5P769d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVB2VLaY9PK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2a189e-f13a-4bab-aee4-e52a5d8b699e"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1yMW5ojZ43o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20941750-8a76-4362-cef0-201637bd005f"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdjsOX9naA-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e8ca15-3548-415d-c7da-f94fe962e717"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkMgQEmaPEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d2b226-e263-44eb-e9d1-781e67f2abbf"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LflYZlN3aYSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d87a2c-1c2c-4657-c6c1-4a4cc15487d8"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25uSDxqadmc"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_j9f6Qjami9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "ec0c97f3-e33b-4a25-c0a8-199d39976b13"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "664             bc01      1         NaN   \n",
              "7518           sks13      1         NaN   \n",
              "7897            ad03      0           *   \n",
              "1734            r-67      1         NaN   \n",
              "6358            d_98      1         NaN   \n",
              "8202            ad03      0           *   \n",
              "4253            ks08      0           *   \n",
              "3212            l-93      1         NaN   \n",
              "3320            l-93      1         NaN   \n",
              "2786            l-93      1         NaN   \n",
              "\n",
              "                                               sentence  \n",
              "664         Mary considers John a fool and Bill a wimp.  \n",
              "7518  Our rabbit and the neighbor's cat like each ot...  \n",
              "7897                      Moya played football with her  \n",
              "1734  They said that Tom would pay up, and pay up he...  \n",
              "6358                        Any student must work hard.  \n",
              "8202  That that the world is round is obvious upset ...  \n",
              "4253   Sally wishes that everyone would praise herself.  \n",
              "3212        A squeaking door announced John's presence.  \n",
              "3320                                 The cattle herded.  \n",
              "2786                                 Steve pelted Anna.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-283a3b24-f253-483b-88a3-8f2927f1d242\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mary considers John a fool and Bill a wimp.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7518</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our rabbit and the neighbor's cat like each ot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7897</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Moya played football with her</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They said that Tom would pay up, and pay up he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6358</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Any student must work hard.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>That that the world is round is obvious upset ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4253</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Sally wishes that everyone would praise herself.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3212</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A squeaking door announced John's presence.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3320</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The cattle herded.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2786</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Steve pelted Anna.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-283a3b24-f253-483b-88a3-8f2927f1d242')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-283a3b24-f253-483b-88a3-8f2927f1d242 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-283a3b24-f253-483b-88a3-8f2927f1d242');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f1c3a26-9e2c-4467-8ccd-2bbb94d09b43\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f1c3a26-9e2c-4467-8ccd-2bbb94d09b43')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f1c3a26-9e2c-4467-8ccd-2bbb94d09b43 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"bc01\",\n          \"sks13\",\n          \"ks08\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The cattle herded.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGFvVN-aqY5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4e2a7913-0a2f-4f31-929a-d079c4499618"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  label\n",
              "4722                  Six inches were grown by the boy.      0\n",
              "8025                                   The fig chuckled      0\n",
              "7472  Henri told Sue in the drawer that Bill put socks.      0\n",
              "5997                         Calvin ate not his dinner.      0\n",
              "659                         John gave Bill of the book.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-237e331a-c684-4dc6-9a96-a3d8f3b81bfd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4722</th>\n",
              "      <td>Six inches were grown by the boy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8025</th>\n",
              "      <td>The fig chuckled</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7472</th>\n",
              "      <td>Henri told Sue in the drawer that Bill put socks.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>Calvin ate not his dinner.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>John gave Bill of the book.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-237e331a-c684-4dc6-9a96-a3d8f3b81bfd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-237e331a-c684-4dc6-9a96-a3d8f3b81bfd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-237e331a-c684-4dc6-9a96-a3d8f3b81bfd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f181a5c6-a142-4586-a394-0a087825328c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f181a5c6-a142-4586-a394-0a087825328c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f181a5c6-a142-4586-a394-0a087825328c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The fig chuckled\",\n          \"John gave Bill of the book.\",\n          \"Henri told Sue in the drawer that Bill put socks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxfCoezXau5n"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Kj3sWgaxan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2346a93d-23fa-42cc-d29a-c69540f92cf7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8T8JMCma1OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ad29e4-16e9-48f4-edff-dd1d76ba318f"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tm_8o_a54i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bfce4a-5b3a-4bcf-fcb5-f0c27a7d3de3"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V80hCa9CbEOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7431db1-f954-413c-9019-484f99bf4042"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU5-MHxHbJrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc995f1d-5460-4209-bbdd-e632d8c3247a"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 10\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 10 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6e3Oe-JbNJM"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcSGMwoFbRAM"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49qrloHrbTmE"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TFSrS7bXI-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0jiGwk3batl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14db1d9e-6091-49bb-9906-68a312939edc"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoerOh8qbegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d8c05c-6255-4042-9712-7c163d535640"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DfOPFdbkI3"
      },
      "source": [
        "#Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hliUyfZsboiy"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXsLGIi6bvl-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcJnBTsbyoJ"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2YjFs8gb1hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a38e52-a522-46ae-fa3d-08e550f0b94b"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:21.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:26.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:31.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.76\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:04.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:08.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:19.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:23.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:04.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:08.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:19.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:23.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:04.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:08.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:19.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:23.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdFfKyoLb7Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "d7dd19cc-dae6-4e00-ff3b-fdb690d90dc8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-N3fVYKcApy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa42f09f-65b1-4616-91ae-ebbd1a9e5c67"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNF30U8FcITz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd47f19-5e49-45d8-f028-3a913134edb9"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTk5LY4cLfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3bf97c-2b29-4642-aa17-b4487a4cf2f2"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy_SFwR8cQ58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6f5f76-d549-4c4e-ecfb-fa004b2c0a8e"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-MCNw0cTCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db815156-c32c-43de-dad0-d7cad8f99b5d"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(0.21713222235566895),\n",
              " 0.0,\n",
              " np.float64(0.34500484066310094),\n",
              " np.float64(0.2757127976394358),\n",
              " np.float64(0.3567530340063379),\n",
              " np.float64(0.8150678894028793),\n",
              " np.float64(0.4879500364742666),\n",
              " 0.0,\n",
              " np.float64(0.7453559924999299),\n",
              " np.float64(0.7177405625652734),\n",
              " np.float64(0.8459051693633014),\n",
              " np.float64(0.5673665146135802),\n",
              " np.float64(0.6979824404521128),\n",
              " np.float64(0.5447047794019222),\n",
              " np.float64(0.2342878320018382),\n",
              " np.float64(0.4252964776724258),\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbT-rwHocVqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e088d26b-e327-47ce-ad62-20b53f4efcdc"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론 코드"
      ],
      "metadata": {
        "id": "xd5BzXwU8HM1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5avgvjcYXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ea3f88-056d-4825-e3d3-c995cb992d0e"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# 1. 모델 및 토크나이저 로드 (예: 'bert-base-uncased' 사용)\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.cuda()               # GPU로 이동\n",
        "model.eval()               # 평가 모드로 전환\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "# 2. 추론할 영어 문장 예시\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "\n",
        "# 3. 문장 토큰화 및 인코딩\n",
        "inputs = tokenizer.encode_plus(\n",
        "    sentence,\n",
        "    add_special_tokens=True,      # [CLS]와 [SEP] 토큰 추가\n",
        "    max_length=10,                # 최대 길이 설정\n",
        "    padding=\"max_length\",         # 최대 길이에 맞춰 패딩\n",
        "    truncation=True,              # 길면 잘라냄\n",
        "    return_tensors=\"pt\"           # 파이토치 텐서 반환\n",
        ")\n",
        "\n",
        "# 4. 텐서를 GPU로 이동\n",
        "inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "# 5. 추론 (forward pass)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs[0]  # 모델의 출력은 튜플이며, 첫 번째 요소가 로짓입니다.\n",
        "\n",
        "# 6. Softmax 적용 (선택 사항) 및 예측 클래스 결정\n",
        "probs = torch.softmax(logits, dim=1)\n",
        "predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# 7. 결과 출력\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", predicted_class.item())\n",
        "print(\"예측 확률:\", probs)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 1\n",
            "예측 확률: tensor([[0.3839, 0.6161]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    입력된 문장에 대해 학습된 모델을 사용하여 예측을 수행합니다.\n",
        "    Returns:\n",
        "        predicted_class: 예측된 클래스 (예: 0 또는 1)\n",
        "        probs: 각 클래스의 확률 (numpy 배열)\n",
        "    \"\"\"\n",
        "    # 문장을 토큰화하고 encode_plus를 통해 [CLS], [SEP] 토큰을 추가하며, 패딩/자르기 적용\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,      # [CLS]와 [SEP] 추가\n",
        "        max_length=max_length,        # 최대 길이\n",
        "        padding=\"max_length\",         # 최대 길이에 맞게 패딩\n",
        "        truncation=True,              # 길면 자르기\n",
        "        return_tensors=\"pt\"           # PyTorch 텐서 반환\n",
        "    )\n",
        "\n",
        "    # GPU로 텐서를 전송\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # 모델을 평가 모드로 전환하고, 추론 시에는 기울기를 계산하지 않도록 설정\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]  # 모델의 출력은 튜플 형태로, 첫 번째 요소가 logits입니다.\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 사용\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PITHYJKhPHD",
        "outputId": "e4be73f8-1241-4124-f9f7-a4b1838a8781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 1\n",
            "예측 확률: [[0.49480566 0.50519437]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cordic 코드"
      ],
      "metadata": {
        "id": "oK0EdSFZjeh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def top_1200_input(attention_scores):\n",
        "    \"\"\"\n",
        "    1200개의 attention_scores를 120개의 리스트로 나누어 각 리스트를 top 함수에 전달\n",
        "    결과는 원본 형태 (1, 12, 10, 10)로 반환\n",
        "    \"\"\"\n",
        "    if isinstance(attention_scores, torch.Tensor):\n",
        "        attention_scores = attention_scores.detach().cpu().numpy()  # ✅ detach() 추가\n",
        "\n",
        "    batch_size, num_heads, seq_length, _ = attention_scores.shape\n",
        "    result_arrays = np.zeros((batch_size, num_heads, seq_length, seq_length))\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        for row in range(seq_length):\n",
        "            for col in range(seq_length // 10):\n",
        "                input_values = attention_scores[0, head, row, col * 10:(col + 1) * 10]\n",
        "                result = top(*input_values)\n",
        "                result_arrays[0, head, row, col * 10:(col + 1) * 10] = result\n",
        "\n",
        "    # ✅ numpy -> torch 변환할 때 `.to(device)` 추가\n",
        "    return torch.tensor(result_arrays, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def float_to_fixed_point(value, int_bits=7, frac_bits=13):\n",
        "    \"\"\"\n",
        "    실수를 32비트 Fixed-Point 바이너리로 변환하는 함수.\n",
        "    - 정수부는 7비트 (Signed, 2의 보수)\n",
        "    - 소수부는 13비트 (항상 양수)\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ PyTorch Tensor 처리\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        value = value.detach().cpu().numpy()\n",
        "\n",
        "    # ✅ NaN 또는 Inf 값 체크 후 예외 처리\n",
        "    if np.isnan(value) or np.isinf(value):\n",
        "        raise ValueError(f\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\")\n",
        "\n",
        "    # ✅ **최대/최소 값 제한 (7비트 표현 범위)**\n",
        "    value = max(min(value, 63), -64)\n",
        "\n",
        "    # ✅ 정수부와 소수부 분리\n",
        "    int_part = np.floor(value).astype(int)\n",
        "    frac_part = abs(value - int_part)  # 소수 부분\n",
        "\n",
        "    # ✅ 2의 보수 변환 (음수 처리)\n",
        "    if int_part < 0:\n",
        "        int_part = (1 << int_bits) + int_part\n",
        "\n",
        "    int_binary = format(int_part, f'0{int_bits}b')\n",
        "\n",
        "    # ✅ 12비트 0 추가 (BERT 출력 형식 유지)\n",
        "    int_binary = \"0\" * 12 + int_binary\n",
        "\n",
        "    # ✅ 소수부 변환 (13비트)\n",
        "    frac_binary = \"\"\n",
        "    for _ in range(frac_bits):\n",
        "        frac_part *= 2\n",
        "        if frac_part >= 1:\n",
        "            frac_binary += \"1\"\n",
        "            frac_part -= 1\n",
        "        else:\n",
        "            frac_binary += \"0\"\n",
        "\n",
        "    # ✅ 최종 32비트 바이너리 문자열 생성\n",
        "    binary_string = int_binary + frac_binary\n",
        "\n",
        "    # ✅ `binary_string`이 음수 값을 포함하는지 확인 후 처리\n",
        "    if \"-\" in binary_string:\n",
        "        raise ValueError(f\"[ERROR] 잘못된 바이너리 문자열 변환 감지: {binary_string}\")\n",
        "\n",
        "    # ✅ 20비트 정수 변환 (부호 처리)\n",
        "    fixed_binary = int(binary_string, 2)\n",
        "    if value < 0:\n",
        "        fixed_binary = (1 << 20) - fixed_binary  # 2의 보수 변환\n",
        "\n",
        "    lower_20_bits = fixed_binary & 0xFFFFF  # 20비트 마스킹\n",
        "\n",
        "    return lower_20_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def binary_to_decimal(binary_str):\n",
        "    # 16비트 중 앞 3비트는 정수부, 뒤 13비트는 소수부\n",
        "    int_part = int(binary_str[:3], 2)  # 정수부\n",
        "    frac_part = int(binary_str[3:], 2) / (2 ** 13)  # 소수부를 2^13으로 나눔\n",
        "    return int_part + frac_part  # 정수부와 소수부 합산\n",
        "\n",
        "def hex_to_binary(hex_str):\n",
        "    \"\"\"\n",
        "    16비트 헥스 값을 16비트 바이너리 문자열로 변환하는 함수\n",
        "\n",
        "    :param hex_str: 변환할 16비트 헥스 값 (예: \"1A2F\" 또는 0x1A2F)\n",
        "    :return: 16비트 바이너리 문자열 (예: \"0001101000101111\")\n",
        "    \"\"\"\n",
        "    # hex_str가 정수형일 경우 문자열로 변환\n",
        "    if isinstance(hex_str, int):\n",
        "        hex_str = format(hex_str, 'X')  # 16진수 대문자로 변환\n",
        "\n",
        "    # 16진수를 10진수로 변환 후, 16비트 이진수로 변환\n",
        "    binary_str = bin(int(hex_str, 16))[2:].zfill(16)\n",
        "    return binary_str\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "def top(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10) : #20bit data\n",
        "\n",
        "    data_11 = float_to_fixed_point(data_1)\n",
        "    data_22 = float_to_fixed_point(data_2)\n",
        "    data_33 = float_to_fixed_point(data_3)\n",
        "    data_44 = float_to_fixed_point(data_4)\n",
        "    data_55 = float_to_fixed_point(data_5)\n",
        "    data_66 = float_to_fixed_point(data_6)\n",
        "    data_77 = float_to_fixed_point(data_7)\n",
        "    data_88 = float_to_fixed_point(data_8)\n",
        "    data_99 = float_to_fixed_point(data_9)\n",
        "    data_1010 = float_to_fixed_point(data_10)\n",
        "\n",
        "    data_list = [data_11, data_22, data_33, data_44, data_55, data_66, data_77, data_88, data_99, data_1010]\n",
        "    #data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    int_list = [0] * 10 #exp_int input\n",
        "    exp_int_output = [0]*10 #exp_int output\n",
        "    fraction_list=[0]*10 #cordic_exu input\n",
        "    exp_fraction_output = [0]*10 #cordic_exu output\n",
        "    exp_whole=[0]*10 #cordic_exu +cordic_int\n",
        "    exp_trunc=[0]*10\n",
        "    exp_accum_input=[0]*10 # accumulator input  (exponential)\n",
        "    exp_accum_output=[0]*1\n",
        "    x_divider = [0] * 10 #cordic diu input\n",
        "    y_dividend = [0] * 10 #cordic diu input\n",
        "    data_out = [0] * 10\n",
        "    binary_values = [0] * 10\n",
        "    int_values = [0.0] * 10\n",
        "    for i in range(10):  # int part\n",
        "        int_list[i] = data_list[i] >> 13\n",
        "    exp_int_output[0], exp_int_output[1], exp_int_output[2], exp_int_output[3], exp_int_output[4], exp_int_output[5], exp_int_output[6], exp_int_output[7], exp_int_output[8], exp_int_output[9]=exp_int(int_list[0], int_list[1], int_list[2], int_list[3], int_list[4], int_list[5], int_list[6], int_list[7], int_list[8], int_list[9])\n",
        "\n",
        "    for i in range(10):  # fraction part\n",
        "        fraction_list[i] = (data_list[i] & 0x1FFF) << 1  # 하위 13비트를 가져오고, 뒤에 0을 추가\n",
        "    exp_fraction_output[0], exp_fraction_output[1], exp_fraction_output[2], exp_fraction_output[3], exp_fraction_output[4], exp_fraction_output[5], exp_fraction_output[6], exp_fraction_output[7], exp_fraction_output[8], exp_fraction_output[9] = cordic_exu(fraction_list[0], fraction_list[1], fraction_list[2], fraction_list[3], fraction_list[4], fraction_list[5], fraction_list[6], fraction_list[7], fraction_list[8], fraction_list[9])\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_whole[i] = exp_int_output[i] * exp_fraction_output[i]\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_trunc[i] = (exp_whole[i] >> 14) & 0xFFFFF  # 33~14 비트를 추출 (20비트)\n",
        "    for i in range(10):\n",
        "       exp_accum_input[i] = (exp_trunc[i] >> 4) & 0xFFFF\n",
        "\n",
        "    exp_accum_output[0] = cordic_accumulator(exp_accum_input[0],exp_accum_input[1],exp_accum_input[2],exp_accum_input[3],exp_accum_input[4],exp_accum_input[5],exp_accum_input[6],exp_accum_input[7],exp_accum_input[8],exp_accum_input[9])\n",
        "\n",
        "\n",
        "\n",
        "    # denominator_sf[i]에서 18~4 비트 추출 후, 앞에 0을 붙여서 x_divider에 할당\n",
        "    x_divider[0] = (exp_accum_output[0] >> 4) & 0x7FFF  # 18~4 비트를 추출 (15비트)\n",
        "\n",
        "    for i in range(10):\n",
        "    # numerator_sf[i]에서 15~4 비트 추출 후, 앞에 4개의 0을 붙여서 y_dividend에 할당\n",
        "        y_dividend[i] = (exp_accum_input[i] >> 4)  # 15~4 비트를 추출 (12비트)\n",
        "\n",
        "    data_out[0],data_out[1],data_out[2],data_out[3],data_out[4],data_out[5],data_out[6],data_out[7],data_out[8],data_out[9], = cordic_diu (x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],y_dividend[0],y_dividend[1],y_dividend[2],y_dividend[3],y_dividend[4],y_dividend[5],y_dividend[6],y_dividend[7],y_dividend[8],y_dividend[9])\n",
        "\n",
        "    for i in range(10):\n",
        "        binary_values[i] = hex_to_binary(data_out[i])\n",
        "       # print(binary_values[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        int_values[i] = binary_to_decimal(binary_values[i])\n",
        "\n",
        "    return int_values[0],int_values[1],int_values[2],int_values[3],int_values[4],int_values[5],int_values[6],int_values[7],int_values[8],int_values[9]\n",
        "\n",
        "def exp_int(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_to_int = [0]*10\n",
        "    int_output=[0]*10\n",
        "    for i in range(10):\n",
        "        # 이진수로 변환하여 signed_binary_to_int 함수에 전달\n",
        "        binary_str = bin(data_list[i])[2:].zfill(7)  # 20비트 이진수 문자열로 변환\n",
        "        data_to_int[i] = signed_binary_to_int(binary_str)\n",
        "\n",
        "    i_max = max(data_to_int)\n",
        "    offset=signed_binary_to_int(bin(0b0001011-i_max)[2:].zfill(7)) #offset\n",
        "\n",
        "    for i in range(10):\n",
        "        int_reg=data_to_int[i]+offset\n",
        "        #print(int_reg)\n",
        "        if int_reg==0 :\n",
        "            int_output[i]=0b0000_0000_0000_0001_00\n",
        "        elif int_reg==1 :\n",
        "            int_output[i]=0b0000_0000_0000_0010_11\n",
        "        elif int_reg==2 :\n",
        "            int_output[i]=0b0000_0000_0000_0111_10\n",
        "        elif int_reg==3 :\n",
        "            int_output[i]=0b0000_0000_0001_0100_00\n",
        "        elif int_reg==4 :\n",
        "            int_output[i]=0b0000_0000_0011_0110_10\n",
        "        elif int_reg==5 :\n",
        "            int_output[i]=0b0000_0000_1001_0100_10\n",
        "        elif int_reg==6 :\n",
        "            int_output[i]=0b0000_0001_1001_0011_10\n",
        "        elif int_reg==7 :\n",
        "            int_output[i]=0b0000_0100_0100_1000_11\n",
        "        elif int_reg==8 :\n",
        "            int_output[i]=0b0000_1011_1010_0101_00\n",
        "        elif int_reg==9 :\n",
        "            int_output[i]=0b0001_1111_1010_0111_00\n",
        "        elif int_reg==10 :\n",
        "            int_output[i]=0b0101_0110_0000_1010_10\n",
        "        elif int_reg==11 :\n",
        "            int_output[i]=0b1110_1001_1110_0010_01\n",
        "        else :\n",
        "            int_output[i]=0\n",
        "\n",
        "    return  int_output[0], int_output[1], int_output[2], int_output[3], int_output[4], int_output[5], int_output[6], int_output[7], int_output[8], int_output[9]\n",
        "\n",
        "\n",
        "def cordic_exu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_exu_pipe(data_list[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "def cordic_exu_pipe(z_in):\n",
        "    x_1, y_1, z_1=exp_unit(0b1001101010001111, 0, z_in, 0b0010001100100111,1)\n",
        "    x_2, y_2, z_2=exp_unit(x_1, y_1, z_1,0b0001000001011000,2)\n",
        "    x_3, y_3, z_3 = exp_unit(x_2, y_2, z_2, 0b0000100000001010, 3)\n",
        "    x_4, y_4, z_4 = exp_unit(x_3, y_3, z_3, 0b0000010000000001, 4)\n",
        "    x_5, y_5, z_5 = exp_unit(x_4, y_4, z_4, 0b0000001000000000, 5)\n",
        "    x_6, y_6, z_6 = exp_unit(x_5, y_5, z_5, 0b0000000100000000, 6)\n",
        "    x_7, y_7, z_7 = exp_unit(x_6, y_6, z_6, 0b0000000010000000, 7)\n",
        "    x_8, y_8, z_8 = exp_unit(x_7, y_7, z_7, 0b0000000001000000, 8)\n",
        "    x_9, y_9, z_9 = exp_unit(x_8, y_8, z_8, 0b0000000000100000, 9)\n",
        "    x_10, y_10, z_10 = exp_unit(x_9, y_9, z_9, 0b0000000000010000, 10)\n",
        "    x_11, y_11, z_11 = exp_unit(x_10, y_10, z_10, 0b0000000000001000, 11)\n",
        "    x_12, y_12, z_12 = exp_unit(x_11, y_11, z_11, 0b0000000000000100, 12)\n",
        "    x_13, y_13, z_13 = exp_unit(x_12, y_12, z_12, 0b0000000000000010, 13)\n",
        "    x_14, y_14, z_14 = exp_unit(x_13, y_13, z_13, 0b0000000000000010, 14)\n",
        "\n",
        "\n",
        "    exp_frac_reg=x_14+y_14\n",
        "    exp_frac = (exp_frac_reg >> 1) & 0xFFFF\n",
        "    #print(bin(exp_frac))\n",
        "    return exp_frac\n",
        "\n",
        "\n",
        "\n",
        "def exp_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "    x_cal = 0\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if z_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "        # 시계방향 (downward)\n",
        "        x_cal = x_in - (y_in >> pipe_num)\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "    else:\n",
        "        # 반시계방향 (upward)\n",
        "        x_cal = x_in + (y_in >> pipe_num)\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return x_cal, y_cal, z_cal\n",
        "\n",
        "\n",
        "def signed_binary_to_int(binary_str):\n",
        "    \"\"\"\n",
        "    2의 보수 표현된 이진수를 정수로 변환하는 함수.\n",
        "    \"\"\"\n",
        "    # ✅ \"0b\" 제거\n",
        "    binary_str = binary_str.replace(\"0b\", \"\")\n",
        "\n",
        "    # ✅ 이진수 길이 확인\n",
        "    n = len(binary_str)\n",
        "\n",
        "    # ✅ 부호 비트 확인 (최상위 비트가 1이면 음수)\n",
        "    if binary_str[0] == '1':\n",
        "        # 2의 보수 변환 (음수)\n",
        "        return int(binary_str, 2) - (1 << n)\n",
        "    else:\n",
        "        # 양수 변환\n",
        "        return int(binary_str, 2)\n",
        "\n",
        "\n",
        "def accumulator(sequence_in, exponential, state):\n",
        "    \"\"\"\n",
        "    입력값을 받아 sum1, sum2, denominator_sf 값을 업데이트하고 출력.\n",
        "    \"\"\"\n",
        "\n",
        "    # accumulator_SEL 토글\n",
        "    state[\"accumulator_SEL\"] = (state[\"accumulator_SEL\"] + sequence_in) & 1\n",
        "\n",
        "    # valid 시프트 레지스터 갱신\n",
        "    state[\"valid\"] = [sequence_in] + state[\"valid\"][:-1]\n",
        "\n",
        "    if state[\"accumulator_SEL\"]:  # accumulator 1 사용\n",
        "        state[\"sum1\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum2\"] = exponential  # sum2 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum1\"]  # denominator_sf 업데이트\n",
        "    else:  # accumulator 2 사용\n",
        "        state[\"sum2\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum1\"] = exponential  # sum1 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum2\"]  # denominator_sf 업데이트\n",
        "\n",
        "    sequence_out = state[\"valid\"][0]\n",
        "\n",
        "    return  state[\"denominator_sf\"]\n",
        "\n",
        "\n",
        "def cordic_accumulator (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    state_accumulator = {\n",
        "    \"denominator_sf\": 0,\n",
        "    \"accumulator_SEL\": 0,\n",
        "    \"valid\": [0] * 11,\n",
        "    \"sum1\": 0,\n",
        "    \"sum2\": 0\n",
        "}\n",
        "\n",
        "    exp_fraction[0]=  accumulator(1,data_list[0],state_accumulator)\n",
        "    exp_fraction[1]=  accumulator( 0 ,data_list[1],state_accumulator)\n",
        "    exp_fraction[2]=  accumulator( 0 ,data_list[2],state_accumulator)\n",
        "    exp_fraction[3]=  accumulator( 0 ,data_list[3],state_accumulator)\n",
        "    exp_fraction[4]=  accumulator( 0 ,data_list[4],state_accumulator)\n",
        "    exp_fraction[5]=  accumulator( 0 ,data_list[5],state_accumulator)\n",
        "    exp_fraction[6]=  accumulator( 0 ,data_list[6],state_accumulator)\n",
        "    exp_fraction[7]=  accumulator( 0 ,data_list[7],state_accumulator)\n",
        "    exp_fraction[8]=  accumulator( 0 ,data_list[8],state_accumulator)\n",
        "    exp_fraction[9]=  accumulator( 0 ,data_list[9],state_accumulator)\n",
        "    #print(hex(exp_fraction[9]))\n",
        "\n",
        "    return  exp_fraction[9]\n",
        "\n",
        "\n",
        "\n",
        "def fifo(exponential, state):\n",
        "\n",
        "\n",
        "        # FIFO 시프트 (FIFO_reg[i] = FIFO_reg[i-1])\n",
        "    state[\"FIFO_reg\"] = [exponential] + state[\"FIFO_reg\"][:-1]\n",
        "\n",
        "    numerator_sf = state[\"FIFO_reg\"][9]\n",
        "\n",
        "\n",
        "    return numerator_sf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def div_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if y_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "\n",
        "\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "    else:\n",
        "\n",
        "\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return  y_cal, z_cal\n",
        "def cordic_diu_pipe(x_in, y_in):\n",
        "    y_1, z_1 = div_unit(x_in, y_in, 0, 0b0010000000000000, 0)\n",
        "    y_2, z_2 = div_unit(x_in, y_1, z_1, 0b0001000000000000, 1)\n",
        "    y_3, z_3 = div_unit(x_in, y_2, z_2, 0b0000100000000000, 2)\n",
        "    y_4, z_4 = div_unit(x_in, y_3, z_3, 0b0000010000000000, 3)\n",
        "    y_5, z_5 = div_unit(x_in, y_4, z_4, 0b0000001000000000, 4)\n",
        "    y_6, z_6 = div_unit(x_in, y_5, z_5, 0b0000000100000000, 5)\n",
        "    y_7, z_7 = div_unit(x_in, y_6, z_6, 0b0000000010000000, 6)\n",
        "    y_8, z_8 = div_unit(x_in, y_7, z_7, 0b0000000001000000, 7)\n",
        "    y_9, z_9 = div_unit(x_in, y_8, z_8, 0b0000000000100000, 8)\n",
        "    y_10, z_10 = div_unit(x_in, y_9, z_9, 0b0000000000010000, 9)\n",
        "    y_11, z_11 = div_unit(x_in, y_10, z_10, 0b0000000000001000, 10)\n",
        "    y_12, z_12 = div_unit(x_in, y_11, z_11, 0b0000000000000100, 11)\n",
        "    y_13, z_13 = div_unit(x_in, y_12, z_12, 0b0000000000000010, 12)\n",
        "    y_14, z_14 = div_unit(x_in, y_13, z_13, 0b0000000000000001, 13)\n",
        "    divided_val = z_14  # Final z value representing atan(y/x)\n",
        "    return divided_val\n",
        "\n",
        "def cordic_diu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10,data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20):\n",
        "    data_list_x = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_list_y = [data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_diu_pipe(data_list_x[i],data_list_y[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "top(1.548621,\n",
        "2.154268,\n",
        "0.15548,\n",
        "1.358942,\n",
        "3.54985,\n",
        "2.1578712,\n",
        "-1.58952,\n",
        "-0.248623,\n",
        "1.6685123,\n",
        "-2.68856\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JuXVpD_Fjj7q",
        "outputId": "64c79585-e718-4f97-d2e0-7117fc3aa8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0528564453125,\n",
              " 0.0977783203125,\n",
              " 0.0128173828125,\n",
              " 0.0438232421875,\n",
              " 0.3963623046875,\n",
              " 0.0980224609375,\n",
              " 0.0552978515625,\n",
              " 0.0140380859375,\n",
              " 0.0599365234375,\n",
              " 0.1666259765625)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 수정 및 적용 코드"
      ],
      "metadata": {
        "id": "o19OXrVjjoaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "scMdkDy7tAqU",
        "outputId": "94a5a623-027d-4e54-beea-13ed430b7aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassificationModified(BertForSequenceClassification):\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Softmax 대신 CORDIC을 적용\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        return logits, probs\n"
      ],
      "metadata": {
        "id": "XySJ0nStsisx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # 여기서 원래 softmax를 적용하는 대신 CORDIC 기반 함수를 사용합니다.\n",
        "        # 예를 들어, top_1200_input(attention_scores)를 사용하여 softmax 결과를 근사합니다.\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        # 만약 반환된 결과가 numpy 형태라면, torch.tensor로 변환해주어야 합니다.\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 원래 출력은 (context_layer, attention_probs) 또는 (context_layer,)인데,\n",
        "        # 필요에 따라 raw attention scores도 반환하도록 할 수 있습니다.\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "# 만약 학습된 가중치를 로드해야 한다면 로드합니다.\n",
        "# model.load_state_dict(torch.load(\"your_checkpoint.pt\"), strict=False)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# 이제 이 모델은 CoLA나 다른 영어 문장 분류 작업에 사용할 수 있습니다.\n",
        "# 예시 추론 함수:\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 문장으로 테스트\n",
        "sentence = \"This is a grammatically acceptable for sentence.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "itlqawMcjmJ3",
        "outputId": "284e5f8a-d05a-4a2e-92d3-7ebec63eaa6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10545\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 22688\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5433707936\n",
            "[TOP] exp_trunc[9] = 331647\n",
            "[TOP] exp_accum_input[9] = 20727\n",
            "[TOP] x_divider = 13294\n",
            "[TOP] y_dividend = 1295\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 18051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4323160347\n",
            "[TOP] exp_trunc[9] = 263864\n",
            "[TOP] exp_accum_input[9] = 16491\n",
            "[TOP] x_divider = 10986\n",
            "[TOP] y_dividend = 1030\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4733418708\n",
            "[TOP] exp_trunc[9] = 288904\n",
            "[TOP] exp_accum_input[9] = 18056\n",
            "[TOP] x_divider = 11030\n",
            "[TOP] y_dividend = 1128\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 36626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8771817122\n",
            "[TOP] exp_trunc[9] = 535389\n",
            "[TOP] exp_accum_input[9] = 33461\n",
            "[TOP] x_divider = 16307\n",
            "[TOP] y_dividend = 2091\n",
            "[TOP] data_out[9] = 1049\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11950\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 18976\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4544695072\n",
            "[TOP] exp_trunc[9] = 277386\n",
            "[TOP] exp_accum_input[9] = 17336\n",
            "[TOP] x_divider = 11789\n",
            "[TOP] y_dividend = 1083\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10760\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 18398\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4406265806\n",
            "[TOP] exp_trunc[9] = 268937\n",
            "[TOP] exp_accum_input[9] = 16808\n",
            "[TOP] x_divider = 10597\n",
            "[TOP] y_dividend = 1050\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 20790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4979142630\n",
            "[TOP] exp_trunc[9] = 303902\n",
            "[TOP] exp_accum_input[9] = 18993\n",
            "[TOP] x_divider = 12566\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10485\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 18111\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4337530167\n",
            "[TOP] exp_trunc[9] = 264741\n",
            "[TOP] exp_accum_input[9] = 16546\n",
            "[TOP] x_divider = 11099\n",
            "[TOP] y_dividend = 1034\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 24696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5914617912\n",
            "[TOP] exp_trunc[9] = 360999\n",
            "[TOP] exp_accum_input[9] = 22562\n",
            "[TOP] x_divider = 12992\n",
            "[TOP] y_dividend = 1410\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 17635\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4223529595\n",
            "[TOP] exp_trunc[9] = 257783\n",
            "[TOP] exp_accum_input[9] = 16111\n",
            "[TOP] x_divider = 11887\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4239336397\n",
            "[TOP] exp_trunc[9] = 258748\n",
            "[TOP] exp_accum_input[9] = 16171\n",
            "[TOP] x_divider = 10796\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 21152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5065840544\n",
            "[TOP] exp_trunc[9] = 309194\n",
            "[TOP] exp_accum_input[9] = 19324\n",
            "[TOP] x_divider = 13778\n",
            "[TOP] y_dividend = 1207\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 25950\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6214947150\n",
            "[TOP] exp_trunc[9] = 379330\n",
            "[TOP] exp_accum_input[9] = 23708\n",
            "[TOP] x_divider = 11736\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 1031\n",
            "[TOP] exp_fraction_output[9] = 21684\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5193252948\n",
            "[TOP] exp_trunc[9] = 316971\n",
            "[TOP] exp_accum_input[9] = 19810\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 33292\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2933224952\n",
            "[TOP] exp_trunc[9] = 179029\n",
            "[TOP] exp_accum_input[9] = 11189\n",
            "[TOP] x_divider = 6314\n",
            "[TOP] y_dividend = 699\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25942\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6213031174\n",
            "[TOP] exp_trunc[9] = 379213\n",
            "[TOP] exp_accum_input[9] = 23700\n",
            "[TOP] x_divider = 15370\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18622\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4459913134\n",
            "[TOP] exp_trunc[9] = 272211\n",
            "[TOP] exp_accum_input[9] = 17013\n",
            "[TOP] x_divider = 11129\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 32763\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7846640211\n",
            "[TOP] exp_trunc[9] = 478920\n",
            "[TOP] exp_accum_input[9] = 29932\n",
            "[TOP] x_divider = 15649\n",
            "[TOP] y_dividend = 1870\n",
            "[TOP] data_out[9] = 979\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 14685\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 581\n",
            "[TOP] exp_fraction_output[9] = 28649\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6861349553\n",
            "[TOP] exp_trunc[9] = 418783\n",
            "[TOP] exp_accum_input[9] = 26173\n",
            "[TOP] x_divider = 12575\n",
            "[TOP] y_dividend = 1635\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 15574\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 22161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5307493017\n",
            "[TOP] exp_trunc[9] = 323943\n",
            "[TOP] exp_accum_input[9] = 20246\n",
            "[TOP] x_divider = 13237\n",
            "[TOP] y_dividend = 1265\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 26216\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6278653352\n",
            "[TOP] exp_trunc[9] = 383218\n",
            "[TOP] exp_accum_input[9] = 23951\n",
            "[TOP] x_divider = 12267\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 42670\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10219336990\n",
            "[TOP] exp_trunc[9] = 623738\n",
            "[TOP] exp_accum_input[9] = 38983\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 2436\n",
            "[TOP] data_out[9] = 1213\n",
            "[TOP] exp_fraction_output[9] = 31661\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7582714517\n",
            "[TOP] exp_trunc[9] = 462812\n",
            "[TOP] exp_accum_input[9] = 28925\n",
            "[TOP] x_divider = 15707\n",
            "[TOP] y_dividend = 1807\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 28966\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6937270102\n",
            "[TOP] exp_trunc[9] = 423417\n",
            "[TOP] exp_accum_input[9] = 26463\n",
            "[TOP] x_divider = 14162\n",
            "[TOP] y_dividend = 1653\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 24792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5937609624\n",
            "[TOP] exp_trunc[9] = 362402\n",
            "[TOP] exp_accum_input[9] = 22650\n",
            "[TOP] x_divider = 11674\n",
            "[TOP] y_dividend = 1415\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 25534\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6115316398\n",
            "[TOP] exp_trunc[9] = 373249\n",
            "[TOP] exp_accum_input[9] = 23328\n",
            "[TOP] x_divider = 12824\n",
            "[TOP] y_dividend = 1458\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 21825\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5227022025\n",
            "[TOP] exp_trunc[9] = 319032\n",
            "[TOP] exp_accum_input[9] = 19939\n",
            "[TOP] x_divider = 12345\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 23341\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5590099477\n",
            "[TOP] exp_trunc[9] = 341192\n",
            "[TOP] exp_accum_input[9] = 21324\n",
            "[TOP] x_divider = 13618\n",
            "[TOP] y_dividend = 1332\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 32814\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7858854558\n",
            "[TOP] exp_trunc[9] = 479666\n",
            "[TOP] exp_accum_input[9] = 29979\n",
            "[TOP] x_divider = 16724\n",
            "[TOP] y_dividend = 1873\n",
            "[TOP] data_out[9] = 917\n",
            "[TOP] exp_fraction_output[9] = 18204\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4359803388\n",
            "[TOP] exp_trunc[9] = 266101\n",
            "[TOP] exp_accum_input[9] = 16631\n",
            "[TOP] x_divider = 11169\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 18150\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4346870550\n",
            "[TOP] exp_trunc[9] = 265311\n",
            "[TOP] exp_accum_input[9] = 16581\n",
            "[TOP] x_divider = 11011\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18631\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4462068607\n",
            "[TOP] exp_trunc[9] = 272343\n",
            "[TOP] exp_accum_input[9] = 17021\n",
            "[TOP] x_divider = 10920\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 19265\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4613909705\n",
            "[TOP] exp_trunc[9] = 281610\n",
            "[TOP] exp_accum_input[9] = 17600\n",
            "[TOP] x_divider = 10973\n",
            "[TOP] y_dividend = 1100\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10828\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18917\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4530564749\n",
            "[TOP] exp_trunc[9] = 276523\n",
            "[TOP] exp_accum_input[9] = 17282\n",
            "[TOP] x_divider = 10840\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 19613\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4697254661\n",
            "[TOP] exp_trunc[9] = 286697\n",
            "[TOP] exp_accum_input[9] = 17918\n",
            "[TOP] x_divider = 11668\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20196\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836881412\n",
            "[TOP] exp_trunc[9] = 295219\n",
            "[TOP] exp_accum_input[9] = 18451\n",
            "[TOP] x_divider = 12199\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 25509\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6109328973\n",
            "[TOP] exp_trunc[9] = 372883\n",
            "[TOP] exp_accum_input[9] = 23305\n",
            "[TOP] x_divider = 13562\n",
            "[TOP] y_dividend = 1456\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4288912276\n",
            "[TOP] exp_trunc[9] = 261774\n",
            "[TOP] exp_accum_input[9] = 16360\n",
            "[TOP] x_divider = 11053\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 18189\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356210933\n",
            "[TOP] exp_trunc[9] = 265882\n",
            "[TOP] exp_accum_input[9] = 16617\n",
            "[TOP] x_divider = 10892\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 779\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 13359\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12605\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 18477\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4425186069\n",
            "[TOP] exp_trunc[9] = 270091\n",
            "[TOP] exp_accum_input[9] = 16880\n",
            "[TOP] x_divider = 14014\n",
            "[TOP] y_dividend = 1055\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 14893\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 20107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4815566179\n",
            "[TOP] exp_trunc[9] = 293918\n",
            "[TOP] exp_accum_input[9] = 18369\n",
            "[TOP] x_divider = 10863\n",
            "[TOP] y_dividend = 1148\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 22856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5473943432\n",
            "[TOP] exp_trunc[9] = 334102\n",
            "[TOP] exp_accum_input[9] = 20881\n",
            "[TOP] x_divider = 14529\n",
            "[TOP] y_dividend = 1305\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 583\n",
            "[TOP] exp_fraction_output[9] = 18548\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4442190356\n",
            "[TOP] exp_trunc[9] = 271129\n",
            "[TOP] exp_accum_input[9] = 16945\n",
            "[TOP] x_divider = 11061\n",
            "[TOP] y_dividend = 1059\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4679531883\n",
            "[TOP] exp_trunc[9] = 285615\n",
            "[TOP] exp_accum_input[9] = 17850\n",
            "[TOP] x_divider = 11685\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 20461\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4900348117\n",
            "[TOP] exp_trunc[9] = 299093\n",
            "[TOP] exp_accum_input[9] = 18693\n",
            "[TOP] x_divider = 11478\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 26894\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6441032318\n",
            "[TOP] exp_trunc[9] = 393129\n",
            "[TOP] exp_accum_input[9] = 24570\n",
            "[TOP] x_divider = 11410\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 19860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4756410420\n",
            "[TOP] exp_trunc[9] = 290308\n",
            "[TOP] exp_accum_input[9] = 18144\n",
            "[TOP] x_divider = 10536\n",
            "[TOP] y_dividend = 1134\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11262\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 839\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11701\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20444\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4896276668\n",
            "[TOP] exp_trunc[9] = 298845\n",
            "[TOP] exp_accum_input[9] = 18677\n",
            "[TOP] x_divider = 11715\n",
            "[TOP] y_dividend = 1167\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 21815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5224627055\n",
            "[TOP] exp_trunc[9] = 318885\n",
            "[TOP] exp_accum_input[9] = 19930\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 21527\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5155651919\n",
            "[TOP] exp_trunc[9] = 314676\n",
            "[TOP] exp_accum_input[9] = 19667\n",
            "[TOP] x_divider = 12489\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185449572\n",
            "[TOP] exp_trunc[9] = 255459\n",
            "[TOP] exp_accum_input[9] = 15966\n",
            "[TOP] x_divider = 11698\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4569123766\n",
            "[TOP] exp_trunc[9] = 278877\n",
            "[TOP] exp_accum_input[9] = 17429\n",
            "[TOP] x_divider = 12182\n",
            "[TOP] y_dividend = 1089\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 17543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4201495871\n",
            "[TOP] exp_trunc[9] = 256438\n",
            "[TOP] exp_accum_input[9] = 16027\n",
            "[TOP] x_divider = 12255\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17766\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254903702\n",
            "[TOP] exp_trunc[9] = 259698\n",
            "[TOP] exp_accum_input[9] = 16231\n",
            "[TOP] x_divider = 10276\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 809\n",
            "[TOP] exp_fraction_output[9] = 20666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949445002\n",
            "[TOP] exp_trunc[9] = 302090\n",
            "[TOP] exp_accum_input[9] = 18880\n",
            "[TOP] x_divider = 10877\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11281\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 26040\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6236501880\n",
            "[TOP] exp_trunc[9] = 380645\n",
            "[TOP] exp_accum_input[9] = 23790\n",
            "[TOP] x_divider = 11992\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 18538\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4439795386\n",
            "[TOP] exp_trunc[9] = 270983\n",
            "[TOP] exp_accum_input[9] = 16936\n",
            "[TOP] x_divider = 11386\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 25020\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5992214940\n",
            "[TOP] exp_trunc[9] = 365735\n",
            "[TOP] exp_accum_input[9] = 22858\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1428\n",
            "[TOP] data_out[9] = 969\n",
            "[TOP] exp_fraction_output[9] = 18292\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380879124\n",
            "[TOP] exp_trunc[9] = 267387\n",
            "[TOP] exp_accum_input[9] = 16711\n",
            "[TOP] x_divider = 11272\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 18294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4381358118\n",
            "[TOP] exp_trunc[9] = 267416\n",
            "[TOP] exp_accum_input[9] = 16713\n",
            "[TOP] x_divider = 11240\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 23762\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5690927714\n",
            "[TOP] exp_trunc[9] = 347346\n",
            "[TOP] exp_accum_input[9] = 21709\n",
            "[TOP] x_divider = 11857\n",
            "[TOP] y_dividend = 1356\n",
            "[TOP] data_out[9] = 937\n",
            "[TOP] exp_fraction_output[9] = 29990\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7182515030\n",
            "[TOP] exp_trunc[9] = 438385\n",
            "[TOP] exp_accum_input[9] = 27399\n",
            "[TOP] x_divider = 12830\n",
            "[TOP] y_dividend = 1712\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 20088\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4811015736\n",
            "[TOP] exp_trunc[9] = 293641\n",
            "[TOP] exp_accum_input[9] = 18352\n",
            "[TOP] x_divider = 11929\n",
            "[TOP] y_dividend = 1147\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 26891\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6440313827\n",
            "[TOP] exp_trunc[9] = 393085\n",
            "[TOP] exp_accum_input[9] = 24567\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1071\n",
            "[TOP] exp_fraction_output[9] = 19436\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4654863692\n",
            "[TOP] exp_trunc[9] = 284110\n",
            "[TOP] exp_accum_input[9] = 17756\n",
            "[TOP] x_divider = 11596\n",
            "[TOP] y_dividend = 1109\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 18565\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4446261805\n",
            "[TOP] exp_trunc[9] = 271378\n",
            "[TOP] exp_accum_input[9] = 16961\n",
            "[TOP] x_divider = 11838\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 11210\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 25440\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6092803680\n",
            "[TOP] exp_trunc[9] = 371875\n",
            "[TOP] exp_accum_input[9] = 23242\n",
            "[TOP] x_divider = 12640\n",
            "[TOP] y_dividend = 1452\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 12939\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 1003\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11779\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4664443572\n",
            "[TOP] exp_trunc[9] = 284695\n",
            "[TOP] exp_accum_input[9] = 17793\n",
            "[TOP] x_divider = 12994\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17528\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4197903416\n",
            "[TOP] exp_trunc[9] = 256219\n",
            "[TOP] exp_accum_input[9] = 16013\n",
            "[TOP] x_divider = 12928\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 22511\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5391316967\n",
            "[TOP] exp_trunc[9] = 329059\n",
            "[TOP] exp_accum_input[9] = 20566\n",
            "[TOP] x_divider = 13605\n",
            "[TOP] y_dividend = 1285\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 19154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4587325538\n",
            "[TOP] exp_trunc[9] = 279988\n",
            "[TOP] exp_accum_input[9] = 17499\n",
            "[TOP] x_divider = 11381\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4191915991\n",
            "[TOP] exp_trunc[9] = 255854\n",
            "[TOP] exp_accum_input[9] = 15990\n",
            "[TOP] x_divider = 10757\n",
            "[TOP] y_dividend = 999\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 18288\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4379921136\n",
            "[TOP] exp_trunc[9] = 267329\n",
            "[TOP] exp_accum_input[9] = 16708\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 19604\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4695099188\n",
            "[TOP] exp_trunc[9] = 286566\n",
            "[TOP] exp_accum_input[9] = 17910\n",
            "[TOP] x_divider = 11040\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18929\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4533438713\n",
            "[TOP] exp_trunc[9] = 276699\n",
            "[TOP] exp_accum_input[9] = 17293\n",
            "[TOP] x_divider = 11820\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 19510\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4672586470\n",
            "[TOP] exp_trunc[9] = 285192\n",
            "[TOP] exp_accum_input[9] = 17824\n",
            "[TOP] x_divider = 10950\n",
            "[TOP] y_dividend = 1114\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 23850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5712003450\n",
            "[TOP] exp_trunc[9] = 348633\n",
            "[TOP] exp_accum_input[9] = 21789\n",
            "[TOP] x_divider = 12004\n",
            "[TOP] y_dividend = 1361\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 17605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4216344685\n",
            "[TOP] exp_trunc[9] = 257345\n",
            "[TOP] exp_accum_input[9] = 16084\n",
            "[TOP] x_divider = 10933\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4255861690\n",
            "[TOP] exp_trunc[9] = 259757\n",
            "[TOP] exp_accum_input[9] = 16234\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21731\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5204509307\n",
            "[TOP] exp_trunc[9] = 317658\n",
            "[TOP] exp_accum_input[9] = 19853\n",
            "[TOP] x_divider = 11488\n",
            "[TOP] y_dividend = 1240\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 17835\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271428995\n",
            "[TOP] exp_trunc[9] = 260707\n",
            "[TOP] exp_accum_input[9] = 16294\n",
            "[TOP] x_divider = 11475\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4513081468\n",
            "[TOP] exp_trunc[9] = 275456\n",
            "[TOP] exp_accum_input[9] = 17216\n",
            "[TOP] x_divider = 11140\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585170065\n",
            "[TOP] exp_trunc[9] = 279856\n",
            "[TOP] exp_accum_input[9] = 17491\n",
            "[TOP] x_divider = 12620\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11781\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12720\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10502\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 26868\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6434805396\n",
            "[TOP] exp_trunc[9] = 392749\n",
            "[TOP] exp_accum_input[9] = 24546\n",
            "[TOP] x_divider = 16212\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 17779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4258017163\n",
            "[TOP] exp_trunc[9] = 259888\n",
            "[TOP] exp_accum_input[9] = 16243\n",
            "[TOP] x_divider = 11161\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 19338\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4631392986\n",
            "[TOP] exp_trunc[9] = 282677\n",
            "[TOP] exp_accum_input[9] = 17667\n",
            "[TOP] x_divider = 12405\n",
            "[TOP] y_dividend = 1104\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 24236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5804449292\n",
            "[TOP] exp_trunc[9] = 354275\n",
            "[TOP] exp_accum_input[9] = 22142\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 1383\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 23326\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586507022\n",
            "[TOP] exp_trunc[9] = 340973\n",
            "[TOP] exp_accum_input[9] = 21310\n",
            "[TOP] x_divider = 12019\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 907\n",
            "[TOP] exp_fraction_output[9] = 17198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4118869406\n",
            "[TOP] exp_trunc[9] = 251395\n",
            "[TOP] exp_accum_input[9] = 15712\n",
            "[TOP] x_divider = 12154\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 19165\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4589960005\n",
            "[TOP] exp_trunc[9] = 280148\n",
            "[TOP] exp_accum_input[9] = 17509\n",
            "[TOP] x_divider = 13570\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 19628\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4700847116\n",
            "[TOP] exp_trunc[9] = 286916\n",
            "[TOP] exp_accum_input[9] = 17932\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1120\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 19049\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562178353\n",
            "[TOP] exp_trunc[9] = 278453\n",
            "[TOP] exp_accum_input[9] = 17403\n",
            "[TOP] x_divider = 13609\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 23612\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5655003164\n",
            "[TOP] exp_trunc[9] = 345154\n",
            "[TOP] exp_accum_input[9] = 21572\n",
            "[TOP] x_divider = 11212\n",
            "[TOP] y_dividend = 1348\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 26603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6371338691\n",
            "[TOP] exp_trunc[9] = 388875\n",
            "[TOP] exp_accum_input[9] = 24304\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 1519\n",
            "[TOP] data_out[9] = 1093\n",
            "[TOP] exp_fraction_output[9] = 19570\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4686956290\n",
            "[TOP] exp_trunc[9] = 286069\n",
            "[TOP] exp_accum_input[9] = 17879\n",
            "[TOP] x_divider = 11083\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17699\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4238857403\n",
            "[TOP] exp_trunc[9] = 258719\n",
            "[TOP] exp_accum_input[9] = 16169\n",
            "[TOP] x_divider = 11586\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20194\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836402418\n",
            "[TOP] exp_trunc[9] = 295190\n",
            "[TOP] exp_accum_input[9] = 18449\n",
            "[TOP] x_divider = 11269\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4200537883\n",
            "[TOP] exp_trunc[9] = 256380\n",
            "[TOP] exp_accum_input[9] = 16023\n",
            "[TOP] x_divider = 12494\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 24900\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963475300\n",
            "[TOP] exp_trunc[9] = 363981\n",
            "[TOP] exp_accum_input[9] = 22748\n",
            "[TOP] x_divider = 12659\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 25175\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6029336975\n",
            "[TOP] exp_trunc[9] = 368001\n",
            "[TOP] exp_accum_input[9] = 23000\n",
            "[TOP] x_divider = 13321\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 17880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4282206360\n",
            "[TOP] exp_trunc[9] = 261365\n",
            "[TOP] exp_accum_input[9] = 16335\n",
            "[TOP] x_divider = 11318\n",
            "[TOP] y_dividend = 1020\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 23240\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5565910280\n",
            "[TOP] exp_trunc[9] = 339716\n",
            "[TOP] exp_accum_input[9] = 21232\n",
            "[TOP] x_divider = 11637\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 29592\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7087195224\n",
            "[TOP] exp_trunc[9] = 432568\n",
            "[TOP] exp_accum_input[9] = 27035\n",
            "[TOP] x_divider = 14586\n",
            "[TOP] y_dividend = 1689\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 19549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4681926853\n",
            "[TOP] exp_trunc[9] = 285762\n",
            "[TOP] exp_accum_input[9] = 17860\n",
            "[TOP] x_divider = 12147\n",
            "[TOP] y_dividend = 1116\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4974592187\n",
            "[TOP] exp_trunc[9] = 303625\n",
            "[TOP] exp_accum_input[9] = 18976\n",
            "[TOP] x_divider = 13006\n",
            "[TOP] y_dividend = 1186\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12027\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 32790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7853106630\n",
            "[TOP] exp_trunc[9] = 479315\n",
            "[TOP] exp_accum_input[9] = 29957\n",
            "[TOP] x_divider = 15772\n",
            "[TOP] y_dividend = 1872\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 17904\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4287954288\n",
            "[TOP] exp_trunc[9] = 261715\n",
            "[TOP] exp_accum_input[9] = 16357\n",
            "[TOP] x_divider = 12088\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 18765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4494161205\n",
            "[TOP] exp_trunc[9] = 274301\n",
            "[TOP] exp_accum_input[9] = 17143\n",
            "[TOP] x_divider = 12011\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 14106\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 18408\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4408660776\n",
            "[TOP] exp_trunc[9] = 269083\n",
            "[TOP] exp_accum_input[9] = 16817\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 23801\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5700268097\n",
            "[TOP] exp_trunc[9] = 347916\n",
            "[TOP] exp_accum_input[9] = 21744\n",
            "[TOP] x_divider = 12166\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20608\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4935554176\n",
            "[TOP] exp_trunc[9] = 301242\n",
            "[TOP] exp_accum_input[9] = 18827\n",
            "[TOP] x_divider = 10942\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17837\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271907989\n",
            "[TOP] exp_trunc[9] = 260736\n",
            "[TOP] exp_accum_input[9] = 16296\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 18665\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4470211505\n",
            "[TOP] exp_trunc[9] = 272840\n",
            "[TOP] exp_accum_input[9] = 17052\n",
            "[TOP] x_divider = 13420\n",
            "[TOP] y_dividend = 1065\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12270\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 651\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10312\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 20152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4826343544\n",
            "[TOP] exp_trunc[9] = 294576\n",
            "[TOP] exp_accum_input[9] = 18411\n",
            "[TOP] x_divider = 11172\n",
            "[TOP] y_dividend = 1150\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17318\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4147609046\n",
            "[TOP] exp_trunc[9] = 253149\n",
            "[TOP] exp_accum_input[9] = 15821\n",
            "[TOP] x_divider = 10474\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18540\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4440274380\n",
            "[TOP] exp_trunc[9] = 271012\n",
            "[TOP] exp_accum_input[9] = 16938\n",
            "[TOP] x_divider = 10600\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22703\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5437300391\n",
            "[TOP] exp_trunc[9] = 331866\n",
            "[TOP] exp_accum_input[9] = 20741\n",
            "[TOP] x_divider = 11384\n",
            "[TOP] y_dividend = 1296\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 18629\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4461589613\n",
            "[TOP] exp_trunc[9] = 272313\n",
            "[TOP] exp_accum_input[9] = 17019\n",
            "[TOP] x_divider = 11241\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18758\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4492484726\n",
            "[TOP] exp_trunc[9] = 274199\n",
            "[TOP] exp_accum_input[9] = 17137\n",
            "[TOP] x_divider = 11123\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 17259\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4133478723\n",
            "[TOP] exp_trunc[9] = 252287\n",
            "[TOP] exp_accum_input[9] = 15767\n",
            "[TOP] x_divider = 11648\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 20459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4899869123\n",
            "[TOP] exp_trunc[9] = 299064\n",
            "[TOP] exp_accum_input[9] = 18691\n",
            "[TOP] x_divider = 11372\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 19391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4644086327\n",
            "[TOP] exp_trunc[9] = 283452\n",
            "[TOP] exp_accum_input[9] = 17715\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1107\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 601\n",
            "[TOP] exp_fraction_output[9] = 23924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5729726228\n",
            "[TOP] exp_trunc[9] = 349714\n",
            "[TOP] exp_accum_input[9] = 21857\n",
            "[TOP] x_divider = 15161\n",
            "[TOP] y_dividend = 1366\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18084\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4331063748\n",
            "[TOP] exp_trunc[9] = 264347\n",
            "[TOP] exp_accum_input[9] = 16521\n",
            "[TOP] x_divider = 11549\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20389\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4883104333\n",
            "[TOP] exp_trunc[9] = 298041\n",
            "[TOP] exp_accum_input[9] = 18627\n",
            "[TOP] x_divider = 12473\n",
            "[TOP] y_dividend = 1164\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 11803\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12347\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 19005\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4551640485\n",
            "[TOP] exp_trunc[9] = 277810\n",
            "[TOP] exp_accum_input[9] = 17363\n",
            "[TOP] x_divider = 10852\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20587\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4930524739\n",
            "[TOP] exp_trunc[9] = 300935\n",
            "[TOP] exp_accum_input[9] = 18808\n",
            "[TOP] x_divider = 11912\n",
            "[TOP] y_dividend = 1175\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 18433\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4414648201\n",
            "[TOP] exp_trunc[9] = 269448\n",
            "[TOP] exp_accum_input[9] = 16840\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1052\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11200\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 715\n",
            "[TOP] exp_fraction_output[9] = 19320\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627082040\n",
            "[TOP] exp_trunc[9] = 282414\n",
            "[TOP] exp_accum_input[9] = 17650\n",
            "[TOP] x_divider = 10945\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229996014\n",
            "[TOP] exp_trunc[9] = 258178\n",
            "[TOP] exp_accum_input[9] = 16136\n",
            "[TOP] x_divider = 12042\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17425\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173235225\n",
            "[TOP] exp_trunc[9] = 254714\n",
            "[TOP] exp_accum_input[9] = 15919\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 994\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 18411\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4409379267\n",
            "[TOP] exp_trunc[9] = 269127\n",
            "[TOP] exp_accum_input[9] = 16820\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 21515\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5152777955\n",
            "[TOP] exp_trunc[9] = 314500\n",
            "[TOP] exp_accum_input[9] = 19656\n",
            "[TOP] x_divider = 11338\n",
            "[TOP] y_dividend = 1228\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 21738\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5206185786\n",
            "[TOP] exp_trunc[9] = 317760\n",
            "[TOP] exp_accum_input[9] = 19860\n",
            "[TOP] x_divider = 10818\n",
            "[TOP] y_dividend = 1241\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 27987\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6702802539\n",
            "[TOP] exp_trunc[9] = 409106\n",
            "[TOP] exp_accum_input[9] = 25569\n",
            "[TOP] x_divider = 12986\n",
            "[TOP] y_dividend = 1598\n",
            "[TOP] data_out[9] = 1009\n",
            "[TOP] exp_fraction_output[9] = 18858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4516434426\n",
            "[TOP] exp_trunc[9] = 275661\n",
            "[TOP] exp_accum_input[9] = 17228\n",
            "[TOP] x_divider = 10406\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12028\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 18833\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4510447001\n",
            "[TOP] exp_trunc[9] = 275295\n",
            "[TOP] exp_accum_input[9] = 17205\n",
            "[TOP] x_divider = 11231\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 19012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553316964\n",
            "[TOP] exp_trunc[9] = 277912\n",
            "[TOP] exp_accum_input[9] = 17369\n",
            "[TOP] x_divider = 11886\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 18965\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4542060605\n",
            "[TOP] exp_trunc[9] = 277225\n",
            "[TOP] exp_accum_input[9] = 17326\n",
            "[TOP] x_divider = 11516\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 19802\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4742519594\n",
            "[TOP] exp_trunc[9] = 289460\n",
            "[TOP] exp_accum_input[9] = 18091\n",
            "[TOP] x_divider = 10443\n",
            "[TOP] y_dividend = 1130\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 17212\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122222364\n",
            "[TOP] exp_trunc[9] = 251600\n",
            "[TOP] exp_accum_input[9] = 15725\n",
            "[TOP] x_divider = 10064\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13551\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 589\n",
            "[TOP] exp_fraction_output[9] = 21101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5053626197\n",
            "[TOP] exp_trunc[9] = 308448\n",
            "[TOP] exp_accum_input[9] = 19278\n",
            "[TOP] x_divider = 12093\n",
            "[TOP] y_dividend = 1204\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 19855\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4755212935\n",
            "[TOP] exp_trunc[9] = 290235\n",
            "[TOP] exp_accum_input[9] = 18139\n",
            "[TOP] x_divider = 11830\n",
            "[TOP] y_dividend = 1133\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10872\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914478440\n",
            "[TOP] exp_trunc[9] = 299955\n",
            "[TOP] exp_accum_input[9] = 18747\n",
            "[TOP] x_divider = 12614\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17681\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4234546457\n",
            "[TOP] exp_trunc[9] = 258456\n",
            "[TOP] exp_accum_input[9] = 16153\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 17618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4219458146\n",
            "[TOP] exp_trunc[9] = 257535\n",
            "[TOP] exp_accum_input[9] = 16095\n",
            "[TOP] x_divider = 10811\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 23311\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5582914567\n",
            "[TOP] exp_trunc[9] = 340754\n",
            "[TOP] exp_accum_input[9] = 21297\n",
            "[TOP] x_divider = 12055\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 903\n",
            "[TOP] exp_fraction_output[9] = 27549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6597902853\n",
            "[TOP] exp_trunc[9] = 402704\n",
            "[TOP] exp_accum_input[9] = 25169\n",
            "[TOP] x_divider = 11877\n",
            "[TOP] y_dividend = 1573\n",
            "[TOP] data_out[9] = 1085\n",
            "[TOP] exp_fraction_output[9] = 23100\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5532380700\n",
            "[TOP] exp_trunc[9] = 337669\n",
            "[TOP] exp_accum_input[9] = 21104\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 11257\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12119\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11278\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12672\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 18290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380400130\n",
            "[TOP] exp_trunc[9] = 267358\n",
            "[TOP] exp_accum_input[9] = 16709\n",
            "[TOP] x_divider = 15782\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 541\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 15434\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 519\n",
            "[TOP] exp_fraction_output[9] = 23491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626024027\n",
            "[TOP] exp_trunc[9] = 343385\n",
            "[TOP] exp_accum_input[9] = 21461\n",
            "[TOP] x_divider = 11613\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6140463583\n",
            "[TOP] exp_trunc[9] = 374784\n",
            "[TOP] exp_accum_input[9] = 23424\n",
            "[TOP] x_divider = 11406\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 1051\n",
            "[TOP] exp_fraction_output[9] = 27184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6510486448\n",
            "[TOP] exp_trunc[9] = 397368\n",
            "[TOP] exp_accum_input[9] = 24835\n",
            "[TOP] x_divider = 10939\n",
            "[TOP] y_dividend = 1552\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 38909\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 9318588773\n",
            "[TOP] exp_trunc[9] = 568761\n",
            "[TOP] exp_accum_input[9] = 35547\n",
            "[TOP] x_divider = 14650\n",
            "[TOP] y_dividend = 2221\n",
            "[TOP] data_out[9] = 1241\n",
            "[TOP] exp_fraction_output[9] = 18880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4521703360\n",
            "[TOP] exp_trunc[9] = 275982\n",
            "[TOP] exp_accum_input[9] = 17248\n",
            "[TOP] x_divider = 11160\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18898\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4526014306\n",
            "[TOP] exp_trunc[9] = 276245\n",
            "[TOP] exp_accum_input[9] = 17265\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11882\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6030534460\n",
            "[TOP] exp_trunc[9] = 368074\n",
            "[TOP] exp_accum_input[9] = 23004\n",
            "[TOP] x_divider = 11702\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 1007\n",
            "[TOP] exp_fraction_output[9] = 33438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8008300686\n",
            "[TOP] exp_trunc[9] = 488787\n",
            "[TOP] exp_accum_input[9] = 30549\n",
            "[TOP] x_divider = 13486\n",
            "[TOP] y_dividend = 1909\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13624\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 26312\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6301645064\n",
            "[TOP] exp_trunc[9] = 384621\n",
            "[TOP] exp_accum_input[9] = 24038\n",
            "[TOP] x_divider = 13254\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 26719\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6399120343\n",
            "[TOP] exp_trunc[9] = 390571\n",
            "[TOP] exp_accum_input[9] = 24410\n",
            "[TOP] x_divider = 13958\n",
            "[TOP] y_dividend = 1525\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 21453\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5137929141\n",
            "[TOP] exp_trunc[9] = 313594\n",
            "[TOP] exp_accum_input[9] = 19599\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1224\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 20994\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5028000018\n",
            "[TOP] exp_trunc[9] = 306884\n",
            "[TOP] exp_accum_input[9] = 19180\n",
            "[TOP] x_divider = 12802\n",
            "[TOP] y_dividend = 1198\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 11437\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11430\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4270471007\n",
            "[TOP] exp_trunc[9] = 260648\n",
            "[TOP] exp_accum_input[9] = 16290\n",
            "[TOP] x_divider = 10452\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 21132\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5061050604\n",
            "[TOP] exp_trunc[9] = 308902\n",
            "[TOP] exp_accum_input[9] = 19306\n",
            "[TOP] x_divider = 12565\n",
            "[TOP] y_dividend = 1206\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20685\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4953995445\n",
            "[TOP] exp_trunc[9] = 302367\n",
            "[TOP] exp_accum_input[9] = 18897\n",
            "[TOP] x_divider = 11166\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 19007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4552119479\n",
            "[TOP] exp_trunc[9] = 277839\n",
            "[TOP] exp_accum_input[9] = 17364\n",
            "[TOP] x_divider = 10533\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 27234\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6522461298\n",
            "[TOP] exp_trunc[9] = 398099\n",
            "[TOP] exp_accum_input[9] = 24881\n",
            "[TOP] x_divider = 14731\n",
            "[TOP] y_dividend = 1555\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 18810\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4504938570\n",
            "[TOP] exp_trunc[9] = 274959\n",
            "[TOP] exp_accum_input[9] = 17184\n",
            "[TOP] x_divider = 12703\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 19595\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4692943715\n",
            "[TOP] exp_trunc[9] = 286434\n",
            "[TOP] exp_accum_input[9] = 17902\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1118\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 24646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5902643062\n",
            "[TOP] exp_trunc[9] = 360268\n",
            "[TOP] exp_accum_input[9] = 22516\n",
            "[TOP] x_divider = 11314\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 1021\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4217781667\n",
            "[TOP] exp_trunc[9] = 257432\n",
            "[TOP] exp_accum_input[9] = 16089\n",
            "[TOP] x_divider = 10966\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 13581\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 607\n",
            "[TOP] exp_fraction_output[9] = 19494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668754518\n",
            "[TOP] exp_trunc[9] = 284958\n",
            "[TOP] exp_accum_input[9] = 17809\n",
            "[TOP] x_divider = 12558\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 19147\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585649059\n",
            "[TOP] exp_trunc[9] = 279885\n",
            "[TOP] exp_accum_input[9] = 17492\n",
            "[TOP] x_divider = 12142\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4792813964\n",
            "[TOP] exp_trunc[9] = 292530\n",
            "[TOP] exp_accum_input[9] = 18283\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1142\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 18371\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4399799387\n",
            "[TOP] exp_trunc[9] = 268542\n",
            "[TOP] exp_accum_input[9] = 16783\n",
            "[TOP] x_divider = 10865\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19498\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4669712506\n",
            "[TOP] exp_trunc[9] = 285016\n",
            "[TOP] exp_accum_input[9] = 17813\n",
            "[TOP] x_divider = 12894\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 23607\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653805679\n",
            "[TOP] exp_trunc[9] = 345080\n",
            "[TOP] exp_accum_input[9] = 21567\n",
            "[TOP] x_divider = 12525\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 11684\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 41979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10053844563\n",
            "[TOP] exp_trunc[9] = 613637\n",
            "[TOP] exp_accum_input[9] = 38352\n",
            "[TOP] x_divider = 19046\n",
            "[TOP] y_dividend = 2397\n",
            "[TOP] data_out[9] = 1029\n",
            "[TOP] exp_fraction_output[9] = 27869\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6674541893\n",
            "[TOP] exp_trunc[9] = 407381\n",
            "[TOP] exp_accum_input[9] = 25461\n",
            "[TOP] x_divider = 15936\n",
            "[TOP] y_dividend = 1591\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 18222\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4364114334\n",
            "[TOP] exp_trunc[9] = 266364\n",
            "[TOP] exp_accum_input[9] = 16647\n",
            "[TOP] x_divider = 12675\n",
            "[TOP] y_dividend = 1040\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 23031\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2029169286\n",
            "[TOP] exp_trunc[9] = 123850\n",
            "[TOP] exp_accum_input[9] = 7740\n",
            "[TOP] x_divider = 6434\n",
            "[TOP] y_dividend = 483\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 28910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6923858270\n",
            "[TOP] exp_trunc[9] = 422598\n",
            "[TOP] exp_accum_input[9] = 26412\n",
            "[TOP] x_divider = 14346\n",
            "[TOP] y_dividend = 1650\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 26208\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6276737376\n",
            "[TOP] exp_trunc[9] = 383101\n",
            "[TOP] exp_accum_input[9] = 23943\n",
            "[TOP] x_divider = 16608\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 739\n",
            "[TOP] exp_fraction_output[9] = 22906\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5485918282\n",
            "[TOP] exp_trunc[9] = 334833\n",
            "[TOP] exp_accum_input[9] = 20927\n",
            "[TOP] x_divider = 11539\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19317\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4626363549\n",
            "[TOP] exp_trunc[9] = 282370\n",
            "[TOP] exp_accum_input[9] = 17648\n",
            "[TOP] x_divider = 11482\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 22958\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5498372126\n",
            "[TOP] exp_trunc[9] = 335594\n",
            "[TOP] exp_accum_input[9] = 20974\n",
            "[TOP] x_divider = 13617\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 20526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4915915422\n",
            "[TOP] exp_trunc[9] = 300043\n",
            "[TOP] exp_accum_input[9] = 18752\n",
            "[TOP] x_divider = 11517\n",
            "[TOP] y_dividend = 1172\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 19491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668036027\n",
            "[TOP] exp_trunc[9] = 284914\n",
            "[TOP] exp_accum_input[9] = 17807\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 21679\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5192055463\n",
            "[TOP] exp_trunc[9] = 316897\n",
            "[TOP] exp_accum_input[9] = 19806\n",
            "[TOP] x_divider = 12265\n",
            "[TOP] y_dividend = 1237\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 27746\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6645083762\n",
            "[TOP] exp_trunc[9] = 405583\n",
            "[TOP] exp_accum_input[9] = 25348\n",
            "[TOP] x_divider = 11529\n",
            "[TOP] y_dividend = 1584\n",
            "[TOP] data_out[9] = 1125\n",
            "[TOP] exp_fraction_output[9] = 26184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6270989448\n",
            "[TOP] exp_trunc[9] = 382750\n",
            "[TOP] exp_accum_input[9] = 23921\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1069\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11261\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 17854\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275979438\n",
            "[TOP] exp_trunc[9] = 260985\n",
            "[TOP] exp_accum_input[9] = 16311\n",
            "[TOP] x_divider = 13483\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 17520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195987440\n",
            "[TOP] exp_trunc[9] = 256102\n",
            "[TOP] exp_accum_input[9] = 16006\n",
            "[TOP] x_divider = 11555\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 28729\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6880509313\n",
            "[TOP] exp_trunc[9] = 419952\n",
            "[TOP] exp_accum_input[9] = 26247\n",
            "[TOP] x_divider = 13307\n",
            "[TOP] y_dividend = 1640\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10854\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 22146\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5303900562\n",
            "[TOP] exp_trunc[9] = 323724\n",
            "[TOP] exp_accum_input[9] = 20232\n",
            "[TOP] x_divider = 13327\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 19051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562657347\n",
            "[TOP] exp_trunc[9] = 278482\n",
            "[TOP] exp_accum_input[9] = 17405\n",
            "[TOP] x_divider = 12252\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 21537\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5158046889\n",
            "[TOP] exp_trunc[9] = 314822\n",
            "[TOP] exp_accum_input[9] = 19676\n",
            "[TOP] x_divider = 11224\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 21039\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5038777383\n",
            "[TOP] exp_trunc[9] = 307542\n",
            "[TOP] exp_accum_input[9] = 19221\n",
            "[TOP] x_divider = 13680\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4390937998\n",
            "[TOP] exp_trunc[9] = 268001\n",
            "[TOP] exp_accum_input[9] = 16750\n",
            "[TOP] x_divider = 11223\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12122\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 17458\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4181138626\n",
            "[TOP] exp_trunc[9] = 255196\n",
            "[TOP] exp_accum_input[9] = 15949\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4247000301\n",
            "[TOP] exp_trunc[9] = 259216\n",
            "[TOP] exp_accum_input[9] = 16201\n",
            "[TOP] x_divider = 11681\n",
            "[TOP] y_dividend = 1012\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17714\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4242449858\n",
            "[TOP] exp_trunc[9] = 258938\n",
            "[TOP] exp_accum_input[9] = 16183\n",
            "[TOP] x_divider = 11219\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 24467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5859773099\n",
            "[TOP] exp_trunc[9] = 357652\n",
            "[TOP] exp_accum_input[9] = 22353\n",
            "[TOP] x_divider = 13017\n",
            "[TOP] y_dividend = 1397\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 17936\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4295618192\n",
            "[TOP] exp_trunc[9] = 262183\n",
            "[TOP] exp_accum_input[9] = 16386\n",
            "[TOP] x_divider = 11293\n",
            "[TOP] y_dividend = 1024\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 20737\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4966449289\n",
            "[TOP] exp_trunc[9] = 303128\n",
            "[TOP] exp_accum_input[9] = 18945\n",
            "[TOP] x_divider = 11408\n",
            "[TOP] y_dividend = 1184\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 18908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4528409276\n",
            "[TOP] exp_trunc[9] = 276392\n",
            "[TOP] exp_accum_input[9] = 17274\n",
            "[TOP] x_divider = 12646\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 21331\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5108710507\n",
            "[TOP] exp_trunc[9] = 311810\n",
            "[TOP] exp_accum_input[9] = 19488\n",
            "[TOP] x_divider = 12179\n",
            "[TOP] y_dividend = 1218\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 26471\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6339725087\n",
            "[TOP] exp_trunc[9] = 386946\n",
            "[TOP] exp_accum_input[9] = 24184\n",
            "[TOP] x_divider = 14785\n",
            "[TOP] y_dividend = 1511\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 29843\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7147308971\n",
            "[TOP] exp_trunc[9] = 436237\n",
            "[TOP] exp_accum_input[9] = 27264\n",
            "[TOP] x_divider = 12719\n",
            "[TOP] y_dividend = 1704\n",
            "[TOP] data_out[9] = 1095\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 13229\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 32217\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2838511002\n",
            "[TOP] exp_trunc[9] = 173248\n",
            "[TOP] exp_accum_input[9] = 10828\n",
            "[TOP] x_divider = 7613\n",
            "[TOP] y_dividend = 676\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12065\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 22273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5334316681\n",
            "[TOP] exp_trunc[9] = 325580\n",
            "[TOP] exp_accum_input[9] = 20348\n",
            "[TOP] x_divider = 13500\n",
            "[TOP] y_dividend = 1271\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 17379\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4162218363\n",
            "[TOP] exp_trunc[9] = 254041\n",
            "[TOP] exp_accum_input[9] = 15877\n",
            "[TOP] x_divider = 11481\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5789121484\n",
            "[TOP] exp_trunc[9] = 353339\n",
            "[TOP] exp_accum_input[9] = 22083\n",
            "[TOP] x_divider = 16257\n",
            "[TOP] y_dividend = 1380\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17671\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4232151487\n",
            "[TOP] exp_trunc[9] = 258310\n",
            "[TOP] exp_accum_input[9] = 16144\n",
            "[TOP] x_divider = 11825\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 23637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5660990589\n",
            "[TOP] exp_trunc[9] = 345519\n",
            "[TOP] exp_accum_input[9] = 21594\n",
            "[TOP] x_divider = 12438\n",
            "[TOP] y_dividend = 1349\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 12744\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5048117766\n",
            "[TOP] exp_trunc[9] = 308112\n",
            "[TOP] exp_accum_input[9] = 19257\n",
            "[TOP] x_divider = 13302\n",
            "[TOP] y_dividend = 1203\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 27363\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6553356411\n",
            "[TOP] exp_trunc[9] = 399985\n",
            "[TOP] exp_accum_input[9] = 24999\n",
            "[TOP] x_divider = 16020\n",
            "[TOP] y_dividend = 1562\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 22149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5304619053\n",
            "[TOP] exp_trunc[9] = 323768\n",
            "[TOP] exp_accum_input[9] = 20235\n",
            "[TOP] x_divider = 11791\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 877\n",
            "[TOP] exp_fraction_output[9] = 19322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627561034\n",
            "[TOP] exp_trunc[9] = 282443\n",
            "[TOP] exp_accum_input[9] = 17652\n",
            "[TOP] x_divider = 11394\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11163\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 12123\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 18896\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4525535312\n",
            "[TOP] exp_trunc[9] = 276216\n",
            "[TOP] exp_accum_input[9] = 17263\n",
            "[TOP] x_divider = 11031\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 18569\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4447219793\n",
            "[TOP] exp_trunc[9] = 271436\n",
            "[TOP] exp_accum_input[9] = 16964\n",
            "[TOP] x_divider = 12091\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 19172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4591636484\n",
            "[TOP] exp_trunc[9] = 280251\n",
            "[TOP] exp_accum_input[9] = 17515\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 18519\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4435244943\n",
            "[TOP] exp_trunc[9] = 270705\n",
            "[TOP] exp_accum_input[9] = 16919\n",
            "[TOP] x_divider = 12249\n",
            "[TOP] y_dividend = 1057\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 26186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6271468442\n",
            "[TOP] exp_trunc[9] = 382780\n",
            "[TOP] exp_accum_input[9] = 23923\n",
            "[TOP] x_divider = 11360\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1077\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10860\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18776\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4496795672\n",
            "[TOP] exp_trunc[9] = 274462\n",
            "[TOP] exp_accum_input[9] = 17153\n",
            "[TOP] x_divider = 11153\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18724\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4484341828\n",
            "[TOP] exp_trunc[9] = 273702\n",
            "[TOP] exp_accum_input[9] = 17106\n",
            "[TOP] x_divider = 11072\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11000\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 28198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6753336406\n",
            "[TOP] exp_trunc[9] = 412190\n",
            "[TOP] exp_accum_input[9] = 25761\n",
            "[TOP] x_divider = 15249\n",
            "[TOP] y_dividend = 1610\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 28591\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6847458727\n",
            "[TOP] exp_trunc[9] = 417935\n",
            "[TOP] exp_accum_input[9] = 26120\n",
            "[TOP] x_divider = 13872\n",
            "[TOP] y_dividend = 1632\n",
            "[TOP] data_out[9] = 965\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20319\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4866339543\n",
            "[TOP] exp_trunc[9] = 297017\n",
            "[TOP] exp_accum_input[9] = 18563\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 1160\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13427\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 22601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412871697\n",
            "[TOP] exp_trunc[9] = 330375\n",
            "[TOP] exp_accum_input[9] = 20648\n",
            "[TOP] x_divider = 13130\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 37387\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8954074339\n",
            "[TOP] exp_trunc[9] = 546513\n",
            "[TOP] exp_accum_input[9] = 34157\n",
            "[TOP] x_divider = 16787\n",
            "[TOP] y_dividend = 2134\n",
            "[TOP] data_out[9] = 1039\n",
            "[TOP] exp_fraction_output[9] = 25840\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6188602480\n",
            "[TOP] exp_trunc[9] = 377722\n",
            "[TOP] exp_accum_input[9] = 23607\n",
            "[TOP] x_divider = 12098\n",
            "[TOP] y_dividend = 1475\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 35844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8584530468\n",
            "[TOP] exp_trunc[9] = 523958\n",
            "[TOP] exp_accum_input[9] = 32747\n",
            "[TOP] x_divider = 13716\n",
            "[TOP] y_dividend = 2046\n",
            "[TOP] data_out[9] = 1221\n",
            "[TOP] exp_fraction_output[9] = 25658\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6145014026\n",
            "[TOP] exp_trunc[9] = 375061\n",
            "[TOP] exp_accum_input[9] = 23441\n",
            "[TOP] x_divider = 13429\n",
            "[TOP] y_dividend = 1465\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20183\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4833767951\n",
            "[TOP] exp_trunc[9] = 295029\n",
            "[TOP] exp_accum_input[9] = 18439\n",
            "[TOP] x_divider = 10613\n",
            "[TOP] y_dividend = 1152\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 36993\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8859712521\n",
            "[TOP] exp_trunc[9] = 540753\n",
            "[TOP] exp_accum_input[9] = 33797\n",
            "[TOP] x_divider = 18570\n",
            "[TOP] y_dividend = 2112\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 43948\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 3872082488\n",
            "[TOP] exp_trunc[9] = 236333\n",
            "[TOP] exp_accum_input[9] = 14770\n",
            "[TOP] x_divider = 7486\n",
            "[TOP] y_dividend = 923\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 24121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5776907137\n",
            "[TOP] exp_trunc[9] = 352594\n",
            "[TOP] exp_accum_input[9] = 22037\n",
            "[TOP] x_divider = 13245\n",
            "[TOP] y_dividend = 1377\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 7897\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 32030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7671088910\n",
            "[TOP] exp_trunc[9] = 468206\n",
            "[TOP] exp_accum_input[9] = 29262\n",
            "[TOP] x_divider = 12925\n",
            "[TOP] y_dividend = 1828\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24213\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5798940861\n",
            "[TOP] exp_trunc[9] = 353939\n",
            "[TOP] exp_accum_input[9] = 22121\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1382\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 10858\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 27273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6531801681\n",
            "[TOP] exp_trunc[9] = 398669\n",
            "[TOP] exp_accum_input[9] = 24916\n",
            "[TOP] x_divider = 12112\n",
            "[TOP] y_dividend = 1557\n",
            "[TOP] data_out[9] = 1053\n",
            "[TOP] exp_fraction_output[9] = 21581\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5168584757\n",
            "[TOP] exp_trunc[9] = 315465\n",
            "[TOP] exp_accum_input[9] = 19716\n",
            "[TOP] x_divider = 11560\n",
            "[TOP] y_dividend = 1232\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6104778530\n",
            "[TOP] exp_trunc[9] = 372606\n",
            "[TOP] exp_accum_input[9] = 23287\n",
            "[TOP] x_divider = 11984\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 25647\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6142379559\n",
            "[TOP] exp_trunc[9] = 374901\n",
            "[TOP] exp_accum_input[9] = 23431\n",
            "[TOP] x_divider = 15670\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 31798\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7615525606\n",
            "[TOP] exp_trunc[9] = 464814\n",
            "[TOP] exp_accum_input[9] = 29050\n",
            "[TOP] x_divider = 12708\n",
            "[TOP] y_dividend = 1815\n",
            "[TOP] data_out[9] = 1169\n",
            "[TOP] exp_fraction_output[9] = 25853\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6191715941\n",
            "[TOP] exp_trunc[9] = 377912\n",
            "[TOP] exp_accum_input[9] = 23619\n",
            "[TOP] x_divider = 13472\n",
            "[TOP] y_dividend = 1476\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 28599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6849374703\n",
            "[TOP] exp_trunc[9] = 418052\n",
            "[TOP] exp_accum_input[9] = 26128\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1633\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 18328\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4389501016\n",
            "[TOP] exp_trunc[9] = 267913\n",
            "[TOP] exp_accum_input[9] = 16744\n",
            "[TOP] x_divider = 11117\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 22925\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5490468725\n",
            "[TOP] exp_trunc[9] = 335111\n",
            "[TOP] exp_accum_input[9] = 20944\n",
            "[TOP] x_divider = 12102\n",
            "[TOP] y_dividend = 1309\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4411774237\n",
            "[TOP] exp_trunc[9] = 269273\n",
            "[TOP] exp_accum_input[9] = 16829\n",
            "[TOP] x_divider = 12442\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 28633\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6857517601\n",
            "[TOP] exp_trunc[9] = 418549\n",
            "[TOP] exp_accum_input[9] = 26159\n",
            "[TOP] x_divider = 15349\n",
            "[TOP] y_dividend = 1634\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 33114\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7930703658\n",
            "[TOP] exp_trunc[9] = 484051\n",
            "[TOP] exp_accum_input[9] = 30253\n",
            "[TOP] x_divider = 16206\n",
            "[TOP] y_dividend = 1890\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21768\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213370696\n",
            "[TOP] exp_trunc[9] = 318198\n",
            "[TOP] exp_accum_input[9] = 19887\n",
            "[TOP] x_divider = 15445\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 32959\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7893581623\n",
            "[TOP] exp_trunc[9] = 481785\n",
            "[TOP] exp_accum_input[9] = 30111\n",
            "[TOP] x_divider = 16141\n",
            "[TOP] y_dividend = 1881\n",
            "[TOP] data_out[9] = 955\n",
            "[TOP] exp_fraction_output[9] = 33686\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8067695942\n",
            "[TOP] exp_trunc[9] = 492413\n",
            "[TOP] exp_accum_input[9] = 30775\n",
            "[TOP] x_divider = 16475\n",
            "[TOP] y_dividend = 1923\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5081407849\n",
            "[TOP] exp_trunc[9] = 310144\n",
            "[TOP] exp_accum_input[9] = 19384\n",
            "[TOP] x_divider = 12340\n",
            "[TOP] y_dividend = 1211\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5497174641\n",
            "[TOP] exp_trunc[9] = 335520\n",
            "[TOP] exp_accum_input[9] = 20970\n",
            "[TOP] x_divider = 13633\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 24526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5873903422\n",
            "[TOP] exp_trunc[9] = 358514\n",
            "[TOP] exp_accum_input[9] = 22407\n",
            "[TOP] x_divider = 12220\n",
            "[TOP] y_dividend = 1400\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 20601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4933877697\n",
            "[TOP] exp_trunc[9] = 301139\n",
            "[TOP] exp_accum_input[9] = 18821\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 25485\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6103581045\n",
            "[TOP] exp_trunc[9] = 372533\n",
            "[TOP] exp_accum_input[9] = 23283\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 835\n",
            "[TOP] exp_fraction_output[9] = 20494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4908251518\n",
            "[TOP] exp_trunc[9] = 299575\n",
            "[TOP] exp_accum_input[9] = 18723\n",
            "[TOP] x_divider = 12187\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 25333\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6067177501\n",
            "[TOP] exp_trunc[9] = 370311\n",
            "[TOP] exp_accum_input[9] = 23144\n",
            "[TOP] x_divider = 15042\n",
            "[TOP] y_dividend = 1446\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6027420999\n",
            "[TOP] exp_trunc[9] = 367884\n",
            "[TOP] exp_accum_input[9] = 22992\n",
            "[TOP] x_divider = 14310\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 11863\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 23952\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2110314912\n",
            "[TOP] exp_trunc[9] = 128803\n",
            "[TOP] exp_accum_input[9] = 8050\n",
            "[TOP] x_divider = 5423\n",
            "[TOP] y_dividend = 503\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 21903\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5245702791\n",
            "[TOP] exp_trunc[9] = 320172\n",
            "[TOP] exp_accum_input[9] = 20010\n",
            "[TOP] x_divider = 11453\n",
            "[TOP] y_dividend = 1250\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20171\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4830893987\n",
            "[TOP] exp_trunc[9] = 294854\n",
            "[TOP] exp_accum_input[9] = 18428\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 23420\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5609019740\n",
            "[TOP] exp_trunc[9] = 342347\n",
            "[TOP] exp_accum_input[9] = 21396\n",
            "[TOP] x_divider = 13078\n",
            "[TOP] y_dividend = 1337\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 14252\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 567\n",
            "[TOP] exp_fraction_output[9] = 24856\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2189962736\n",
            "[TOP] exp_trunc[9] = 133664\n",
            "[TOP] exp_accum_input[9] = 8354\n",
            "[TOP] x_divider = 7201\n",
            "[TOP] y_dividend = 522\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 18889\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1664234234\n",
            "[TOP] exp_trunc[9] = 101576\n",
            "[TOP] exp_accum_input[9] = 6348\n",
            "[TOP] x_divider = 5850\n",
            "[TOP] y_dividend = 396\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 17772\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4256340684\n",
            "[TOP] exp_trunc[9] = 259786\n",
            "[TOP] exp_accum_input[9] = 16236\n",
            "[TOP] x_divider = 11906\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 631\n",
            "[TOP] exp_fraction_output[9] = 17468\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4183533596\n",
            "[TOP] exp_trunc[9] = 255342\n",
            "[TOP] exp_accum_input[9] = 15958\n",
            "[TOP] x_divider = 13071\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 13266\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 24616\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5895458152\n",
            "[TOP] exp_trunc[9] = 359830\n",
            "[TOP] exp_accum_input[9] = 22489\n",
            "[TOP] x_divider = 17693\n",
            "[TOP] y_dividend = 1405\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 20926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5011714222\n",
            "[TOP] exp_trunc[9] = 305890\n",
            "[TOP] exp_accum_input[9] = 19118\n",
            "[TOP] x_divider = 11780\n",
            "[TOP] y_dividend = 1194\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 18754\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4491526738\n",
            "[TOP] exp_trunc[9] = 274141\n",
            "[TOP] exp_accum_input[9] = 17133\n",
            "[TOP] x_divider = 12039\n",
            "[TOP] y_dividend = 1070\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17214\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122701358\n",
            "[TOP] exp_trunc[9] = 251629\n",
            "[TOP] exp_accum_input[9] = 15726\n",
            "[TOP] x_divider = 10739\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17226\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4125575322\n",
            "[TOP] exp_trunc[9] = 251805\n",
            "[TOP] exp_accum_input[9] = 15737\n",
            "[TOP] x_divider = 10355\n",
            "[TOP] y_dividend = 983\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 21770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213849690\n",
            "[TOP] exp_trunc[9] = 318228\n",
            "[TOP] exp_accum_input[9] = 19889\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 18831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4509968007\n",
            "[TOP] exp_trunc[9] = 275266\n",
            "[TOP] exp_accum_input[9] = 17204\n",
            "[TOP] x_divider = 12274\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 18953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4539186641\n",
            "[TOP] exp_trunc[9] = 277049\n",
            "[TOP] exp_accum_input[9] = 17315\n",
            "[TOP] x_divider = 10512\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17518\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195508446\n",
            "[TOP] exp_trunc[9] = 256073\n",
            "[TOP] exp_accum_input[9] = 16004\n",
            "[TOP] x_divider = 11380\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 19217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4602413849\n",
            "[TOP] exp_trunc[9] = 280909\n",
            "[TOP] exp_accum_input[9] = 17556\n",
            "[TOP] x_divider = 10585\n",
            "[TOP] y_dividend = 1097\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 20644\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4944176068\n",
            "[TOP] exp_trunc[9] = 301768\n",
            "[TOP] exp_accum_input[9] = 18860\n",
            "[TOP] x_divider = 10861\n",
            "[TOP] y_dividend = 1178\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19199\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4598102903\n",
            "[TOP] exp_trunc[9] = 280645\n",
            "[TOP] exp_accum_input[9] = 17540\n",
            "[TOP] x_divider = 10850\n",
            "[TOP] y_dividend = 1096\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 25224\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6041072328\n",
            "[TOP] exp_trunc[9] = 368717\n",
            "[TOP] exp_accum_input[9] = 23044\n",
            "[TOP] x_divider = 11252\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 1047\n",
            "[TOP] exp_fraction_output[9] = 23170\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5549145490\n",
            "[TOP] exp_trunc[9] = 338692\n",
            "[TOP] exp_accum_input[9] = 21168\n",
            "[TOP] x_divider = 11840\n",
            "[TOP] y_dividend = 1323\n",
            "[TOP] data_out[9] = 915\n",
            "[TOP] exp_fraction_output[9] = 21353\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5113979441\n",
            "[TOP] exp_trunc[9] = 312132\n",
            "[TOP] exp_accum_input[9] = 19508\n",
            "[TOP] x_divider = 11460\n",
            "[TOP] y_dividend = 1219\n",
            "[TOP] data_out[9] = 871\n",
            "[TOP] exp_fraction_output[9] = 21785\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5217442145\n",
            "[TOP] exp_trunc[9] = 318447\n",
            "[TOP] exp_accum_input[9] = 19902\n",
            "[TOP] x_divider = 10963\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 23107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5534057179\n",
            "[TOP] exp_trunc[9] = 337772\n",
            "[TOP] exp_accum_input[9] = 21110\n",
            "[TOP] x_divider = 11337\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 953\n",
            "[TOP] exp_fraction_output[9] = 22112\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5295757664\n",
            "[TOP] exp_trunc[9] = 323227\n",
            "[TOP] exp_accum_input[9] = 20201\n",
            "[TOP] x_divider = 12541\n",
            "[TOP] y_dividend = 1262\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 21790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5218639630\n",
            "[TOP] exp_trunc[9] = 318520\n",
            "[TOP] exp_accum_input[9] = 19907\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1244\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 25459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6097354123\n",
            "[TOP] exp_trunc[9] = 372152\n",
            "[TOP] exp_accum_input[9] = 23259\n",
            "[TOP] x_divider = 12078\n",
            "[TOP] y_dividend = 1453\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 28295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6776567615\n",
            "[TOP] exp_trunc[9] = 413608\n",
            "[TOP] exp_accum_input[9] = 25850\n",
            "[TOP] x_divider = 13416\n",
            "[TOP] y_dividend = 1615\n",
            "[TOP] data_out[9] = 987\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 27047\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6477675359\n",
            "[TOP] exp_trunc[9] = 395365\n",
            "[TOP] exp_accum_input[9] = 24710\n",
            "[TOP] x_divider = 16395\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 22813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5463645061\n",
            "[TOP] exp_trunc[9] = 333474\n",
            "[TOP] exp_accum_input[9] = 20842\n",
            "[TOP] x_divider = 13514\n",
            "[TOP] y_dividend = 1302\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6156509882\n",
            "[TOP] exp_trunc[9] = 375763\n",
            "[TOP] exp_accum_input[9] = 23485\n",
            "[TOP] x_divider = 13073\n",
            "[TOP] y_dividend = 1467\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 17850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275021450\n",
            "[TOP] exp_trunc[9] = 260926\n",
            "[TOP] exp_accum_input[9] = 16307\n",
            "[TOP] x_divider = 11368\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17443\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4177546171\n",
            "[TOP] exp_trunc[9] = 254977\n",
            "[TOP] exp_accum_input[9] = 15936\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10931\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 29143\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6979661071\n",
            "[TOP] exp_trunc[9] = 426004\n",
            "[TOP] exp_accum_input[9] = 26625\n",
            "[TOP] x_divider = 14588\n",
            "[TOP] y_dividend = 1664\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12761\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 629\n",
            "[TOP] exp_fraction_output[9] = 30236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7241431292\n",
            "[TOP] exp_trunc[9] = 441981\n",
            "[TOP] exp_accum_input[9] = 27623\n",
            "[TOP] x_divider = 15270\n",
            "[TOP] y_dividend = 1726\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 20490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907293530\n",
            "[TOP] exp_trunc[9] = 299517\n",
            "[TOP] exp_accum_input[9] = 18719\n",
            "[TOP] x_divider = 13310\n",
            "[TOP] y_dividend = 1169\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4500627624\n",
            "[TOP] exp_trunc[9] = 274696\n",
            "[TOP] exp_accum_input[9] = 17168\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1073\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 27416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6566049752\n",
            "[TOP] exp_trunc[9] = 400759\n",
            "[TOP] exp_accum_input[9] = 25047\n",
            "[TOP] x_divider = 11998\n",
            "[TOP] y_dividend = 1565\n",
            "[TOP] data_out[9] = 1067\n",
            "[TOP] exp_fraction_output[9] = 28860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6911883420\n",
            "[TOP] exp_trunc[9] = 421867\n",
            "[TOP] exp_accum_input[9] = 26366\n",
            "[TOP] x_divider = 13191\n",
            "[TOP] y_dividend = 1647\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 29779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7131981163\n",
            "[TOP] exp_trunc[9] = 435301\n",
            "[TOP] exp_accum_input[9] = 27206\n",
            "[TOP] x_divider = 13079\n",
            "[TOP] y_dividend = 1700\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 17366\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4159104902\n",
            "[TOP] exp_trunc[9] = 253851\n",
            "[TOP] exp_accum_input[9] = 15865\n",
            "[TOP] x_divider = 11259\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 26536\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6355292392\n",
            "[TOP] exp_trunc[9] = 387896\n",
            "[TOP] exp_accum_input[9] = 24243\n",
            "[TOP] x_divider = 11466\n",
            "[TOP] y_dividend = 1515\n",
            "[TOP] data_out[9] = 1081\n",
            "[TOP] exp_fraction_output[9] = 23193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5554653921\n",
            "[TOP] exp_trunc[9] = 339029\n",
            "[TOP] exp_accum_input[9] = 21189\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 1324\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23577\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5646620769\n",
            "[TOP] exp_trunc[9] = 344642\n",
            "[TOP] exp_accum_input[9] = 21540\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 1346\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 31609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7570260673\n",
            "[TOP] exp_trunc[9] = 462052\n",
            "[TOP] exp_accum_input[9] = 28878\n",
            "[TOP] x_divider = 13385\n",
            "[TOP] y_dividend = 1804\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 22186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5313480442\n",
            "[TOP] exp_trunc[9] = 324309\n",
            "[TOP] exp_accum_input[9] = 20269\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1266\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 17474\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4184970578\n",
            "[TOP] exp_trunc[9] = 255430\n",
            "[TOP] exp_accum_input[9] = 15964\n",
            "[TOP] x_divider = 10496\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4312622479\n",
            "[TOP] exp_trunc[9] = 263221\n",
            "[TOP] exp_accum_input[9] = 16451\n",
            "[TOP] x_divider = 11097\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11796\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 17161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4110008017\n",
            "[TOP] exp_trunc[9] = 250854\n",
            "[TOP] exp_accum_input[9] = 15678\n",
            "[TOP] x_divider = 12331\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11544\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18015\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4314538455\n",
            "[TOP] exp_trunc[9] = 263338\n",
            "[TOP] exp_accum_input[9] = 16458\n",
            "[TOP] x_divider = 10713\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17275\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4137310675\n",
            "[TOP] exp_trunc[9] = 252521\n",
            "[TOP] exp_accum_input[9] = 15782\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 986\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17552\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4203651344\n",
            "[TOP] exp_trunc[9] = 256570\n",
            "[TOP] exp_accum_input[9] = 16035\n",
            "[TOP] x_divider = 10350\n",
            "[TOP] y_dividend = 1002\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 22462\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5379581614\n",
            "[TOP] exp_trunc[9] = 328343\n",
            "[TOP] exp_accum_input[9] = 20521\n",
            "[TOP] x_divider = 11290\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 18009\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4313101473\n",
            "[TOP] exp_trunc[9] = 263250\n",
            "[TOP] exp_accum_input[9] = 16453\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18206\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4360282382\n",
            "[TOP] exp_trunc[9] = 266130\n",
            "[TOP] exp_accum_input[9] = 16633\n",
            "[TOP] x_divider = 11258\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4148567034\n",
            "[TOP] exp_trunc[9] = 253208\n",
            "[TOP] exp_accum_input[9] = 15825\n",
            "[TOP] x_divider = 10461\n",
            "[TOP] y_dividend = 989\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11719\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 683\n",
            "[TOP] exp_fraction_output[9] = 21032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5037100904\n",
            "[TOP] exp_trunc[9] = 307440\n",
            "[TOP] exp_accum_input[9] = 19215\n",
            "[TOP] x_divider = 11626\n",
            "[TOP] y_dividend = 1200\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 23243\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5566628771\n",
            "[TOP] exp_trunc[9] = 339760\n",
            "[TOP] exp_accum_input[9] = 21235\n",
            "[TOP] x_divider = 12278\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18821\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4507573037\n",
            "[TOP] exp_trunc[9] = 275120\n",
            "[TOP] exp_accum_input[9] = 17195\n",
            "[TOP] x_divider = 13368\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 22599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412392703\n",
            "[TOP] exp_trunc[9] = 330346\n",
            "[TOP] exp_accum_input[9] = 20646\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 26391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6320565327\n",
            "[TOP] exp_trunc[9] = 385776\n",
            "[TOP] exp_accum_input[9] = 24111\n",
            "[TOP] x_divider = 14123\n",
            "[TOP] y_dividend = 1506\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25188\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6032450436\n",
            "[TOP] exp_trunc[9] = 368191\n",
            "[TOP] exp_accum_input[9] = 23011\n",
            "[TOP] x_divider = 14432\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 21416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5129067752\n",
            "[TOP] exp_trunc[9] = 313053\n",
            "[TOP] exp_accum_input[9] = 19565\n",
            "[TOP] x_divider = 12398\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 29409\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7043367273\n",
            "[TOP] exp_trunc[9] = 429893\n",
            "[TOP] exp_accum_input[9] = 26868\n",
            "[TOP] x_divider = 14547\n",
            "[TOP] y_dividend = 1679\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25837\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2276394722\n",
            "[TOP] exp_trunc[9] = 138940\n",
            "[TOP] exp_accum_input[9] = 8683\n",
            "[TOP] x_divider = 6291\n",
            "[TOP] y_dividend = 542\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 24815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5943118055\n",
            "[TOP] exp_trunc[9] = 362739\n",
            "[TOP] exp_accum_input[9] = 22671\n",
            "[TOP] x_divider = 14134\n",
            "[TOP] y_dividend = 1416\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 23324\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586028028\n",
            "[TOP] exp_trunc[9] = 340944\n",
            "[TOP] exp_accum_input[9] = 21309\n",
            "[TOP] x_divider = 11095\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 11195\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 24045\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5758705365\n",
            "[TOP] exp_trunc[9] = 351483\n",
            "[TOP] exp_accum_input[9] = 21967\n",
            "[TOP] x_divider = 13000\n",
            "[TOP] y_dividend = 1372\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 29003\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6946131491\n",
            "[TOP] exp_trunc[9] = 423958\n",
            "[TOP] exp_accum_input[9] = 26497\n",
            "[TOP] x_divider = 15963\n",
            "[TOP] y_dividend = 1656\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 27073\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6483902281\n",
            "[TOP] exp_trunc[9] = 395745\n",
            "[TOP] exp_accum_input[9] = 24734\n",
            "[TOP] x_divider = 14354\n",
            "[TOP] y_dividend = 1545\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 27050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6478393850\n",
            "[TOP] exp_trunc[9] = 395409\n",
            "[TOP] exp_accum_input[9] = 24713\n",
            "[TOP] x_divider = 12938\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 977\n",
            "[TOP] exp_fraction_output[9] = 22122\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5298152634\n",
            "[TOP] exp_trunc[9] = 323373\n",
            "[TOP] exp_accum_input[9] = 20210\n",
            "[TOP] x_divider = 13012\n",
            "[TOP] y_dividend = 1263\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 21822\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5226303534\n",
            "[TOP] exp_trunc[9] = 318988\n",
            "[TOP] exp_accum_input[9] = 19936\n",
            "[TOP] x_divider = 12487\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12393\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17660\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229517020\n",
            "[TOP] exp_trunc[9] = 258149\n",
            "[TOP] exp_accum_input[9] = 16134\n",
            "[TOP] x_divider = 12694\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 23205\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5557527885\n",
            "[TOP] exp_trunc[9] = 339204\n",
            "[TOP] exp_accum_input[9] = 21200\n",
            "[TOP] x_divider = 12836\n",
            "[TOP] y_dividend = 1325\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 14111\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14478\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 921\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 18624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4460392128\n",
            "[TOP] exp_trunc[9] = 272240\n",
            "[TOP] exp_accum_input[9] = 17015\n",
            "[TOP] x_divider = 11611\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 11091\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18191\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356689927\n",
            "[TOP] exp_trunc[9] = 265911\n",
            "[TOP] exp_accum_input[9] = 16619\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 26258\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6288712226\n",
            "[TOP] exp_trunc[9] = 383832\n",
            "[TOP] exp_accum_input[9] = 23989\n",
            "[TOP] x_divider = 12630\n",
            "[TOP] y_dividend = 1499\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 22637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5421493589\n",
            "[TOP] exp_trunc[9] = 330901\n",
            "[TOP] exp_accum_input[9] = 20681\n",
            "[TOP] x_divider = 11913\n",
            "[TOP] y_dividend = 1292\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 11733\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 20618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4937949146\n",
            "[TOP] exp_trunc[9] = 301388\n",
            "[TOP] exp_accum_input[9] = 18836\n",
            "[TOP] x_divider = 12339\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 23709\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5678234373\n",
            "[TOP] exp_trunc[9] = 346571\n",
            "[TOP] exp_accum_input[9] = 21660\n",
            "[TOP] x_divider = 12533\n",
            "[TOP] y_dividend = 1353\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 18351\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4395009447\n",
            "[TOP] exp_trunc[9] = 268250\n",
            "[TOP] exp_accum_input[9] = 16765\n",
            "[TOP] x_divider = 11976\n",
            "[TOP] y_dividend = 1047\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 27286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6534915142\n",
            "[TOP] exp_trunc[9] = 398859\n",
            "[TOP] exp_accum_input[9] = 24928\n",
            "[TOP] x_divider = 12974\n",
            "[TOP] y_dividend = 1558\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 21316\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5105118052\n",
            "[TOP] exp_trunc[9] = 311591\n",
            "[TOP] exp_accum_input[9] = 19474\n",
            "[TOP] x_divider = 12248\n",
            "[TOP] y_dividend = 1217\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 23018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5512741946\n",
            "[TOP] exp_trunc[9] = 336471\n",
            "[TOP] exp_accum_input[9] = 21029\n",
            "[TOP] x_divider = 11794\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 23817\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5704100049\n",
            "[TOP] exp_trunc[9] = 348150\n",
            "[TOP] exp_accum_input[9] = 21759\n",
            "[TOP] x_divider = 11722\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 951\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 11659\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 22490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5386287530\n",
            "[TOP] exp_trunc[9] = 328752\n",
            "[TOP] exp_accum_input[9] = 20547\n",
            "[TOP] x_divider = 10790\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5719188360\n",
            "[TOP] exp_trunc[9] = 349071\n",
            "[TOP] exp_accum_input[9] = 21816\n",
            "[TOP] x_divider = 11254\n",
            "[TOP] y_dividend = 1363\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 19652\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4706595044\n",
            "[TOP] exp_trunc[9] = 287267\n",
            "[TOP] exp_accum_input[9] = 17954\n",
            "[TOP] x_divider = 11006\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 10769\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 17397\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4166529309\n",
            "[TOP] exp_trunc[9] = 254304\n",
            "[TOP] exp_accum_input[9] = 15894\n",
            "[TOP] x_divider = 11350\n",
            "[TOP] y_dividend = 993\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11572\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22901\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5484720797\n",
            "[TOP] exp_trunc[9] = 334760\n",
            "[TOP] exp_accum_input[9] = 20922\n",
            "[TOP] x_divider = 10916\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 23824\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5705776528\n",
            "[TOP] exp_trunc[9] = 348252\n",
            "[TOP] exp_accum_input[9] = 21765\n",
            "[TOP] x_divider = 11600\n",
            "[TOP] y_dividend = 1360\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4305916563\n",
            "[TOP] exp_trunc[9] = 262812\n",
            "[TOP] exp_accum_input[9] = 16425\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1026\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 22322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5346052034\n",
            "[TOP] exp_trunc[9] = 326297\n",
            "[TOP] exp_accum_input[9] = 20393\n",
            "[TOP] x_divider = 12478\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22922\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5489750234\n",
            "[TOP] exp_trunc[9] = 335067\n",
            "[TOP] exp_accum_input[9] = 20941\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1308\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10837\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10951\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 18715\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4482186355\n",
            "[TOP] exp_trunc[9] = 273570\n",
            "[TOP] exp_accum_input[9] = 17098\n",
            "[TOP] x_divider = 13642\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 10978\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 17427\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173714219\n",
            "[TOP] exp_trunc[9] = 254743\n",
            "[TOP] exp_accum_input[9] = 15921\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 995\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 20290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4859394130\n",
            "[TOP] exp_trunc[9] = 296593\n",
            "[TOP] exp_accum_input[9] = 18537\n",
            "[TOP] x_divider = 11075\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 28032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6713579904\n",
            "[TOP] exp_trunc[9] = 409764\n",
            "[TOP] exp_accum_input[9] = 25610\n",
            "[TOP] x_divider = 17852\n",
            "[TOP] y_dividend = 1600\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 18082\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4330584754\n",
            "[TOP] exp_trunc[9] = 264317\n",
            "[TOP] exp_accum_input[9] = 16519\n",
            "[TOP] x_divider = 11156\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13246\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 605\n",
            "[TOP] exp_fraction_output[9] = 36600\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8765590200\n",
            "[TOP] exp_trunc[9] = 535009\n",
            "[TOP] exp_accum_input[9] = 33438\n",
            "[TOP] x_divider = 19558\n",
            "[TOP] y_dividend = 2089\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11546\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 21696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5196126912\n",
            "[TOP] exp_trunc[9] = 317146\n",
            "[TOP] exp_accum_input[9] = 19821\n",
            "[TOP] x_divider = 10919\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 24254\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5808760238\n",
            "[TOP] exp_trunc[9] = 354538\n",
            "[TOP] exp_accum_input[9] = 22158\n",
            "[TOP] x_divider = 12597\n",
            "[TOP] y_dividend = 1384\n",
            "[TOP] data_out[9] = 899\n",
            "[TOP] exp_fraction_output[9] = 25193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6033647921\n",
            "[TOP] exp_trunc[9] = 368264\n",
            "[TOP] exp_accum_input[9] = 23016\n",
            "[TOP] x_divider = 13186\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 23121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5537410137\n",
            "[TOP] exp_trunc[9] = 337976\n",
            "[TOP] exp_accum_input[9] = 21123\n",
            "[TOP] x_divider = 13934\n",
            "[TOP] y_dividend = 1320\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 18167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4350941999\n",
            "[TOP] exp_trunc[9] = 265560\n",
            "[TOP] exp_accum_input[9] = 16597\n",
            "[TOP] x_divider = 11412\n",
            "[TOP] y_dividend = 1037\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 24070\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5764692790\n",
            "[TOP] exp_trunc[9] = 351848\n",
            "[TOP] exp_accum_input[9] = 21990\n",
            "[TOP] x_divider = 14412\n",
            "[TOP] y_dividend = 1374\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 17346\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4154314962\n",
            "[TOP] exp_trunc[9] = 253559\n",
            "[TOP] exp_accum_input[9] = 15847\n",
            "[TOP] x_divider = 10781\n",
            "[TOP] y_dividend = 990\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4696296673\n",
            "[TOP] exp_trunc[9] = 286639\n",
            "[TOP] exp_accum_input[9] = 17914\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4224487583\n",
            "[TOP] exp_trunc[9] = 257842\n",
            "[TOP] exp_accum_input[9] = 16115\n",
            "[TOP] x_divider = 11369\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 21242\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5087395274\n",
            "[TOP] exp_trunc[9] = 310509\n",
            "[TOP] exp_accum_input[9] = 19406\n",
            "[TOP] x_divider = 11535\n",
            "[TOP] y_dividend = 1212\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 18787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4499430139\n",
            "[TOP] exp_trunc[9] = 274623\n",
            "[TOP] exp_accum_input[9] = 17163\n",
            "[TOP] x_divider = 11035\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 17706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4240533882\n",
            "[TOP] exp_trunc[9] = 258821\n",
            "[TOP] exp_accum_input[9] = 16176\n",
            "[TOP] x_divider = 11067\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 24642\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5901685074\n",
            "[TOP] exp_trunc[9] = 360210\n",
            "[TOP] exp_accum_input[9] = 22513\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 30984\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7420575048\n",
            "[TOP] exp_trunc[9] = 452915\n",
            "[TOP] exp_accum_input[9] = 28307\n",
            "[TOP] x_divider = 12499\n",
            "[TOP] y_dividend = 1769\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24348\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5831272956\n",
            "[TOP] exp_trunc[9] = 355912\n",
            "[TOP] exp_accum_input[9] = 22244\n",
            "[TOP] x_divider = 12721\n",
            "[TOP] y_dividend = 1390\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 28716\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6877395852\n",
            "[TOP] exp_trunc[9] = 419762\n",
            "[TOP] exp_accum_input[9] = 26235\n",
            "[TOP] x_divider = 12710\n",
            "[TOP] y_dividend = 1639\n",
            "[TOP] data_out[9] = 1055\n",
            "[TOP] exp_fraction_output[9] = 25229\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6042269813\n",
            "[TOP] exp_trunc[9] = 368790\n",
            "[TOP] exp_accum_input[9] = 23049\n",
            "[TOP] x_divider = 12639\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 933\n",
            "[TOP] exp_fraction_output[9] = 23605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653326685\n",
            "[TOP] exp_trunc[9] = 345051\n",
            "[TOP] exp_accum_input[9] = 21565\n",
            "[TOP] x_divider = 10784\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 10622\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 27720\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6638856840\n",
            "[TOP] exp_trunc[9] = 405203\n",
            "[TOP] exp_accum_input[9] = 25325\n",
            "[TOP] x_divider = 13212\n",
            "[TOP] y_dividend = 1582\n",
            "[TOP] data_out[9] = 981\n",
            "[TOP] exp_fraction_output[9] = 22973\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5501964581\n",
            "[TOP] exp_trunc[9] = 335813\n",
            "[TOP] exp_accum_input[9] = 20988\n",
            "[TOP] x_divider = 11058\n",
            "[TOP] y_dividend = 1311\n",
            "[TOP] data_out[9] = 971\n",
            "[TOP] exp_fraction_output[9] = 22503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5389400991\n",
            "[TOP] exp_trunc[9] = 328942\n",
            "[TOP] exp_accum_input[9] = 20558\n",
            "[TOP] x_divider = 11890\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 26910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6444864270\n",
            "[TOP] exp_trunc[9] = 393363\n",
            "[TOP] exp_accum_input[9] = 24585\n",
            "[TOP] x_divider = 13004\n",
            "[TOP] y_dividend = 1536\n",
            "[TOP] data_out[9] = 967\n",
            "[TOP] exp_fraction_output[9] = 21042\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5039495874\n",
            "[TOP] exp_trunc[9] = 307586\n",
            "[TOP] exp_accum_input[9] = 19224\n",
            "[TOP] x_divider = 15418\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 639\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1551634766\n",
            "[TOP] exp_trunc[9] = 94704\n",
            "[TOP] exp_accum_input[9] = 5919\n",
            "[TOP] x_divider = 6579\n",
            "[TOP] y_dividend = 369\n",
            "[TOP] data_out[9] = 457\n",
            "[TOP] exp_fraction_output[9] = 19103\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4575111191\n",
            "[TOP] exp_trunc[9] = 279242\n",
            "[TOP] exp_accum_input[9] = 17452\n",
            "[TOP] x_divider = 14043\n",
            "[TOP] y_dividend = 1090\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12901\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 21849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5232769953\n",
            "[TOP] exp_trunc[9] = 319382\n",
            "[TOP] exp_accum_input[9] = 19961\n",
            "[TOP] x_divider = 12718\n",
            "[TOP] y_dividend = 1247\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 493\n",
            "[TOP] exp_fraction_output[9] = 17358\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4157188926\n",
            "[TOP] exp_trunc[9] = 253734\n",
            "[TOP] exp_accum_input[9] = 15858\n",
            "[TOP] x_divider = 14744\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 22095\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5291686215\n",
            "[TOP] exp_trunc[9] = 322978\n",
            "[TOP] exp_accum_input[9] = 20186\n",
            "[TOP] x_divider = 14721\n",
            "[TOP] y_dividend = 1261\n",
            "[TOP] data_out[9] = 703\n",
            "[TOP] exp_fraction_output[9] = 26061\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6241531317\n",
            "[TOP] exp_trunc[9] = 380952\n",
            "[TOP] exp_accum_input[9] = 23809\n",
            "[TOP] x_divider = 16576\n",
            "[TOP] y_dividend = 1488\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 26286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6295418142\n",
            "[TOP] exp_trunc[9] = 384241\n",
            "[TOP] exp_accum_input[9] = 24015\n",
            "[TOP] x_divider = 15505\n",
            "[TOP] y_dividend = 1500\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 18251\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4371059747\n",
            "[TOP] exp_trunc[9] = 266788\n",
            "[TOP] exp_accum_input[9] = 16674\n",
            "[TOP] x_divider = 11178\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17155\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4108571035\n",
            "[TOP] exp_trunc[9] = 250767\n",
            "[TOP] exp_accum_input[9] = 15672\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21783\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5216963151\n",
            "[TOP] exp_trunc[9] = 318418\n",
            "[TOP] exp_accum_input[9] = 19901\n",
            "[TOP] x_divider = 13081\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 24438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5852827686\n",
            "[TOP] exp_trunc[9] = 357228\n",
            "[TOP] exp_accum_input[9] = 22326\n",
            "[TOP] x_divider = 12876\n",
            "[TOP] y_dividend = 1395\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 22203\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5317551891\n",
            "[TOP] exp_trunc[9] = 324557\n",
            "[TOP] exp_accum_input[9] = 20284\n",
            "[TOP] x_divider = 11374\n",
            "[TOP] y_dividend = 1267\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20522\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914957434\n",
            "[TOP] exp_trunc[9] = 299985\n",
            "[TOP] exp_accum_input[9] = 18749\n",
            "[TOP] x_divider = 12385\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 24902\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963954294\n",
            "[TOP] exp_trunc[9] = 364010\n",
            "[TOP] exp_accum_input[9] = 22750\n",
            "[TOP] x_divider = 12596\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 925\n",
            "[TOP] exp_fraction_output[9] = 21406\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5126672782\n",
            "[TOP] exp_trunc[9] = 312907\n",
            "[TOP] exp_accum_input[9] = 19556\n",
            "[TOP] x_divider = 11043\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 19576\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4688393272\n",
            "[TOP] exp_trunc[9] = 286156\n",
            "[TOP] exp_accum_input[9] = 17884\n",
            "[TOP] x_divider = 11828\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 35963\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8613030611\n",
            "[TOP] exp_trunc[9] = 525697\n",
            "[TOP] exp_accum_input[9] = 32856\n",
            "[TOP] x_divider = 15390\n",
            "[TOP] y_dividend = 2053\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 32277\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7730244669\n",
            "[TOP] exp_trunc[9] = 471816\n",
            "[TOP] exp_accum_input[9] = 29488\n",
            "[TOP] x_divider = 12203\n",
            "[TOP] y_dividend = 1843\n",
            "[TOP] data_out[9] = 1237\n",
            "[TOP] exp_fraction_output[9] = 23105\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5533578185\n",
            "[TOP] exp_trunc[9] = 337742\n",
            "[TOP] exp_accum_input[9] = 21108\n",
            "[TOP] x_divider = 10676\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 1015\n",
            "[TOP] exp_fraction_output[9] = 30295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7255561615\n",
            "[TOP] exp_trunc[9] = 442844\n",
            "[TOP] exp_accum_input[9] = 27677\n",
            "[TOP] x_divider = 11700\n",
            "[TOP] y_dividend = 1729\n",
            "[TOP] data_out[9] = 1211\n",
            "[TOP] exp_fraction_output[9] = 22467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5380779099\n",
            "[TOP] exp_trunc[9] = 328416\n",
            "[TOP] exp_accum_input[9] = 20526\n",
            "[TOP] x_divider = 11189\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 26878\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6437200366\n",
            "[TOP] exp_trunc[9] = 392895\n",
            "[TOP] exp_accum_input[9] = 24555\n",
            "[TOP] x_divider = 10882\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 1153\n",
            "[TOP] exp_fraction_output[9] = 22050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5280908850\n",
            "[TOP] exp_trunc[9] = 322321\n",
            "[TOP] exp_accum_input[9] = 20145\n",
            "[TOP] x_divider = 11313\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 30634\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7336751098\n",
            "[TOP] exp_trunc[9] = 447799\n",
            "[TOP] exp_accum_input[9] = 27987\n",
            "[TOP] x_divider = 12590\n",
            "[TOP] y_dividend = 1749\n",
            "[TOP] data_out[9] = 1137\n",
            "[TOP] exp_fraction_output[9] = 35180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8425504460\n",
            "[TOP] exp_trunc[9] = 514251\n",
            "[TOP] exp_accum_input[9] = 32140\n",
            "[TOP] x_divider = 12922\n",
            "[TOP] y_dividend = 2008\n",
            "[TOP] data_out[9] = 1273\n",
            "[TOP] exp_fraction_output[9] = 17157\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4109050029\n",
            "[TOP] exp_trunc[9] = 250796\n",
            "[TOP] exp_accum_input[9] = 15674\n",
            "[TOP] x_divider = 10220\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19405\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4647439285\n",
            "[TOP] exp_trunc[9] = 283657\n",
            "[TOP] exp_accum_input[9] = 17728\n",
            "[TOP] x_divider = 11390\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10802\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 17255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4132520735\n",
            "[TOP] exp_trunc[9] = 252229\n",
            "[TOP] exp_accum_input[9] = 15764\n",
            "[TOP] x_divider = 10370\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10791\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 18706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4480030882\n",
            "[TOP] exp_trunc[9] = 273439\n",
            "[TOP] exp_accum_input[9] = 17089\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 22063\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5284022311\n",
            "[TOP] exp_trunc[9] = 322511\n",
            "[TOP] exp_accum_input[9] = 20156\n",
            "[TOP] x_divider = 11646\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 20899\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5005247803\n",
            "[TOP] exp_trunc[9] = 305496\n",
            "[TOP] exp_accum_input[9] = 19093\n",
            "[TOP] x_divider = 11405\n",
            "[TOP] y_dividend = 1193\n",
            "[TOP] data_out[9] = 857\n",
            "[TOP] exp_fraction_output[9] = 18669\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4471169493\n",
            "[TOP] exp_trunc[9] = 272898\n",
            "[TOP] exp_accum_input[9] = 17056\n",
            "[TOP] x_divider = 10642\n",
            "[TOP] y_dividend = 1066\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10875\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17654\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4228080038\n",
            "[TOP] exp_trunc[9] = 258061\n",
            "[TOP] exp_accum_input[9] = 16128\n",
            "[TOP] x_divider = 10935\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 21467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5141282099\n",
            "[TOP] exp_trunc[9] = 313798\n",
            "[TOP] exp_accum_input[9] = 19612\n",
            "[TOP] x_divider = 12696\n",
            "[TOP] y_dividend = 1225\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19676\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4712342972\n",
            "[TOP] exp_trunc[9] = 287618\n",
            "[TOP] exp_accum_input[9] = 17976\n",
            "[TOP] x_divider = 10868\n",
            "[TOP] y_dividend = 1123\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 19307\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4623968579\n",
            "[TOP] exp_trunc[9] = 282224\n",
            "[TOP] exp_accum_input[9] = 17639\n",
            "[TOP] x_divider = 12526\n",
            "[TOP] y_dividend = 1102\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 18101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4335135197\n",
            "[TOP] exp_trunc[9] = 264595\n",
            "[TOP] exp_accum_input[9] = 16537\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 613\n",
            "[TOP] exp_fraction_output[9] = 26035\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6235304395\n",
            "[TOP] exp_trunc[9] = 380572\n",
            "[TOP] exp_accum_input[9] = 23785\n",
            "[TOP] x_divider = 15724\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25004\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5988382988\n",
            "[TOP] exp_trunc[9] = 365501\n",
            "[TOP] exp_accum_input[9] = 22843\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1427\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 26321\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6303800537\n",
            "[TOP] exp_trunc[9] = 384753\n",
            "[TOP] exp_accum_input[9] = 24047\n",
            "[TOP] x_divider = 14992\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 20624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4939386128\n",
            "[TOP] exp_trunc[9] = 301476\n",
            "[TOP] exp_accum_input[9] = 18842\n",
            "[TOP] x_divider = 11761\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20166\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4829696502\n",
            "[TOP] exp_trunc[9] = 294781\n",
            "[TOP] exp_accum_input[9] = 18423\n",
            "[TOP] x_divider = 10880\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 23156\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5545792532\n",
            "[TOP] exp_trunc[9] = 338488\n",
            "[TOP] exp_accum_input[9] = 21155\n",
            "[TOP] x_divider = 11724\n",
            "[TOP] y_dividend = 1322\n",
            "[TOP] data_out[9] = 923\n",
            "[TOP] exp_fraction_output[9] = 23385\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5600637345\n",
            "[TOP] exp_trunc[9] = 341835\n",
            "[TOP] exp_accum_input[9] = 21364\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1335\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 11092\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 17708\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4241012876\n",
            "[TOP] exp_trunc[9] = 258850\n",
            "[TOP] exp_accum_input[9] = 16178\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11296\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 18255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4372017735\n",
            "[TOP] exp_trunc[9] = 266846\n",
            "[TOP] exp_accum_input[9] = 16677\n",
            "[TOP] x_divider = 10387\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 19926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4772217222\n",
            "[TOP] exp_trunc[9] = 291273\n",
            "[TOP] exp_accum_input[9] = 18204\n",
            "[TOP] x_divider = 11048\n",
            "[TOP] y_dividend = 1137\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17804\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4264004588\n",
            "[TOP] exp_trunc[9] = 260254\n",
            "[TOP] exp_accum_input[9] = 16265\n",
            "[TOP] x_divider = 12129\n",
            "[TOP] y_dividend = 1016\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 19811\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4744675067\n",
            "[TOP] exp_trunc[9] = 289591\n",
            "[TOP] exp_accum_input[9] = 18099\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1131\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 10601\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 22496\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5387724512\n",
            "[TOP] exp_trunc[9] = 328840\n",
            "[TOP] exp_accum_input[9] = 20552\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 18107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4336572179\n",
            "[TOP] exp_trunc[9] = 264683\n",
            "[TOP] exp_accum_input[9] = 16542\n",
            "[TOP] x_divider = 12238\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10968\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 24557\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5881327829\n",
            "[TOP] exp_trunc[9] = 358967\n",
            "[TOP] exp_accum_input[9] = 22435\n",
            "[TOP] x_divider = 12595\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 911\n",
            "[TOP] exp_fraction_output[9] = 18136\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4343517592\n",
            "[TOP] exp_trunc[9] = 265107\n",
            "[TOP] exp_accum_input[9] = 16569\n",
            "[TOP] x_divider = 11755\n",
            "[TOP] y_dividend = 1035\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 17764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254424708\n",
            "[TOP] exp_trunc[9] = 259669\n",
            "[TOP] exp_accum_input[9] = 16229\n",
            "[TOP] x_divider = 11186\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 18154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4347828538\n",
            "[TOP] exp_trunc[9] = 265370\n",
            "[TOP] exp_accum_input[9] = 16585\n",
            "[TOP] x_divider = 10758\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10911\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18378\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4401475866\n",
            "[TOP] exp_trunc[9] = 268644\n",
            "[TOP] exp_accum_input[9] = 16790\n",
            "[TOP] x_divider = 10707\n",
            "[TOP] y_dividend = 1049\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 23493\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626503021\n",
            "[TOP] exp_trunc[9] = 343414\n",
            "[TOP] exp_accum_input[9] = 21463\n",
            "[TOP] x_divider = 15737\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 18876\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4520745372\n",
            "[TOP] exp_trunc[9] = 275924\n",
            "[TOP] exp_accum_input[9] = 17245\n",
            "[TOP] x_divider = 13422\n",
            "[TOP] y_dividend = 1077\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 15293\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 611\n",
            "[TOP] exp_fraction_output[9] = 21812\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5223908564\n",
            "[TOP] exp_trunc[9] = 318842\n",
            "[TOP] exp_accum_input[9] = 19927\n",
            "[TOP] x_divider = 14613\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17478\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185928566\n",
            "[TOP] exp_trunc[9] = 255488\n",
            "[TOP] exp_accum_input[9] = 15968\n",
            "[TOP] x_divider = 12454\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 17486\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4187844542\n",
            "[TOP] exp_trunc[9] = 255605\n",
            "[TOP] exp_accum_input[9] = 15975\n",
            "[TOP] x_divider = 12297\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 20813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4984651061\n",
            "[TOP] exp_trunc[9] = 304238\n",
            "[TOP] exp_accum_input[9] = 19014\n",
            "[TOP] x_divider = 12933\n",
            "[TOP] y_dividend = 1188\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4993272953\n",
            "[TOP] exp_trunc[9] = 304765\n",
            "[TOP] exp_accum_input[9] = 19047\n",
            "[TOP] x_divider = 12914\n",
            "[TOP] y_dividend = 1190\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 17167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4111444999\n",
            "[TOP] exp_trunc[9] = 250942\n",
            "[TOP] exp_accum_input[9] = 15683\n",
            "[TOP] x_divider = 11978\n",
            "[TOP] y_dividend = 980\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 19967\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4782036599\n",
            "[TOP] exp_trunc[9] = 291872\n",
            "[TOP] exp_accum_input[9] = 18242\n",
            "[TOP] x_divider = 12874\n",
            "[TOP] y_dividend = 1140\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 18599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4454404703\n",
            "[TOP] exp_trunc[9] = 271875\n",
            "[TOP] exp_accum_input[9] = 16992\n",
            "[TOP] x_divider = 13205\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20372\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4879032884\n",
            "[TOP] exp_trunc[9] = 297792\n",
            "[TOP] exp_accum_input[9] = 18612\n",
            "[TOP] x_divider = 10928\n",
            "[TOP] y_dividend = 1163\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 24270\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5812592190\n",
            "[TOP] exp_trunc[9] = 354772\n",
            "[TOP] exp_accum_input[9] = 22173\n",
            "[TOP] x_divider = 11951\n",
            "[TOP] y_dividend = 1385\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 18605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455841685\n",
            "[TOP] exp_trunc[9] = 271962\n",
            "[TOP] exp_accum_input[9] = 16997\n",
            "[TOP] x_divider = 11656\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20502\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4910167494\n",
            "[TOP] exp_trunc[9] = 299692\n",
            "[TOP] exp_accum_input[9] = 18730\n",
            "[TOP] x_divider = 11903\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10881\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4965491301\n",
            "[TOP] exp_trunc[9] = 303069\n",
            "[TOP] exp_accum_input[9] = 18941\n",
            "[TOP] x_divider = 11822\n",
            "[TOP] y_dividend = 1183\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 17456\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4180659632\n",
            "[TOP] exp_trunc[9] = 255167\n",
            "[TOP] exp_accum_input[9] = 15947\n",
            "[TOP] x_divider = 11571\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 18357\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396446429\n",
            "[TOP] exp_trunc[9] = 268337\n",
            "[TOP] exp_accum_input[9] = 16771\n",
            "[TOP] x_divider = 10651\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4230954002\n",
            "[TOP] exp_trunc[9] = 258236\n",
            "[TOP] exp_accum_input[9] = 16139\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 12828\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4680489871\n",
            "[TOP] exp_trunc[9] = 285674\n",
            "[TOP] exp_accum_input[9] = 17854\n",
            "[TOP] x_divider = 11206\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 18359\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396925423\n",
            "[TOP] exp_trunc[9] = 268367\n",
            "[TOP] exp_accum_input[9] = 16772\n",
            "[TOP] x_divider = 10786\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14967\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 24567\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5883722799\n",
            "[TOP] exp_trunc[9] = 359113\n",
            "[TOP] exp_accum_input[9] = 22444\n",
            "[TOP] x_divider = 15260\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17301\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4143537597\n",
            "[TOP] exp_trunc[9] = 252901\n",
            "[TOP] exp_accum_input[9] = 15806\n",
            "[TOP] x_divider = 12146\n",
            "[TOP] y_dividend = 987\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 12600\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6208720228\n",
            "[TOP] exp_trunc[9] = 378950\n",
            "[TOP] exp_accum_input[9] = 23684\n",
            "[TOP] x_divider = 16238\n",
            "[TOP] y_dividend = 1480\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 19062\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4565291814\n",
            "[TOP] exp_trunc[9] = 278643\n",
            "[TOP] exp_accum_input[9] = 17415\n",
            "[TOP] x_divider = 11347\n",
            "[TOP] y_dividend = 1088\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 20415\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4889331255\n",
            "[TOP] exp_trunc[9] = 298421\n",
            "[TOP] exp_accum_input[9] = 18651\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1165\n",
            "[TOP] data_out[9] = 861\n",
            "[TOP] exp_fraction_output[9] = 18636\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4463266092\n",
            "[TOP] exp_trunc[9] = 272416\n",
            "[TOP] exp_accum_input[9] = 17026\n",
            "[TOP] x_divider = 12609\n",
            "[TOP] y_dividend = 1064\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11264\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 19370\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4639056890\n",
            "[TOP] exp_trunc[9] = 283145\n",
            "[TOP] exp_accum_input[9] = 17696\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1106\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4315256946\n",
            "[TOP] exp_trunc[9] = 263382\n",
            "[TOP] exp_accum_input[9] = 16461\n",
            "[TOP] x_divider = 11415\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12660\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 19701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4718330397\n",
            "[TOP] exp_trunc[9] = 287984\n",
            "[TOP] exp_accum_input[9] = 17999\n",
            "[TOP] x_divider = 10867\n",
            "[TOP] y_dividend = 1124\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 21765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5212652205\n",
            "[TOP] exp_trunc[9] = 318155\n",
            "[TOP] exp_accum_input[9] = 19884\n",
            "[TOP] x_divider = 13401\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 22248\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5328329256\n",
            "[TOP] exp_trunc[9] = 325215\n",
            "[TOP] exp_accum_input[9] = 20325\n",
            "[TOP] x_divider = 15274\n",
            "[TOP] y_dividend = 1270\n",
            "[TOP] data_out[9] = 681\n",
            "[TOP] exp_fraction_output[9] = 20653\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4946331541\n",
            "[TOP] exp_trunc[9] = 301900\n",
            "[TOP] exp_accum_input[9] = 18868\n",
            "[TOP] x_divider = 14548\n",
            "[TOP] y_dividend = 1179\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 17236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4127970292\n",
            "[TOP] exp_trunc[9] = 251951\n",
            "[TOP] exp_accum_input[9] = 15746\n",
            "[TOP] x_divider = 12702\n",
            "[TOP] y_dividend = 984\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 20799\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4981298103\n",
            "[TOP] exp_trunc[9] = 304034\n",
            "[TOP] exp_accum_input[9] = 19002\n",
            "[TOP] x_divider = 13789\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 11303\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22068\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5285219796\n",
            "[TOP] exp_trunc[9] = 322584\n",
            "[TOP] exp_accum_input[9] = 20161\n",
            "[TOP] x_divider = 14163\n",
            "[TOP] y_dividend = 1260\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 19646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4705158062\n",
            "[TOP] exp_trunc[9] = 287180\n",
            "[TOP] exp_accum_input[9] = 17948\n",
            "[TOP] x_divider = 14401\n",
            "[TOP] y_dividend = 1121\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 24771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5932580187\n",
            "[TOP] exp_trunc[9] = 362095\n",
            "[TOP] exp_accum_input[9] = 22630\n",
            "[TOP] x_divider = 18996\n",
            "[TOP] y_dividend = 1414\n",
            "[TOP] data_out[9] = 609\n",
            "[TOP] exp_fraction_output[9] = 17149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4107134053\n",
            "[TOP] exp_trunc[9] = 250679\n",
            "[TOP] exp_accum_input[9] = 15667\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 29382\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7036900854\n",
            "[TOP] exp_trunc[9] = 429498\n",
            "[TOP] exp_accum_input[9] = 26843\n",
            "[TOP] x_divider = 20285\n",
            "[TOP] y_dividend = 1677\n",
            "[TOP] data_out[9] = 677\n",
            "[TOP] exp_fraction_output[9] = 20694\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4956150918\n",
            "[TOP] exp_trunc[9] = 302499\n",
            "[TOP] exp_accum_input[9] = 18906\n",
            "[TOP] x_divider = 14997\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 643\n",
            "[TOP] exp_fraction_output[9] = 19662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4708990014\n",
            "[TOP] exp_trunc[9] = 287413\n",
            "[TOP] exp_accum_input[9] = 17963\n",
            "[TOP] x_divider = 15698\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 585\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12362\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 14012\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13550\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 591\n",
            "[TOP] exp_fraction_output[9] = 21167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5069432999\n",
            "[TOP] exp_trunc[9] = 309413\n",
            "[TOP] exp_accum_input[9] = 19338\n",
            "[TOP] x_divider = 14782\n",
            "[TOP] y_dividend = 1208\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4259933139\n",
            "[TOP] exp_trunc[9] = 260005\n",
            "[TOP] exp_accum_input[9] = 16250\n",
            "[TOP] x_divider = 14554\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 20968\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5021773096\n",
            "[TOP] exp_trunc[9] = 306504\n",
            "[TOP] exp_accum_input[9] = 19156\n",
            "[TOP] x_divider = 14328\n",
            "[TOP] y_dividend = 1197\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20668\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949923996\n",
            "[TOP] exp_trunc[9] = 302119\n",
            "[TOP] exp_accum_input[9] = 18882\n",
            "[TOP] x_divider = 16874\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 573\n",
            "[TOP] exp_fraction_output[9] = 18603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455362691\n",
            "[TOP] exp_trunc[9] = 271933\n",
            "[TOP] exp_accum_input[9] = 16995\n",
            "[TOP] x_divider = 13297\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 18579\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4449614763\n",
            "[TOP] exp_trunc[9] = 271582\n",
            "[TOP] exp_accum_input[9] = 16973\n",
            "[TOP] x_divider = 10571\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 10815\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12919\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 853\n",
            "[TOP] exp_fraction_output[9] = 18064\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4326273808\n",
            "[TOP] exp_trunc[9] = 264054\n",
            "[TOP] exp_accum_input[9] = 16503\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1031\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 12583\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19412\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4649115764\n",
            "[TOP] exp_trunc[9] = 283759\n",
            "[TOP] exp_accum_input[9] = 17734\n",
            "[TOP] x_divider = 10685\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 22334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5348925998\n",
            "[TOP] exp_trunc[9] = 326472\n",
            "[TOP] exp_accum_input[9] = 20404\n",
            "[TOP] x_divider = 10855\n",
            "[TOP] y_dividend = 1275\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5866239518\n",
            "[TOP] exp_trunc[9] = 358046\n",
            "[TOP] exp_accum_input[9] = 22377\n",
            "[TOP] x_divider = 11899\n",
            "[TOP] y_dividend = 1398\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4221374122\n",
            "[TOP] exp_trunc[9] = 257652\n",
            "[TOP] exp_accum_input[9] = 16103\n",
            "[TOP] x_divider = 11279\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 729\n",
            "입력 문장: This is a grammatically acceptable for sentence.\n",
            "예측된 클래스: 0\n",
            "예측 확률: [[0.56147647 0.43852356]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 encoder 내 각 레이어의 self-attention 모듈 타입을 출력하여\n",
        "# 수정된 BertSelfAttentionModified가 적용되었는지 확인합니다.\n",
        "print(\"수정된 Attention Layers 확인:\")\n",
        "for i, layer in enumerate(model.bert.encoder.layer):\n",
        "    attn_layer = layer.attention.self\n",
        "    print(f\"Layer {i} self-attention layer type: {type(attn_layer)}\")\n"
      ],
      "metadata": {
        "id": "8WUtGKLuoW2z",
        "outputId": "c305e0a3-fa6f-4c90-c148-1bf1aa178885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수정된 Attention Layers 확인:\n",
            "Layer 0 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 1 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 2 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 3 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 4 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 5 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 6 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 7 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 8 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 9 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 10 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 11 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 디버깅용 잡동사니코드"
      ],
      "metadata": {
        "id": "H5iP5hhyJuf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\"[TOP] exp_fraction_output[{i}] = {exp_fraction_output[i]}\")\n",
        "    print(f\"[TOP] exp_int_output[{i}] = {exp_int_output[i]}\")\n",
        "    print(f\"[TOP] exp_whole[{i}] = {exp_whole[i]}\")\n",
        "    print(f\"[TOP] exp_trunc[{i}] = {exp_trunc[i]}\")\n",
        "    print(f\"[TOP] exp_accum_input[{i}] = {exp_accum_input[i]}\")\n",
        "    print(f\"[TOP] x_divider = {x_divider[0]}\")\n",
        "    print(f\"[TOP] y_dividend = {y_dividend[i]}\")\n",
        "    print(f\"[TOP] data_out[{i}] = {data_out[i]}\")"
      ],
      "metadata": {
        "id": "rAamcoXWSjnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # ✅ 디버깅용 Q, K 체크\n",
        "        with torch.no_grad():\n",
        "            h = hidden_states.detach().cpu().numpy()\n",
        "            print(\"=== Hidden States 디버깅 ===\")\n",
        "            print(\"min:\", np.min(h), \"max:\", np.max(h))\n",
        "            print(\"NaN 수:\", np.isnan(h).sum(), \"Inf 수:\", np.isinf(h).sum())\n",
        "            q = query_layer.detach().cpu().numpy()\n",
        "            k = key_layer.detach().cpu().numpy()\n",
        "            print(\"=== Q, K 디버깅 ===\")\n",
        "            print(\"Q min/max:\", np.min(q), np.max(q))\n",
        "            print(\"K min/max:\", np.min(k), np.max(k))\n",
        "            print(\"Q NaN 수:\", np.isnan(q).sum(), \"K NaN 수:\", np.isnan(k).sum())\n",
        "            print(\"Q Inf 수:\", np.isinf(q).sum(), \"K Inf 수:\", np.isinf(k).sum())\n",
        "\n",
        "        # attention score 계산 후 clamp\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 계산 직후 ===\")\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item(), \"Inf 수:\", torch.isinf(attention_scores).sum().item())\n",
        "        attention_scores = torch.clamp(attention_scores, min=-10.0, max=10.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 예시 ===\")\n",
        "            print(attention_scores[0, 0, 0, :10])\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item())\n",
        "        # CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "\n",
        "# 2. BertLayerWithNaNCheck: layer 0 내부 모듈 NaN 추적\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n",
        "\n",
        "\n",
        "# 3. BertEncoderModified\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "        self.layer[0] = BertLayerWithNaNCheck(self.layer[0])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_values=None, use_cache=False, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=True):\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            with torch.no_grad():\n",
        "                hs = hidden_states.detach().cpu().numpy()\n",
        "                if np.isnan(hs).sum() > 0:\n",
        "                    print(f\"[NaN DETECTED] ❌ in hidden_states BEFORE layer {i}\")\n",
        "                else:\n",
        "                    print(f\"[OK] ✅ hidden_states BEFORE layer {i}\")\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                head_mask[i] if head_mask is not None else None,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_values[i] if past_key_values is not None else None,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        return (hidden_states,)\n",
        "\n",
        "\n",
        "# 4. BertWithModifiedAttentionForClassification\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 5. Load config and model\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "dTD-BGlHKAh5",
        "outputId": "383af56c-e173-4f09-db40-610c833f616d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayerWithNaNCheck(\n",
              "          (layer): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttentionModified(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-11): 11 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def predict_sentence(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_class = np.argmax(probs)\n",
        "\n",
        "    return pred_class, probs\n"
      ],
      "metadata": {
        "id": "sgc6czs9DqMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_nan(module, input, output):\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        if torch.isnan(output).any():\n",
        "            print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "    elif isinstance(output, (tuple, list)):\n",
        "        for o in output:\n",
        "            if torch.is_tensor(o) and torch.isnan(o).any():\n",
        "                print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    module.register_forward_hook(check_for_nan)\n"
      ],
      "metadata": {
        "id": "2QHFLakIFV8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        # Attention\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        # Intermediate\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        # Output\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n"
      ],
      "metadata": {
        "id": "gAA7SyorIaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentence = \"The cat is sitting on the mat.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "\n",
        "print(\"문장:\", sentence)\n",
        "print(\"예측 클래스:\", pred_class)\n",
        "print(\"클래스별 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "yv0VeRUlDpLp",
        "outputId": "1cd74ea5-ca18-4524-d57c-bed12d1b5f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] ✅ hidden_states BEFORE layer 0\n",
            "=== [Layer 0] BEFORE ===\n",
            "min: -4.571415901184082 max: 3.8990025520324707\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Hidden States 디버깅 ===\n",
            "min: -4.571416 max: 3.8990026\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: -2.1867194 2.275151\n",
            "K min/max: -2.4105966 2.2897592\n",
            "Q NaN 수: 0 K NaN 수: 0\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([ 0.4173, -0.1602, -0.0771,  0.3595,  0.1112, -0.1229, -0.2256, -0.0283,\n",
            "        -0.1147, -0.0984], device='cuda:0')\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0\n",
            "[NaN DETECTED] ❌ after Attention in Layer 0\n",
            "[NaN DETECTED] ❌ after Intermediate in Layer 0\n",
            "[NaN DETECTED] ❌ after Output in Layer 0\n",
            "=== [Layer 0] AFTER ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "[NaN DETECTED] ❌ in hidden_states BEFORE layer 1\n",
            "=== Hidden States 디버깅 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: nan nan\n",
            "K min/max: nan nan\n",
            "Q NaN 수: 49152 K NaN 수: 49152\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
            "min: nan max: nan\n",
            "NaN 수: 49152\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "float_to_fixed_point() - NaN 또는 무한대 값 감지: nan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-8b7cffed0231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The cat is sitting on the mat.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"문장:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-c78350183b4c>\u001b[0m in \u001b[0;36mpredict_sentence\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[OK] ✅ hidden_states BEFORE layer {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN 수:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcordic_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_1200_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop_1200_input\u001b[0;34m(attention_scores)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mresult_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#20bit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdata_22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# ✅ NaN 또는 Inf 값 체크 후 예외 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# ✅ **최대/최소 값 제한 (7비트 표현 범위)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: float_to_fixed_point() - NaN 또는 무한대 값 감지: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        batch_size, num_heads, seq_len, _ = attention_scores.shape\n",
        "        for b in range(batch_size):\n",
        "          for h in range(num_heads):\n",
        "            for row in range(seq_len):\n",
        "              float_row = attention_scores[b, h, row, :10].detach().cpu().numpy().tolist()\n",
        "\n",
        "              if any(np.isnan(f) for f in float_row):\n",
        "                print(f\"[NaN] b={b}, head={h}, row={row} - 입력에 NaN 있음! ❌\")\n",
        "                continue  # 이건 skip하고 다음으로\n",
        "              try:\n",
        "                cordic_attention = top(*float_row)\n",
        "              except Exception as e:\n",
        "                print(f\"[ERROR] top() 실패: {e}\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          sample_row = attention_scores[0, 0, 0, :10]  # [10개 float]\n",
        "          float_row = sample_row.detach().cpu().numpy().tolist()\n",
        "          print(\"[DEBUG] top 입력값:\", float_row)\n",
        "\n",
        "          try:\n",
        "            top_result = top(*float_row)\n",
        "            print(\"[DEBUG] top 출력값:\", top_result)\n",
        "          except Exception as e:\n",
        "            print(\"[ERROR] top에서 예외 발생:\", e)\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top(*float_row)\n",
        "\n",
        "        if any(np.isnan(c) for c in cordic_attention):\n",
        "          print(f\"[NaN DETECTED] ❌ top() 결과에 NaN 존재! input: {float_row}\")\n",
        "\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # ⚠️ 먼저 너의 원본 문장 리스트가 있어야 해!\n",
        "# 예: dataset_sentences = [\"문장1\", \"문장2\", ..., \"문장N\"]\n",
        "        # 🧪 validation_dataloader에서 원문 문장 추출 (예시)\n",
        "        dataset_sentences = []\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "          input_ids = batch[0]\n",
        "          for ids in input_ids:\n",
        "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            dataset_sentences.append(text)\n",
        "\n",
        "        for i, sentence in enumerate(dataset_sentences):\n",
        "          try:\n",
        "            pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "          except Exception as e:\n",
        "            print(f\"[❌ NaN 발생] 문장 index = {i}\")\n",
        "            print(\"문장 내용:\", sentence)\n",
        "            print(\"에러 메시지:\", e)\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "_dPquKTIGbOE",
        "outputId": "54615d57-7130-49f1-b469-d5b229ca4e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 적용 모델 validation"
      ],
      "metadata": {
        "id": "JSRL23_d7voR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "metadata": {
        "id": "h4m08g6HohMF",
        "outputId": "1644ccf7-52ee-466d-92bd-1fbd4b207b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "float_to_fixed_point() - NaN 또는 무한대 값 감지: nan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e22c35b96d8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# The documentation for this `model` function is here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         outputs = model(b_input_ids,\n\u001b[0m\u001b[1;32m     41\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         attention_mask=b_input_mask)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6e3b41572d66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# CORDIC 방식으로 softmax 대체\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcordic_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_1200_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcordic_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mtop_1200_input\u001b[0;34m(attention_scores)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mresult_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#20bit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdata_22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# ✅ NaN 또는 Inf 값 체크 후 예외 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# ✅ **최대/최소 값 제한 (7비트 표현 범위)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: float_to_fixed_point() - NaN 또는 무한대 값 감지: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg-qog2IyzCc",
        "outputId": "84868b88-7f46-4258-8f8c-8f39d188821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 저장 및 로드 코드.\n",
        "path = '/content/model/'\n",
        "torch.save(model.state_dict(), path+\"CoLA.pt\")\n",
        "model.load_state_dict(torch.load(path+\"CoLA.pt\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6f3ddQs0x7V-",
        "outputId": "917276df-e585-497f-b9f8-9b06944ce3af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}