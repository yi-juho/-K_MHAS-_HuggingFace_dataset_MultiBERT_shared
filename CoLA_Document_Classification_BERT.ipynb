{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLA_Document_Classification_BERT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yi-juho/-K_MHAS-_HuggingFace_dataset_MultiBERT_shared/blob/main/CoLA_Document_Classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Classification BERT [CoLA]\n",
        "\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SGcOaw5P769d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg-qog2IyzCc",
        "outputId": "27a33dcf-5591-498d-a6fc-1a78c6273a00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cordic"
      ],
      "metadata": {
        "id": "x6iTkvfbQ6Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def top_1200_input(attention_scores, dim=-1):\n",
        "    \"\"\"\n",
        "    1200개의 attention_scores를 120개의 리스트로 나누어 각 리스트를 top 함수에 전달\n",
        "    \"\"\"\n",
        "    chunk_size = len(attention_scores) // dim\n",
        "    data_list = [attention_scores[i * chunk_size:(i + 1) * chunk_size] for i in range(120)]\n",
        "\n",
        "    # 120개의 결과 리스트 생성\n",
        "    result_arrays = []\n",
        "\n",
        "    # 각 10개씩 top 함수에 전달\n",
        "    for i in range(120):\n",
        "        result = top(*data_list[i])  # 리스트를 개별 인자로 풀어서 전달\n",
        "        result_arrays.append(result)  # 결과 저장\n",
        "\n",
        "    return result_arrays\n",
        "\n",
        "\n",
        "\n",
        "def float_to_fixed_point(value, int_bits=7, frac_bits=13):\n",
        "    \"\"\"\n",
        "    실수를 32비트 Fixed-Point 바이너리로 변환하는 함수.\n",
        "    - 31~20 비트: 항상 0 (상위 12비트)\n",
        "    - 19~13 비트: 정수부 (7비트, Signed, 2의 보수)\n",
        "    - 12~0 비트: 소수부 (13비트, 항상 양수)\n",
        "\n",
        "    정수부는 내림하여 변환하고, 소수부를 정확한 2진 변환 방식으로 변환.\n",
        "    \"\"\"\n",
        "    max_frac_value = 2**frac_bits  # 13비트 소수부 정밀도 (8192)\n",
        "\n",
        "    # ✅ 1. 정수부와 소수부 정확하게 분리 (내림 적용)\n",
        "    int_part = np.floor(value).astype(int)\n",
        "    frac_part = abs(value - int_part)  # 소수부 절댓값 유지\n",
        "\n",
        "    # ✅ 2. 정수부(7비트) 변환 (2의 보수 변환)\n",
        "    if int_part < 0:\n",
        "        int_binary = format((1 << int_bits) + int_part, f'0{int_bits}b')  # 7비트 2의 보수 변환\n",
        "    else:\n",
        "        int_binary = format(int_part, f'0{int_bits}b')  # 7비트 양수 표현\n",
        "\n",
        "    # ✅ 3. 앞 12비트는 항상 0으로 유지\n",
        "    int_binary = \"0\" * 12 + int_binary\n",
        "\n",
        "    # ✅ 4. 소수부 변환 (부동소수점 오차 최소화)\n",
        "    frac_binary = \"\"\n",
        "    frac_value = frac_part  # 소수부 값 유지\n",
        "    for _ in range(frac_bits):  # 13비트 반복\n",
        "        frac_value *= 2\n",
        "        if frac_value >= 1:\n",
        "            frac_binary += \"1\"\n",
        "            frac_value -= 1\n",
        "        else:\n",
        "            frac_binary += \"0\"\n",
        "\n",
        "    # ✅ 5. 최종 32비트 바이너리 생성\n",
        "    fixed_binary = int_binary + frac_binary\n",
        "    fixed_binary_int = int(fixed_binary, 2)\n",
        "    lower_20_bits = fixed_binary_int & 0xFFFFF\n",
        "    return lower_20_bits\n",
        "\n",
        "def binary_to_decimal(binary_str):\n",
        "    # 16비트 중 앞 3비트는 정수부, 뒤 13비트는 소수부\n",
        "    int_part = int(binary_str[:3], 2)  # 정수부\n",
        "    frac_part = int(binary_str[3:], 2) / (2 ** 13)  # 소수부를 2^13으로 나눔\n",
        "    return int_part + frac_part  # 정수부와 소수부 합산\n",
        "\n",
        "def hex_to_binary(hex_str):\n",
        "    \"\"\"\n",
        "    16비트 헥스 값을 16비트 바이너리 문자열로 변환하는 함수\n",
        "\n",
        "    :param hex_str: 변환할 16비트 헥스 값 (예: \"1A2F\" 또는 0x1A2F)\n",
        "    :return: 16비트 바이너리 문자열 (예: \"0001101000101111\")\n",
        "    \"\"\"\n",
        "    # hex_str가 정수형일 경우 문자열로 변환\n",
        "    if isinstance(hex_str, int):\n",
        "        hex_str = format(hex_str, 'X')  # 16진수 대문자로 변환\n",
        "\n",
        "    # 16진수를 10진수로 변환 후, 16비트 이진수로 변환\n",
        "    binary_str = bin(int(hex_str, 16))[2:].zfill(16)\n",
        "    return binary_str\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "def top(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10) : #20bit data\n",
        "\n",
        "    data_11 = float_to_fixed_point(data_1)\n",
        "    data_22 = float_to_fixed_point(data_2)\n",
        "    data_33 = float_to_fixed_point(data_3)\n",
        "    data_44 = float_to_fixed_point(data_4)\n",
        "    data_55 = float_to_fixed_point(data_5)\n",
        "    data_66 = float_to_fixed_point(data_6)\n",
        "    data_77 = float_to_fixed_point(data_7)\n",
        "    data_88 = float_to_fixed_point(data_8)\n",
        "    data_99 = float_to_fixed_point(data_9)\n",
        "    data_1010 = float_to_fixed_point(data_10)\n",
        "\n",
        "\n",
        "    data_list = [data_11, data_22, data_33, data_44, data_55, data_66, data_77, data_88, data_99, data_1010]\n",
        "    #data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    int_list = [0] * 10 #exp_int input\n",
        "    exp_int_output = [0]*10 #exp_int output\n",
        "    fraction_list=[0]*10 #cordic_exu input\n",
        "    exp_fraction_output = [0]*10 #cordic_exu output\n",
        "    exp_whole=[0]*10 #cordic_exu +cordic_int\n",
        "    exp_trunc=[0]*10\n",
        "    exp_accum_input=[0]*10 # accumulator input  (exponential)\n",
        "    exp_accum_output=[0]*1\n",
        "    x_divider = [0] * 10 #cordic diu input\n",
        "    y_dividend = [0] * 10 #cordic diu input\n",
        "    data_out = [0] * 10\n",
        "    binary_values = [0] * 10\n",
        "    int_values = [0.0] * 10\n",
        "    for i in range(10):  # int part\n",
        "        int_list[i] = data_list[i] >> 13\n",
        "    exp_int_output[0], exp_int_output[1], exp_int_output[2], exp_int_output[3], exp_int_output[4], exp_int_output[5], exp_int_output[6], exp_int_output[7], exp_int_output[8], exp_int_output[9]=exp_int(int_list[0], int_list[1], int_list[2], int_list[3], int_list[4], int_list[5], int_list[6], int_list[7], int_list[8], int_list[9])\n",
        "\n",
        "    for i in range(10):  # fraction part\n",
        "        fraction_list[i] = (data_list[i] & 0x1FFF) << 1  # 하위 13비트를 가져오고, 뒤에 0을 추가\n",
        "    exp_fraction_output[0], exp_fraction_output[1], exp_fraction_output[2], exp_fraction_output[3], exp_fraction_output[4], exp_fraction_output[5], exp_fraction_output[6], exp_fraction_output[7], exp_fraction_output[8], exp_fraction_output[9] = cordic_exu(fraction_list[0], fraction_list[1], fraction_list[2], fraction_list[3], fraction_list[4], fraction_list[5], fraction_list[6], fraction_list[7], fraction_list[8], fraction_list[9])\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_whole[i] = exp_int_output[i] * exp_fraction_output[i]\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_trunc[i] = (exp_whole[i] >> 14) & 0xFFFFF  # 33~14 비트를 추출 (20비트)\n",
        "    for i in range(10):\n",
        "       exp_accum_input[i] = (exp_trunc[i] >> 4) & 0xFFFF\n",
        "\n",
        "    exp_accum_output[0] = cordic_accumulator(exp_accum_input[0],exp_accum_input[1],exp_accum_input[2],exp_accum_input[3],exp_accum_input[4],exp_accum_input[5],exp_accum_input[6],exp_accum_input[7],exp_accum_input[8],exp_accum_input[9])\n",
        "\n",
        "\n",
        "\n",
        "    # denominator_sf[i]에서 18~4 비트 추출 후, 앞에 0을 붙여서 x_divider에 할당\n",
        "    x_divider[0] = (exp_accum_output[0] >> 4) & 0x7FFF  # 18~4 비트를 추출 (15비트)\n",
        "\n",
        "    for i in range(10):\n",
        "    # numerator_sf[i]에서 15~4 비트 추출 후, 앞에 4개의 0을 붙여서 y_dividend에 할당\n",
        "        y_dividend[i] = (exp_accum_input[i] >> 4)  # 15~4 비트를 추출 (12비트)\n",
        "\n",
        "    data_out[0],data_out[1],data_out[2],data_out[3],data_out[4],data_out[5],data_out[6],data_out[7],data_out[8],data_out[9], = cordic_diu (x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],y_dividend[0],y_dividend[1],y_dividend[2],y_dividend[3],y_dividend[4],y_dividend[5],y_dividend[6],y_dividend[7],y_dividend[8],y_dividend[9])\n",
        "\n",
        "    for i in range(10):\n",
        "        binary_values[i] = hex_to_binary(data_out[i])\n",
        "       # print(binary_values[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        int_values[i] = binary_to_decimal(binary_values[i])\n",
        "        #print(int_values[i])\n",
        "\n",
        "    return int_values[0],int_values[1],int_values[2],int_values[3],int_values[4],int_values[5],int_values[6],int_values[7],int_values[8],int_values[9]\n",
        "   #return int_values\n",
        "\n",
        "def exp_int(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_to_int = [0]*10\n",
        "    int_output=[0]*10\n",
        "    for i in range(10):\n",
        "        # 이진수로 변환하여 signed_binary_to_int 함수에 전달\n",
        "        binary_str = bin(data_list[i])[2:].zfill(7)  # 20비트 이진수 문자열로 변환\n",
        "        data_to_int[i] = signed_binary_to_int(binary_str)\n",
        "\n",
        "    i_max = max(data_to_int)\n",
        "    offset=signed_binary_to_int(bin(0b0001011-i_max)[2:].zfill(7)) #offset\n",
        "\n",
        "    for i in range(10):\n",
        "        int_reg=data_to_int[i]+offset\n",
        "        #print(int_reg)\n",
        "        if int_reg==0 :\n",
        "            int_output[i]=0b0000_0000_0000_0001_00\n",
        "        elif int_reg==1 :\n",
        "            int_output[i]=0b0000_0000_0000_0010_11\n",
        "        elif int_reg==2 :\n",
        "            int_output[i]=0b0000_0000_0000_0111_10\n",
        "        elif int_reg==3 :\n",
        "            int_output[i]=0b0000_0000_0001_0100_00\n",
        "        elif int_reg==4 :\n",
        "            int_output[i]=0b0000_0000_0011_0110_10\n",
        "        elif int_reg==5 :\n",
        "            int_output[i]=0b0000_0000_1001_0100_10\n",
        "        elif int_reg==6 :\n",
        "            int_output[i]=0b0000_0001_1001_0011_10\n",
        "        elif int_reg==7 :\n",
        "            int_output[i]=0b0000_0100_0100_1000_11\n",
        "        elif int_reg==8 :\n",
        "            int_output[i]=0b0000_1011_1010_0101_00\n",
        "        elif int_reg==9 :\n",
        "            int_output[i]=0b0001_1111_1010_0111_00\n",
        "        elif int_reg==10 :\n",
        "            int_output[i]=0b0101_0110_0000_1010_10\n",
        "        elif int_reg==11 :\n",
        "            int_output[i]=0b1110_1001_1110_0010_01\n",
        "        else :\n",
        "            int_output[i]=0\n",
        "\n",
        "    return  int_output[0], int_output[1], int_output[2], int_output[3], int_output[4], int_output[5], int_output[6], int_output[7], int_output[8], int_output[9]\n",
        "\n",
        "\n",
        "def cordic_exu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_exu_pipe(data_list[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "def cordic_exu_pipe(z_in):\n",
        "    x_1, y_1, z_1=exp_unit(0b1001101010001111, 0, z_in, 0b0010001100100111,1)\n",
        "    x_2, y_2, z_2=exp_unit(x_1, y_1, z_1,0b0001000001011000,2)\n",
        "    x_3, y_3, z_3 = exp_unit(x_2, y_2, z_2, 0b0000100000001010, 3)\n",
        "    x_4, y_4, z_4 = exp_unit(x_3, y_3, z_3, 0b0000010000000001, 4)\n",
        "    x_5, y_5, z_5 = exp_unit(x_4, y_4, z_4, 0b0000001000000000, 5)\n",
        "    x_6, y_6, z_6 = exp_unit(x_5, y_5, z_5, 0b0000000100000000, 6)\n",
        "    x_7, y_7, z_7 = exp_unit(x_6, y_6, z_6, 0b0000000010000000, 7)\n",
        "    x_8, y_8, z_8 = exp_unit(x_7, y_7, z_7, 0b0000000001000000, 8)\n",
        "    x_9, y_9, z_9 = exp_unit(x_8, y_8, z_8, 0b0000000000100000, 9)\n",
        "    x_10, y_10, z_10 = exp_unit(x_9, y_9, z_9, 0b0000000000010000, 10)\n",
        "    x_11, y_11, z_11 = exp_unit(x_10, y_10, z_10, 0b0000000000001000, 11)\n",
        "    x_12, y_12, z_12 = exp_unit(x_11, y_11, z_11, 0b0000000000000100, 12)\n",
        "    x_13, y_13, z_13 = exp_unit(x_12, y_12, z_12, 0b0000000000000010, 13)\n",
        "    x_14, y_14, z_14 = exp_unit(x_13, y_13, z_13, 0b0000000000000010, 14)\n",
        "\n",
        "\n",
        "    exp_frac_reg=x_14+y_14\n",
        "    exp_frac = (exp_frac_reg >> 1) & 0xFFFF\n",
        "    #print(bin(exp_frac))\n",
        "    return exp_frac\n",
        "\n",
        "\n",
        "\n",
        "def exp_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "    x_cal = 0\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if z_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "        # 시계방향 (downward)\n",
        "        x_cal = x_in - (y_in >> pipe_num)\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "    else:\n",
        "        # 반시계방향 (upward)\n",
        "        x_cal = x_in + (y_in >> pipe_num)\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return x_cal, y_cal, z_cal\n",
        "\n",
        "\n",
        "def signed_binary_to_int(binary_str):\n",
        "    # 이진수의 길이\n",
        "    n = len(binary_str)\n",
        "\n",
        "    # 부호 비트 확인 (최상위 비트가 1이면 음수)\n",
        "    if binary_str[0] == '1':\n",
        "        # 2의 보수 방법으로 음수 변환\n",
        "        return int(binary_str, 2) - (1 << n)\n",
        "    else:\n",
        "        # 양수는 그냥 변환\n",
        "        return int(binary_str, 2)\n",
        "\n",
        "def accumulator(sequence_in, exponential, state):\n",
        "    \"\"\"\n",
        "    입력값을 받아 sum1, sum2, denominator_sf 값을 업데이트하고 출력.\n",
        "    \"\"\"\n",
        "\n",
        "    # accumulator_SEL 토글\n",
        "    state[\"accumulator_SEL\"] = (state[\"accumulator_SEL\"] + sequence_in) & 1\n",
        "\n",
        "    # valid 시프트 레지스터 갱신\n",
        "    state[\"valid\"] = [sequence_in] + state[\"valid\"][:-1]\n",
        "\n",
        "    if state[\"accumulator_SEL\"]:  # accumulator 1 사용\n",
        "        state[\"sum1\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum2\"] = exponential  # sum2 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum1\"]  # denominator_sf 업데이트\n",
        "    else:  # accumulator 2 사용\n",
        "        state[\"sum2\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum1\"] = exponential  # sum1 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum2\"]  # denominator_sf 업데이트\n",
        "\n",
        "    sequence_out = state[\"valid\"][0]\n",
        "\n",
        "    return  state[\"denominator_sf\"]\n",
        "\n",
        "\n",
        "def cordic_accumulator (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    state_accumulator = {\n",
        "    \"denominator_sf\": 0,\n",
        "    \"accumulator_SEL\": 0,\n",
        "    \"valid\": [0] * 11,\n",
        "    \"sum1\": 0,\n",
        "    \"sum2\": 0\n",
        "}\n",
        "\n",
        "    exp_fraction[0]=  accumulator(1,data_list[0],state_accumulator)\n",
        "    exp_fraction[1]=  accumulator( 0 ,data_list[1],state_accumulator)\n",
        "    exp_fraction[2]=  accumulator( 0 ,data_list[2],state_accumulator)\n",
        "    exp_fraction[3]=  accumulator( 0 ,data_list[3],state_accumulator)\n",
        "    exp_fraction[4]=  accumulator( 0 ,data_list[4],state_accumulator)\n",
        "    exp_fraction[5]=  accumulator( 0 ,data_list[5],state_accumulator)\n",
        "    exp_fraction[6]=  accumulator( 0 ,data_list[6],state_accumulator)\n",
        "    exp_fraction[7]=  accumulator( 0 ,data_list[7],state_accumulator)\n",
        "    exp_fraction[8]=  accumulator( 0 ,data_list[8],state_accumulator)\n",
        "    exp_fraction[9]=  accumulator( 0 ,data_list[9],state_accumulator)\n",
        "    #print(hex(exp_fraction[9]))\n",
        "\n",
        "    return  exp_fraction[9]\n",
        "\n",
        "\n",
        "\n",
        "def fifo(exponential, state):\n",
        "\n",
        "\n",
        "        # FIFO 시프트 (FIFO_reg[i] = FIFO_reg[i-1])\n",
        "    state[\"FIFO_reg\"] = [exponential] + state[\"FIFO_reg\"][:-1]\n",
        "\n",
        "    numerator_sf = state[\"FIFO_reg\"][9]\n",
        "\n",
        "\n",
        "    return numerator_sf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def div_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if y_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "\n",
        "\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "    else:\n",
        "\n",
        "\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return  y_cal, z_cal\n",
        "def cordic_diu_pipe(x_in, y_in):\n",
        "    y_1, z_1 = div_unit(x_in, y_in, 0, 0b0010000000000000, 0)\n",
        "    y_2, z_2 = div_unit(x_in, y_1, z_1, 0b0001000000000000, 1)\n",
        "    y_3, z_3 = div_unit(x_in, y_2, z_2, 0b0000100000000000, 2)\n",
        "    y_4, z_4 = div_unit(x_in, y_3, z_3, 0b0000010000000000, 3)\n",
        "    y_5, z_5 = div_unit(x_in, y_4, z_4, 0b0000001000000000, 4)\n",
        "    y_6, z_6 = div_unit(x_in, y_5, z_5, 0b0000000100000000, 5)\n",
        "    y_7, z_7 = div_unit(x_in, y_6, z_6, 0b0000000010000000, 6)\n",
        "    y_8, z_8 = div_unit(x_in, y_7, z_7, 0b0000000001000000, 7)\n",
        "    y_9, z_9 = div_unit(x_in, y_8, z_8, 0b0000000000100000, 8)\n",
        "    y_10, z_10 = div_unit(x_in, y_9, z_9, 0b0000000000010000, 9)\n",
        "    y_11, z_11 = div_unit(x_in, y_10, z_10, 0b0000000000001000, 10)\n",
        "    y_12, z_12 = div_unit(x_in, y_11, z_11, 0b0000000000000100, 11)\n",
        "    y_13, z_13 = div_unit(x_in, y_12, z_12, 0b0000000000000010, 12)\n",
        "    y_14, z_14 = div_unit(x_in, y_13, z_13, 0b0000000000000001, 13)\n",
        "    divided_val = z_14  # Final z value representing atan(y/x)\n",
        "    return divided_val\n",
        "\n",
        "def cordic_diu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10,data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20):\n",
        "    data_list_x = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_list_y = [data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_diu_pipe(data_list_x[i],data_list_y[i])\n",
        "        #print(bin(data_list[i]))\n",
        "        #print(hex(exp_fraction[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "'''\n",
        "top(\n",
        "    0b00000011000110001110,  # 첫 번째 data_in 값\n",
        "    0b00000100010011110000,  # 두 번째 data_in 값\n",
        "    0b00000000010011111010,  # 세 번째 data_in 값\n",
        "    0b00000010101101111100,  # 네 번째 data_in 값\n",
        "    0b00000111000110011000,  # 다섯 번째 data_in 값\n",
        "    0b00000100010100001101,  # 여섯 번째 data_in 값\n",
        "    0b11111100110100100011,  # 일곱 번째 data_in 값\n",
        "    0b11111111100000001011,  # 여덟 번째 data_in 값\n",
        "    0b00000011010101100100,  # 아홉 번째 data_in 값\n",
        "    0b11111010100111110111   # 열 번째 data_in 값\n",
        ")\n",
        "'''\n",
        "top(1.548621,\n",
        "2.154268,\n",
        "0.15548,\n",
        "1.358942,\n",
        "3.54985,\n",
        "2.1578712,\n",
        "-1.58952,\n",
        "-0.248623,\n",
        "1.6685123,\n",
        "-2.68856\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # ✅ 기존 Softmax -> Sigmoid Normalization 적용\n",
        "        attention_probs = top(attention_scores)\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "HM1AXhHcQ04X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 구조 적용,, 재시작"
      ],
      "metadata": {
        "id": "eA7KlC3ftQ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None,\n",
        "                past_key_value=None,\n",
        "                output_attentions=False):\n",
        "\n",
        "        # 1) Query/Key/Value 계산 (원본과 동일)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        key_layer   = self.transpose_for_scores(\n",
        "                          self.key(encoder_hidden_states\n",
        "                                   if encoder_hidden_states is not None\n",
        "                                   else hidden_states)\n",
        "                      )\n",
        "        value_layer = self.transpose_for_scores(\n",
        "                          self.value(encoder_hidden_states\n",
        "                                     if encoder_hidden_states is not None\n",
        "                                     else hidden_states)\n",
        "                      )\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 2) Attention score 계산 & scaling\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2)\n",
        "        ) / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        # 3) CORDIC-Softmax: top()을 120번 자동 호출\n",
        "        #    attention_scores.shape == [B, H, L, L] with L=10\n",
        "        # ---------------------------------------------------\n",
        "        B, H, L, _ = attention_scores.size()             # B=1, H=12, L=10\n",
        "        flat = attention_scores.view(-1, L)              # shape = [B*H, 10]\n",
        "        rows = []\n",
        "        for row in flat:                                 # 자동으로 1*12 = 12 행 × 10 쿼리 = 120 호출\n",
        "            # row.tolist() → Python float 리스트 길이 10\n",
        "            top_out = top(*row.tolist())                 # 여러분의 top(data1…data10)\n",
        "            # 다시 tensor 로 만들 때, dtype/device 일치시키기\n",
        "            rows.append(torch.tensor(\n",
        "                top_out,\n",
        "                dtype=attention_scores.dtype,\n",
        "                device=attention_scores.device\n",
        "            ))\n",
        "        attention_probs = torch.stack(rows)              # shape = [B*H, 10]\n",
        "        attention_probs = attention_probs.view(B, H, L, L)\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        # 4) Dropout & Context 계산 (원본과 동일)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) \\\n",
        "                  if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "zfSHmJVQtUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None,\n",
        "                past_key_value=None,\n",
        "                output_attentions=False):\n",
        "\n",
        "        # 1) query/key/value (원본과 동일)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        key_layer   = self.transpose_for_scores(\n",
        "                          self.key(encoder_hidden_states\n",
        "                                   if encoder_hidden_states is not None\n",
        "                                   else hidden_states)\n",
        "                      )\n",
        "        value_layer = self.transpose_for_scores(\n",
        "                          self.value(encoder_hidden_states\n",
        "                                     if encoder_hidden_states is not None\n",
        "                                     else hidden_states)\n",
        "                      )\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 2) 스코어 계산 및 scaling\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2)\n",
        "        ) / (self.attention_head_size ** 0.5)\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # 3) CORDIC-Softmax 교체\n",
        "\n",
        "        B, H, L, _ = attention_scores.size()           # e.g. 1,12,10,10\n",
        "        flat = attention_scores.view(-1, L)            # [B*H, L]\n",
        "        rows = []\n",
        "\n",
        "        for row in flat:\n",
        "            # (1) 안정화: max 빼기\n",
        "            row = row - row.max()\n",
        "            # (2) 정수부 범위 클램핑 (–63.999 ~ +63.999)\n",
        "            row = row.clamp(min=-63.999, max=63.999)\n",
        "            # (3) Python float 리스트로 변환 → top()\n",
        "            out = top(*row.tolist())\n",
        "            # (4) 다시 tensor 생성\n",
        "            rows.append(torch.tensor(\n",
        "                out,\n",
        "                dtype=attention_scores.dtype,\n",
        "                device=attention_scores.device\n",
        "            ))\n",
        "\n",
        "        attention_probs = torch.stack(rows).view(B, H, L, L)\n",
        "        # ─────────────────────────────────────────────\n",
        "\n",
        "        # 4) dropout & context 계산 (원본과 동일)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) \\\n",
        "                  if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "JxoOuCl6wyU6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "for layer in model.bert.encoder.layer:\n",
        "    orig = layer.attention.self\n",
        "    # Modified 모듈 생성\n",
        "    mod = BertSelfAttentionModified(config)\n",
        "    # 기존 가중치 로드\n",
        "    mod.load_state_dict(orig.state_dict(), strict=False)\n",
        "\n",
        "    mod.to(device)\n",
        "    # 교체\n",
        "    layer.attention.self = mod\n"
      ],
      "metadata": {
        "id": "peAPhZdDvZzN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, layer in enumerate(model.bert.encoder.layer):\n",
        "    attn = layer.attention.self\n",
        "    print(f\"Layer {idx:2d}: {attn.__class__.__name__:<30} (module: {attn.__class__.__module__})\")\n"
      ],
      "metadata": {
        "id": "huL4Du0TtzwD",
        "outputId": "e78ea4bb-51f7-4760-851b-a68d98e9af6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer  0: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  1: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  2: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  3: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  4: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  5: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  6: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  7: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  8: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  9: BertSelfAttentionModified      (module: __main__)\n",
            "Layer 10: BertSelfAttentionModified      (module: __main__)\n",
            "Layer 11: BertSelfAttentionModified      (module: __main__)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        b_input_ids,\n",
        "        attention_mask=b_input_mask,\n",
        "        output_attentions=True\n",
        "    )\n",
        "\n",
        "# 2) 튜플에서 마지막 레이어 확률만 꺼내기\n",
        "all_attentions = outputs.attentions    # tuple of length 12, each is [B, H, L, L]\n",
        "attention_probs = all_attentions[-1]   # 마지막 레이어의 [B, H, L, L] 텐서\n",
        "\n",
        "# 3) 마지막 축(키 방향)으로 합이 1인지 확인\n",
        "sums = attention_probs.sum(dim=-1)     # shape = [B, H, L]\n",
        "print(\"어텐션 확률 합:\", sums)\n"
      ],
      "metadata": {
        "id": "m2Fs1SMmxKoy",
        "outputId": "b53fbc04-9dc4-478c-d0c7-790cb78357b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어텐션 확률 합: tensor([[[0.9954, 0.9844, 0.9863, 0.9880, 0.9773, 0.9746, 0.9824, 0.9749,\n",
            "          0.9888, 0.9961],\n",
            "         [0.9993, 0.9905, 0.9956, 0.9958, 0.9900, 0.9880, 0.9827, 0.9714,\n",
            "          0.9900, 0.9963],\n",
            "         [0.9968, 0.9998, 0.9927, 0.9978, 0.9944, 0.9944, 0.9980, 0.9846,\n",
            "          0.9988, 0.9883],\n",
            "         [0.9954, 0.9902, 0.9792, 0.9968, 0.9924, 0.9917, 0.9885, 0.9841,\n",
            "          0.9888, 0.9949],\n",
            "         [0.9978, 0.9998, 0.9863, 0.9983, 0.9922, 1.0024, 0.9983, 0.9888,\n",
            "          0.9905, 0.9968],\n",
            "         [0.9944, 1.0017, 0.9944, 0.9849, 0.9819, 0.9885, 1.0015, 0.9814,\n",
            "          0.9939, 0.9866],\n",
            "         [0.9937, 0.9944, 0.9883, 0.9927, 0.9890, 0.9888, 0.9900, 0.9727,\n",
            "          1.0012, 0.9939],\n",
            "         [0.9731, 0.9785, 0.9944, 0.9802, 0.9961, 0.9805, 0.9941, 0.9885,\n",
            "          0.9868, 0.9851],\n",
            "         [0.9900, 0.9966, 0.9775, 0.9985, 0.9849, 0.9907, 0.9902, 0.9829,\n",
            "          0.9976, 0.9983],\n",
            "         [0.9966, 0.9980, 0.9924, 0.9937, 0.9946, 0.9954, 0.9944, 0.9927,\n",
            "          0.9963, 0.9958],\n",
            "         [0.9971, 0.9917, 0.9968, 0.9937, 0.9915, 0.9980, 0.9980, 0.9749,\n",
            "          0.9907, 0.9941],\n",
            "         [0.9963, 0.9924, 0.9731, 0.9888, 0.9766, 0.9771, 0.9846, 0.9927,\n",
            "          0.9805, 0.9880]],\n",
            "\n",
            "        [[0.9978, 0.9941, 0.9980, 0.9978, 0.9978, 0.9937, 0.9993, 0.9939,\n",
            "          0.9966, 0.9961],\n",
            "         [0.9961, 0.9983, 0.9941, 0.9985, 0.9993, 0.9961, 0.9976, 0.9971,\n",
            "          0.9973, 0.9961],\n",
            "         [0.9990, 0.9956, 1.0017, 1.0002, 0.9956, 0.9937, 0.9990, 0.9983,\n",
            "          0.9939, 0.9978],\n",
            "         [0.9985, 0.9917, 1.0010, 0.9963, 0.9971, 0.9958, 0.9958, 0.9990,\n",
            "          0.9968, 0.9949],\n",
            "         [0.9858, 0.9897, 0.9929, 0.9990, 0.9988, 0.9973, 0.9985, 0.9917,\n",
            "          0.9924, 0.9995],\n",
            "         [0.9932, 0.9983, 0.9961, 0.9937, 0.9978, 0.9941, 0.9944, 1.0010,\n",
            "          0.9978, 0.9978],\n",
            "         [0.9973, 0.9944, 0.9985, 0.9980, 0.9990, 1.0000, 0.9988, 0.9971,\n",
            "          0.9993, 0.9944],\n",
            "         [0.9949, 0.9980, 0.9993, 0.9958, 0.9998, 0.9978, 0.9978, 0.9990,\n",
            "          0.9973, 0.9958],\n",
            "         [1.0010, 0.9951, 0.9875, 0.9976, 0.9946, 0.9829, 0.9988, 0.9753,\n",
            "          0.9939, 0.9971],\n",
            "         [0.9958, 0.9973, 0.9966, 0.9973, 0.9912, 0.9912, 0.9946, 0.9927,\n",
            "          0.9973, 0.9927],\n",
            "         [0.9978, 0.9961, 0.9976, 0.9978, 0.9919, 0.9995, 0.9963, 0.9958,\n",
            "          0.9954, 1.0010],\n",
            "         [1.0005, 0.9849, 0.9998, 0.9954, 1.0005, 0.9927, 0.9976, 0.9922,\n",
            "          0.9983, 0.9958]],\n",
            "\n",
            "        [[0.9985, 0.9973, 0.9978, 0.9958, 0.9993, 0.9961, 0.9937, 1.0000,\n",
            "          0.9963, 0.9966],\n",
            "         [1.0000, 1.0002, 0.9941, 0.9973, 0.9978, 0.9978, 0.9951, 1.0000,\n",
            "          0.9993, 0.9968],\n",
            "         [0.9956, 0.9976, 0.9978, 0.9978, 0.9995, 0.9985, 0.9968, 0.9937,\n",
            "          0.9866, 0.9954],\n",
            "         [0.9968, 0.9966, 0.9971, 0.9954, 0.9976, 0.9971, 0.9983, 0.9922,\n",
            "          0.9954, 0.9976],\n",
            "         [0.9929, 0.9917, 0.9937, 0.9929, 0.9976, 0.9949, 0.9956, 0.9978,\n",
            "          1.0000, 0.9939],\n",
            "         [0.9971, 0.9956, 0.9988, 1.0000, 0.9963, 0.9990, 0.9915, 0.9944,\n",
            "          0.9961, 0.9961],\n",
            "         [0.9973, 0.9973, 0.9983, 0.9968, 0.9985, 0.9971, 0.9990, 0.9980,\n",
            "          0.9990, 0.9946],\n",
            "         [0.9988, 0.9978, 0.9976, 0.9976, 0.9971, 0.9988, 0.9973, 0.9963,\n",
            "          0.9963, 0.9961],\n",
            "         [0.9968, 0.9985, 0.9934, 0.9915, 0.9980, 0.9946, 0.9861, 0.9734,\n",
            "          0.9954, 0.9978],\n",
            "         [0.9976, 0.9971, 1.0010, 0.9976, 0.9956, 0.9966, 0.9973, 0.9988,\n",
            "          0.9929, 1.0002],\n",
            "         [0.9985, 0.9978, 0.9971, 0.9973, 0.9980, 0.9980, 0.9978, 0.9941,\n",
            "          0.9939, 0.9963],\n",
            "         [0.9993, 0.9958, 0.9961, 0.9980, 0.9985, 0.9988, 0.9990, 0.9968,\n",
            "          0.9980, 0.9963]],\n",
            "\n",
            "        [[0.9915, 0.9937, 0.9900, 0.9963, 0.9927, 0.9941, 0.9927, 0.9863,\n",
            "          0.9961, 1.0002],\n",
            "         [1.0000, 0.9961, 0.9895, 0.9961, 0.9983, 0.9998, 0.9993, 0.9707,\n",
            "          0.9954, 0.9946],\n",
            "         [0.9998, 0.9966, 0.9907, 0.9941, 0.9958, 0.9993, 0.9973, 0.9866,\n",
            "          0.9966, 0.9885],\n",
            "         [0.9934, 0.9966, 0.9885, 0.9827, 0.9934, 0.9980, 0.9983, 0.9678,\n",
            "          0.9924, 1.0000],\n",
            "         [0.9971, 0.9961, 0.9941, 0.9954, 0.9980, 0.9990, 0.9951, 0.9705,\n",
            "          0.9993, 1.0010],\n",
            "         [0.9980, 0.9929, 0.9954, 0.9905, 0.9922, 0.9861, 0.9824, 0.9807,\n",
            "          0.9978, 0.9983],\n",
            "         [0.9988, 0.9958, 0.9941, 0.9939, 0.9934, 0.9922, 0.9968, 0.9946,\n",
            "          1.0022, 0.9983],\n",
            "         [0.9868, 0.9846, 0.9810, 0.9902, 0.9980, 0.9958, 0.9966, 0.9927,\n",
            "          0.9966, 0.9976],\n",
            "         [0.9937, 0.9988, 0.9866, 0.9829, 0.9958, 1.0002, 0.9924, 0.9768,\n",
            "          0.9985, 0.9983],\n",
            "         [0.9963, 0.9954, 0.9968, 0.9993, 0.9990, 0.9980, 0.9973, 0.9961,\n",
            "          0.9951, 0.9971],\n",
            "         [0.9993, 0.9946, 0.9971, 0.9990, 0.9946, 0.9934, 0.9854, 0.9807,\n",
            "          1.0005, 0.9978],\n",
            "         [1.0005, 0.9946, 0.9937, 0.9924, 0.9961, 0.9956, 0.9946, 0.9746,\n",
            "          0.9983, 0.9961]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def run_validation(model, validation_dataloader, device):\n",
        "    print(\"== Self-Attention Modules ==\")\n",
        "    for idx, layer in enumerate(model.bert.encoder.layer):\n",
        "        cls_name = layer.attention.self.__class__.__name__\n",
        "        print(f\"  Layer {idx:2d}: {cls_name}\")\n",
        "    print(\"=============================\\n\")\n",
        "    model.eval()\n",
        "    eval_accuracy = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "            # 배치를 GPU로\n",
        "            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n",
        "\n",
        "            #logits만 얻기\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # CPU로 내리고 numpy 변환\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "            # 배치별 정확도 계산\n",
        "            batch_acc = flat_accuracy(logits, label_ids)\n",
        "            eval_accuracy += batch_acc\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "    avg_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Accuracy: {avg_accuracy:.2f}\")\n",
        "    print(f\"Validation Time   : {format_time(time.time() - t0)}\")\n",
        "\n",
        "    return avg_accuracy\n",
        "\n",
        "avg_acc = run_validation(model, validation_dataloader, device)"
      ],
      "metadata": {
        "id": "r3GHXPFAyqKm",
        "outputId": "d984121b-9484-4c75-c2e5-6e09e150cb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Self-Attention Modules ==\n",
            "  Layer  0: BertSelfAttentionModified\n",
            "  Layer  1: BertSelfAttentionModified\n",
            "  Layer  2: BertSelfAttentionModified\n",
            "  Layer  3: BertSelfAttentionModified\n",
            "  Layer  4: BertSelfAttentionModified\n",
            "  Layer  5: BertSelfAttentionModified\n",
            "  Layer  6: BertSelfAttentionModified\n",
            "  Layer  7: BertSelfAttentionModified\n",
            "  Layer  8: BertSelfAttentionModified\n",
            "  Layer  9: BertSelfAttentionModified\n",
            "  Layer 10: BertSelfAttentionModified\n",
            "  Layer 11: BertSelfAttentionModified\n",
            "=============================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-901affe8a864>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-901affe8a864>\u001b[0m in \u001b[0;36mrun_validation\u001b[0;34m(model, validation_dataloader, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#logits만 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1676\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-19ccea1496bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m63.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m63.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# (3) Python float 리스트로 변환 → top()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m# (4) 다시 tensor 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             rows.append(torch.tensor(\n",
            "\u001b[0;32m<ipython-input-34-b8146705381d>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mdata_44\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mdata_55\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mdata_66\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mdata_77\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b8146705381d>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# ✅ 1. 정수부와 소수부 정확하게 분리 (내림 적용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mint_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mfrac_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint_part\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 소수부 절댓값 유지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# ✅ 2. 정수부(7비트) 변환 (2의 보수 변환)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ."
      ],
      "metadata": {
        "id": "cSd2dA-wQ94T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVB2VLaY9PK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce1d1e8-a074-48a5-81b7-8ececb5b1096"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1yMW5ojZ43o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5075a6-604c-463a-aa00-ab3118658ca5"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdjsOX9naA-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a33e04-62a0-4e31-9cbd-ce2fef37ef4f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkMgQEmaPEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efd2b03-c5eb-4772-9491-ea820812dd98"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LflYZlN3aYSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0027c2e9-f649-4e8c-d067-07141ca28008"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25uSDxqadmc"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_j9f6Qjami9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "af3ddbea-35f3-495d-ff60-deaeaecb337e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "572             bc01      1         NaN   \n",
              "6146            c_13      1         NaN   \n",
              "3170            l-93      1         NaN   \n",
              "7253           sks13      1         NaN   \n",
              "2090           rhl07      1         NaN   \n",
              "2071           rhl07      1         NaN   \n",
              "5647            c_13      0           *   \n",
              "4458            ks08      0           *   \n",
              "7706            ad03      1         NaN   \n",
              "6130            c_13      0           *   \n",
              "\n",
              "                                               sentence  \n",
              "572                          The tub filled with water.  \n",
              "6146          The captain sank the boat with a torpedo.  \n",
              "3170                                  The baby dressed.  \n",
              "7253      I prefer for the girl to put a picture there.  \n",
              "2090  I threw Mary the ball, but she was looking at ...  \n",
              "2071                  To whom did you send the package?  \n",
              "5647                   Quickly walks went to the store.  \n",
              "4458                                 They will do come.  \n",
              "7706                  The landlord donated a helicopter  \n",
              "6130                           Bill was bitten the dog.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-187e194d-666c-43ad-8fa5-c68596d9c825\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The tub filled with water.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6146</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The captain sank the boat with a torpedo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3170</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The baby dressed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7253</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I prefer for the girl to put a picture there.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2090</th>\n",
              "      <td>rhl07</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I threw Mary the ball, but she was looking at ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2071</th>\n",
              "      <td>rhl07</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>To whom did you send the package?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5647</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Quickly walks went to the store.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4458</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They will do come.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7706</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The landlord donated a helicopter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6130</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Bill was bitten the dog.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-187e194d-666c-43ad-8fa5-c68596d9c825')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-187e194d-666c-43ad-8fa5-c68596d9c825 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-187e194d-666c-43ad-8fa5-c68596d9c825');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7e307f3b-f8cd-4269-b61a-b03a8e9d9f35\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e307f3b-f8cd-4269-b61a-b03a8e9d9f35')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7e307f3b-f8cd-4269-b61a-b03a8e9d9f35 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"bc01\",\n          \"c_13\",\n          \"ks08\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The landlord donated a helicopter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGFvVN-aqY5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ee1cf7c-0ebc-48d1-df41-8e75507aff6d"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence  label\n",
              "8175  Did that Medea killed her children upset Jason?      0\n",
              "496                      They tried all to like John.      0\n",
              "6612               The Ethel was sitting at her desk.      0\n",
              "3369           The collaborators escaped the convict.      0\n",
              "4554          Ann may spending her vacation in Italy.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08662d46-4401-47ec-bdf1-df76891c16e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8175</th>\n",
              "      <td>Did that Medea killed her children upset Jason?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>They tried all to like John.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6612</th>\n",
              "      <td>The Ethel was sitting at her desk.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3369</th>\n",
              "      <td>The collaborators escaped the convict.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4554</th>\n",
              "      <td>Ann may spending her vacation in Italy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08662d46-4401-47ec-bdf1-df76891c16e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08662d46-4401-47ec-bdf1-df76891c16e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08662d46-4401-47ec-bdf1-df76891c16e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-280cb989-eff9-4df6-8223-d169dfed60cd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-280cb989-eff9-4df6-8223-d169dfed60cd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-280cb989-eff9-4df6-8223-d169dfed60cd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"They tried all to like John.\",\n          \"Ann may spending her vacation in Italy.\",\n          \"The Ethel was sitting at her desk.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxfCoezXau5n"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Kj3sWgaxan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e32fc1-fb47-4222-90f7-7c1aa9c53511"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8T8JMCma1OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0878ade7-8559-484d-d475-8315077428be"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tm_8o_a54i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919d160d-0249-4abb-e4d7-bf3b4d0fb867"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V80hCa9CbEOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6408f9-01a8-4faa-d638-bf6b7cdec21b"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU5-MHxHbJrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2da186-ecbf-48a8-d983-a9e27adb8caf"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 10\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 10 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6e3Oe-JbNJM"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcSGMwoFbRAM"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49qrloHrbTmE"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TFSrS7bXI-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0jiGwk3batl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1986d26-f140-4f6d-e54e-9c9cbeee87be"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoerOh8qbegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0553fad1-d30e-4d94-822f-59fdbf30694c"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DfOPFdbkI3"
      },
      "source": [
        "#Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hliUyfZsboiy"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXsLGIi6bvl-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcJnBTsbyoJ"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2YjFs8gb1hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efb120f-8987-4621-bcfa-8ab25b8eae12"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:00:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:00:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:00:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:00:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdFfKyoLb7Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dadf4f06-a4ef-46ff-b04f-e5e6b99e3c33"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-N3fVYKcApy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f2d1b3-3802-4df9-a0c7-d5dc4df16df7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNF30U8FcITz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149b5de7-d094-4f1f-e5d6-917821c64277"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTk5LY4cLfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51733a4-4af4-4721-bfb2-b934dc11a86c"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy_SFwR8cQ58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0b5b8b-0420-4352-d4bd-9d19656b97fb"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-MCNw0cTCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db99c5c3-1c45-42f5-b6ff-2ca62e91e72b"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(0.21713222235566895),\n",
              " 0.0,\n",
              " np.float64(0.4177371279974369),\n",
              " np.float64(0.4127594582445936),\n",
              " np.float64(0.44440090347500916),\n",
              " np.float64(0.6979824404521128),\n",
              " np.float64(0.19377278632539327),\n",
              " 0.0,\n",
              " np.float64(0.9165151389911681),\n",
              " np.float64(0.6146362971528592),\n",
              " np.float64(0.8459051693633014),\n",
              " np.float64(0.647150228929434),\n",
              " np.float64(0.7562449037944323),\n",
              " np.float64(0.5204956780951701),\n",
              " np.float64(0.5204956780951701),\n",
              " np.float64(0.49382916465843113),\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbT-rwHocVqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210fba7c-2a16-459d-c8cf-22b0e95d4fb3"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론 코드"
      ],
      "metadata": {
        "id": "xd5BzXwU8HM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# (1) device, tokenizer은 이미 세팅되어 있다고 가정합니다.\n",
        "#    model도 BertSelfAttentionModified로 교체된 채로 메모리에 올라와 있어야 합니다.\n",
        "\n",
        "MAX_LEN = 10  # validation 때 쓰신 것과 동일한 토큰 길이\n",
        "\n",
        "def predict_with_cordic(model, tokenizer, sentence, device):\n",
        "    \"\"\"\n",
        "    CORDIC-Softmax 패치된 model + tokenizer를 이용해\n",
        "    단일 문장(sentence)에 대해 CoLA 예측을 수행합니다.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 토크나이징 + 패딩\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # GPU/CPU 동일하게 맞추기\n",
        "    inputs = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    # 순전파만 수행\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            token_type_ids=inputs.get(\"token_type_ids\", None)\n",
        "        )\n",
        "        # HuggingFace 버전 따라 .logits 또는 [0]\n",
        "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n",
        "\n",
        "    # CORDIC-Softmax로 이미 변환된 attention을 쓰므로\n",
        "    # classification head 출력만 softmax\n",
        "    probs = torch.softmax(logits, dim=-1)    # [1, 2]\n",
        "    pred  = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    return pred, probs.cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "# — 사용 예시 —\n",
        "sentence = \"Here is a sample sentence to test CORDIC inference.\"\n",
        "pred, probs = predict_with_cordic(model, tokenizer, sentence, device)\n",
        "\n",
        "label_name = \"acceptable\" if pred == 1 else \"unacceptable\"\n",
        "print(f\"Sentence        : {sentence}\")\n",
        "print(f\"Predicted label : {pred} ({label_name})\")\n",
        "print(f\"Probabilities   : [0]={probs[0]:.4f}, [1]={probs[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "ddIcD3HipFSO",
        "outputId": "6c16838f-80f6-4da8-ec56-809e8b2e7481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence        : Here is a sample sentence to test CORDIC inference.\n",
            "Predicted label : 1 (acceptable)\n",
            "Probabilities   : [0]=0.2259, [1]=0.7741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5avgvjcYXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d731a4d-adf9-475b-a709-34946ac56710"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# 1. 모델 및 토크나이저 로드 (예: 'bert-base-uncased' 사용)\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.cuda()               # GPU로 이동\n",
        "model.eval()               # 평가 모드로 전환\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "# 2. 추론할 영어 문장 예시\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "\n",
        "# 3. 문장 토큰화 및 인코딩\n",
        "inputs = tokenizer.encode_plus(\n",
        "    sentence,\n",
        "    add_special_tokens=True,      # [CLS]와 [SEP] 토큰 추가\n",
        "    max_length=10,                # 최대 길이 설정\n",
        "    padding=\"max_length\",         # 최대 길이에 맞춰 패딩\n",
        "    truncation=True,              # 길면 잘라냄\n",
        "    return_tensors=\"pt\"           # 파이토치 텐서 반환\n",
        ")\n",
        "\n",
        "# 4. 텐서를 GPU로 이동\n",
        "inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "# 5. 추론 (forward pass)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs[0]  # 모델의 출력은 튜플이며, 첫 번째 요소가 로짓입니다.\n",
        "\n",
        "# 6. Softmax 적용 (선택 사항) 및 예측 클래스 결정\n",
        "probs = torch.softmax(logits, dim=1)\n",
        "predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# 7. 결과 출력\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", predicted_class.item())\n",
        "print(\"예측 확률:\", probs)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 0\n",
            "예측 확률: tensor([[0.7347, 0.2653]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    입력된 문장에 대해 학습된 모델을 사용하여 예측을 수행합니다.\n",
        "    Returns:\n",
        "        predicted_class: 예측된 클래스 (예: 0 또는 1)\n",
        "        probs: 각 클래스의 확률 (numpy 배열)\n",
        "    \"\"\"\n",
        "    # 문장을 토큰화하고 encode_plus를 통해 [CLS], [SEP] 토큰을 추가하며, 패딩/자르기 적용\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,      # [CLS]와 [SEP] 추가\n",
        "        max_length=max_length,        # 최대 길이\n",
        "        padding=\"max_length\",         # 최대 길이에 맞게 패딩\n",
        "        truncation=True,              # 길면 자르기\n",
        "        return_tensors=\"pt\"           # PyTorch 텐서 반환\n",
        "    )\n",
        "\n",
        "    # GPU로 텐서를 전송\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # 모델을 평가 모드로 전환하고, 추론 시에는 기울기를 계산하지 않도록 설정\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]  # 모델의 출력은 튜플 형태로, 첫 번째 요소가 logits입니다.\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 사용\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PITHYJKhPHD",
        "outputId": "c1e48647-320b-42aa-d562-a50a05b01a97"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 0\n",
            "예측 확률: [[0.73471546 0.26528448]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cordic 코드"
      ],
      "metadata": {
        "id": "oK0EdSFZjeh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def top_1200_input(attention_scores):\n",
        "    \"\"\"\n",
        "    1200개의 attention_scores를 120개의 리스트로 나누어 각 리스트를 top 함수에 전달\n",
        "    결과는 원본 형태 (1, 12, 10, 10)로 반환\n",
        "    \"\"\"\n",
        "    if isinstance(attention_scores, torch.Tensor):\n",
        "        attention_scores = attention_scores.detach().cpu().numpy()  # ✅ detach() 추가\n",
        "\n",
        "    batch_size, num_heads, seq_length, _ = attention_scores.shape\n",
        "    result_arrays = np.zeros((batch_size, num_heads, seq_length, seq_length))\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        for row in range(seq_length):\n",
        "            for col in range(seq_length // 10):\n",
        "                input_values = attention_scores[0, head, row, col * 10:(col + 1) * 10]\n",
        "                result = top(*input_values)\n",
        "                result_arrays[0, head, row, col * 10:(col + 1) * 10] = result\n",
        "\n",
        "    # ✅ numpy -> torch 변환할 때 `.to(device)` 추가\n",
        "    return torch.tensor(result_arrays, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def float_to_fixed_point(value, int_bits=7, frac_bits=13):\n",
        "    \"\"\"\n",
        "    실수를 32비트 Fixed-Point 바이너리로 변환하는 함수.\n",
        "    - 정수부는 7비트 (Signed, 2의 보수)\n",
        "    - 소수부는 13비트 (항상 양수)\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ PyTorch Tensor 처리\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        value = value.detach().cpu().numpy()\n",
        "\n",
        "    # ✅ NaN 또는 Inf 값 체크 후 예외 처리\n",
        "    if np.isnan(value) or np.isinf(value):\n",
        "        raise ValueError(f\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\")\n",
        "\n",
        "    # ✅ **최대/최소 값 제한 (7비트 표현 범위)**\n",
        "    value = max(min(value, 63), -64)\n",
        "\n",
        "    # ✅ 정수부와 소수부 분리\n",
        "    int_part = np.floor(value).astype(int)\n",
        "    frac_part = abs(value - int_part)  # 소수 부분\n",
        "\n",
        "    # ✅ 2의 보수 변환 (음수 처리)\n",
        "    if int_part < 0:\n",
        "        int_part = (1 << int_bits) + int_part\n",
        "\n",
        "    int_binary = format(int_part, f'0{int_bits}b')\n",
        "\n",
        "    # ✅ 12비트 0 추가 (BERT 출력 형식 유지)\n",
        "    int_binary = \"0\" * 12 + int_binary\n",
        "\n",
        "    # ✅ 소수부 변환 (13비트)\n",
        "    frac_binary = \"\"\n",
        "    for _ in range(frac_bits):\n",
        "        frac_part *= 2\n",
        "        if frac_part >= 1:\n",
        "            frac_binary += \"1\"\n",
        "            frac_part -= 1\n",
        "        else:\n",
        "            frac_binary += \"0\"\n",
        "\n",
        "    # ✅ 최종 32비트 바이너리 문자열 생성\n",
        "    binary_string = int_binary + frac_binary\n",
        "\n",
        "    # ✅ `binary_string`이 음수 값을 포함하는지 확인 후 처리\n",
        "    if \"-\" in binary_string:\n",
        "        raise ValueError(f\"[ERROR] 잘못된 바이너리 문자열 변환 감지: {binary_string}\")\n",
        "\n",
        "    # ✅ 20비트 정수 변환 (부호 처리)\n",
        "    fixed_binary = int(binary_string, 2)\n",
        "    if value < 0:\n",
        "        fixed_binary = (1 << 20) - fixed_binary  # 2의 보수 변환\n",
        "\n",
        "    lower_20_bits = fixed_binary & 0xFFFFF  # 20비트 마스킹\n",
        "\n",
        "    return lower_20_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def binary_to_decimal(binary_str):\n",
        "    # 16비트 중 앞 3비트는 정수부, 뒤 13비트는 소수부\n",
        "    int_part = int(binary_str[:3], 2)  # 정수부\n",
        "    frac_part = int(binary_str[3:], 2) / (2 ** 13)  # 소수부를 2^13으로 나눔\n",
        "    return int_part + frac_part  # 정수부와 소수부 합산\n",
        "\n",
        "def hex_to_binary(hex_str):\n",
        "    \"\"\"\n",
        "    16비트 헥스 값을 16비트 바이너리 문자열로 변환하는 함수\n",
        "\n",
        "    :param hex_str: 변환할 16비트 헥스 값 (예: \"1A2F\" 또는 0x1A2F)\n",
        "    :return: 16비트 바이너리 문자열 (예: \"0001101000101111\")\n",
        "    \"\"\"\n",
        "    # hex_str가 정수형일 경우 문자열로 변환\n",
        "    if isinstance(hex_str, int):\n",
        "        hex_str = format(hex_str, 'X')  # 16진수 대문자로 변환\n",
        "\n",
        "    # 16진수를 10진수로 변환 후, 16비트 이진수로 변환\n",
        "    binary_str = bin(int(hex_str, 16))[2:].zfill(16)\n",
        "    return binary_str\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "def top(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10) : #20bit data\n",
        "\n",
        "    data_11 = float_to_fixed_point(data_1)\n",
        "    data_22 = float_to_fixed_point(data_2)\n",
        "    data_33 = float_to_fixed_point(data_3)\n",
        "    data_44 = float_to_fixed_point(data_4)\n",
        "    data_55 = float_to_fixed_point(data_5)\n",
        "    data_66 = float_to_fixed_point(data_6)\n",
        "    data_77 = float_to_fixed_point(data_7)\n",
        "    data_88 = float_to_fixed_point(data_8)\n",
        "    data_99 = float_to_fixed_point(data_9)\n",
        "    data_1010 = float_to_fixed_point(data_10)\n",
        "\n",
        "    data_list = [data_11, data_22, data_33, data_44, data_55, data_66, data_77, data_88, data_99, data_1010]\n",
        "    #data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    int_list = [0] * 10 #exp_int input\n",
        "    exp_int_output = [0]*10 #exp_int output\n",
        "    fraction_list=[0]*10 #cordic_exu input\n",
        "    exp_fraction_output = [0]*10 #cordic_exu output\n",
        "    exp_whole=[0]*10 #cordic_exu +cordic_int\n",
        "    exp_trunc=[0]*10\n",
        "    exp_accum_input=[0]*10 # accumulator input  (exponential)\n",
        "    exp_accum_output=[0]*1\n",
        "    x_divider = [0] * 10 #cordic diu input\n",
        "    y_dividend = [0] * 10 #cordic diu input\n",
        "    data_out = [0] * 10\n",
        "    binary_values = [0] * 10\n",
        "    int_values = [0.0] * 10\n",
        "    for i in range(10):  # int part\n",
        "        int_list[i] = data_list[i] >> 13\n",
        "    exp_int_output[0], exp_int_output[1], exp_int_output[2], exp_int_output[3], exp_int_output[4], exp_int_output[5], exp_int_output[6], exp_int_output[7], exp_int_output[8], exp_int_output[9]=exp_int(int_list[0], int_list[1], int_list[2], int_list[3], int_list[4], int_list[5], int_list[6], int_list[7], int_list[8], int_list[9])\n",
        "\n",
        "    for i in range(10):  # fraction part\n",
        "        fraction_list[i] = (data_list[i] & 0x1FFF) << 1  # 하위 13비트를 가져오고, 뒤에 0을 추가\n",
        "    exp_fraction_output[0], exp_fraction_output[1], exp_fraction_output[2], exp_fraction_output[3], exp_fraction_output[4], exp_fraction_output[5], exp_fraction_output[6], exp_fraction_output[7], exp_fraction_output[8], exp_fraction_output[9] = cordic_exu(fraction_list[0], fraction_list[1], fraction_list[2], fraction_list[3], fraction_list[4], fraction_list[5], fraction_list[6], fraction_list[7], fraction_list[8], fraction_list[9])\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_whole[i] = exp_int_output[i] * exp_fraction_output[i]\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_trunc[i] = (exp_whole[i] >> 14) & 0xFFFFF  # 33~14 비트를 추출 (20비트)\n",
        "    for i in range(10):\n",
        "       exp_accum_input[i] = (exp_trunc[i] >> 4) & 0xFFFF\n",
        "\n",
        "    exp_accum_output[0] = cordic_accumulator(exp_accum_input[0],exp_accum_input[1],exp_accum_input[2],exp_accum_input[3],exp_accum_input[4],exp_accum_input[5],exp_accum_input[6],exp_accum_input[7],exp_accum_input[8],exp_accum_input[9])\n",
        "\n",
        "\n",
        "\n",
        "    # denominator_sf[i]에서 18~4 비트 추출 후, 앞에 0을 붙여서 x_divider에 할당\n",
        "    x_divider[0] = (exp_accum_output[0] >> 4) & 0x7FFF  # 18~4 비트를 추출 (15비트)\n",
        "\n",
        "    for i in range(10):\n",
        "    # numerator_sf[i]에서 15~4 비트 추출 후, 앞에 4개의 0을 붙여서 y_dividend에 할당\n",
        "        y_dividend[i] = (exp_accum_input[i] >> 4)  # 15~4 비트를 추출 (12비트)\n",
        "\n",
        "    data_out[0],data_out[1],data_out[2],data_out[3],data_out[4],data_out[5],data_out[6],data_out[7],data_out[8],data_out[9], = cordic_diu (x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],y_dividend[0],y_dividend[1],y_dividend[2],y_dividend[3],y_dividend[4],y_dividend[5],y_dividend[6],y_dividend[7],y_dividend[8],y_dividend[9])\n",
        "\n",
        "    for i in range(10):\n",
        "        binary_values[i] = hex_to_binary(data_out[i])\n",
        "       # print(binary_values[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        int_values[i] = binary_to_decimal(binary_values[i])\n",
        "\n",
        "    return int_values[0],int_values[1],int_values[2],int_values[3],int_values[4],int_values[5],int_values[6],int_values[7],int_values[8],int_values[9]\n",
        "\n",
        "def exp_int(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_to_int = [0]*10\n",
        "    int_output=[0]*10\n",
        "    for i in range(10):\n",
        "        # 이진수로 변환하여 signed_binary_to_int 함수에 전달\n",
        "        binary_str = bin(data_list[i])[2:].zfill(7)  # 20비트 이진수 문자열로 변환\n",
        "        data_to_int[i] = signed_binary_to_int(binary_str)\n",
        "\n",
        "    i_max = max(data_to_int)\n",
        "    offset=signed_binary_to_int(bin(0b0001011-i_max)[2:].zfill(7)) #offset\n",
        "\n",
        "    for i in range(10):\n",
        "        int_reg=data_to_int[i]+offset\n",
        "        #print(int_reg)\n",
        "        if int_reg==0 :\n",
        "            int_output[i]=0b0000_0000_0000_0001_00\n",
        "        elif int_reg==1 :\n",
        "            int_output[i]=0b0000_0000_0000_0010_11\n",
        "        elif int_reg==2 :\n",
        "            int_output[i]=0b0000_0000_0000_0111_10\n",
        "        elif int_reg==3 :\n",
        "            int_output[i]=0b0000_0000_0001_0100_00\n",
        "        elif int_reg==4 :\n",
        "            int_output[i]=0b0000_0000_0011_0110_10\n",
        "        elif int_reg==5 :\n",
        "            int_output[i]=0b0000_0000_1001_0100_10\n",
        "        elif int_reg==6 :\n",
        "            int_output[i]=0b0000_0001_1001_0011_10\n",
        "        elif int_reg==7 :\n",
        "            int_output[i]=0b0000_0100_0100_1000_11\n",
        "        elif int_reg==8 :\n",
        "            int_output[i]=0b0000_1011_1010_0101_00\n",
        "        elif int_reg==9 :\n",
        "            int_output[i]=0b0001_1111_1010_0111_00\n",
        "        elif int_reg==10 :\n",
        "            int_output[i]=0b0101_0110_0000_1010_10\n",
        "        elif int_reg==11 :\n",
        "            int_output[i]=0b1110_1001_1110_0010_01\n",
        "        else :\n",
        "            int_output[i]=0\n",
        "\n",
        "    return  int_output[0], int_output[1], int_output[2], int_output[3], int_output[4], int_output[5], int_output[6], int_output[7], int_output[8], int_output[9]\n",
        "\n",
        "\n",
        "def cordic_exu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_exu_pipe(data_list[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "def cordic_exu_pipe(z_in):\n",
        "    x_1, y_1, z_1=exp_unit(0b1001101010001111, 0, z_in, 0b0010001100100111,1)\n",
        "    x_2, y_2, z_2=exp_unit(x_1, y_1, z_1,0b0001000001011000,2)\n",
        "    x_3, y_3, z_3 = exp_unit(x_2, y_2, z_2, 0b0000100000001010, 3)\n",
        "    x_4, y_4, z_4 = exp_unit(x_3, y_3, z_3, 0b0000010000000001, 4)\n",
        "    x_5, y_5, z_5 = exp_unit(x_4, y_4, z_4, 0b0000001000000000, 5)\n",
        "    x_6, y_6, z_6 = exp_unit(x_5, y_5, z_5, 0b0000000100000000, 6)\n",
        "    x_7, y_7, z_7 = exp_unit(x_6, y_6, z_6, 0b0000000010000000, 7)\n",
        "    x_8, y_8, z_8 = exp_unit(x_7, y_7, z_7, 0b0000000001000000, 8)\n",
        "    x_9, y_9, z_9 = exp_unit(x_8, y_8, z_8, 0b0000000000100000, 9)\n",
        "    x_10, y_10, z_10 = exp_unit(x_9, y_9, z_9, 0b0000000000010000, 10)\n",
        "    x_11, y_11, z_11 = exp_unit(x_10, y_10, z_10, 0b0000000000001000, 11)\n",
        "    x_12, y_12, z_12 = exp_unit(x_11, y_11, z_11, 0b0000000000000100, 12)\n",
        "    x_13, y_13, z_13 = exp_unit(x_12, y_12, z_12, 0b0000000000000010, 13)\n",
        "    x_14, y_14, z_14 = exp_unit(x_13, y_13, z_13, 0b0000000000000010, 14)\n",
        "\n",
        "\n",
        "    exp_frac_reg=x_14+y_14\n",
        "    exp_frac = (exp_frac_reg >> 1) & 0xFFFF\n",
        "    #print(bin(exp_frac))\n",
        "    return exp_frac\n",
        "\n",
        "\n",
        "\n",
        "def exp_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "    x_cal = 0\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if z_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "        # 시계방향 (downward)\n",
        "        x_cal = x_in - (y_in >> pipe_num)\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "    else:\n",
        "        # 반시계방향 (upward)\n",
        "        x_cal = x_in + (y_in >> pipe_num)\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return x_cal, y_cal, z_cal\n",
        "\n",
        "\n",
        "def signed_binary_to_int(binary_str):\n",
        "    \"\"\"\n",
        "    2의 보수 표현된 이진수를 정수로 변환하는 함수.\n",
        "    \"\"\"\n",
        "    # ✅ \"0b\" 제거\n",
        "    binary_str = binary_str.replace(\"0b\", \"\")\n",
        "\n",
        "    # ✅ 이진수 길이 확인\n",
        "    n = len(binary_str)\n",
        "\n",
        "    # ✅ 부호 비트 확인 (최상위 비트가 1이면 음수)\n",
        "    if binary_str[0] == '1':\n",
        "        # 2의 보수 변환 (음수)\n",
        "        return int(binary_str, 2) - (1 << n)\n",
        "    else:\n",
        "        # 양수 변환\n",
        "        return int(binary_str, 2)\n",
        "\n",
        "\n",
        "def accumulator(sequence_in, exponential, state):\n",
        "    \"\"\"\n",
        "    입력값을 받아 sum1, sum2, denominator_sf 값을 업데이트하고 출력.\n",
        "    \"\"\"\n",
        "\n",
        "    # accumulator_SEL 토글\n",
        "    state[\"accumulator_SEL\"] = (state[\"accumulator_SEL\"] + sequence_in) & 1\n",
        "\n",
        "    # valid 시프트 레지스터 갱신\n",
        "    state[\"valid\"] = [sequence_in] + state[\"valid\"][:-1]\n",
        "\n",
        "    if state[\"accumulator_SEL\"]:  # accumulator 1 사용\n",
        "        state[\"sum1\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum2\"] = exponential  # sum2 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum1\"]  # denominator_sf 업데이트\n",
        "    else:  # accumulator 2 사용\n",
        "        state[\"sum2\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum1\"] = exponential  # sum1 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum2\"]  # denominator_sf 업데이트\n",
        "\n",
        "    sequence_out = state[\"valid\"][0]\n",
        "\n",
        "    return  state[\"denominator_sf\"]\n",
        "\n",
        "\n",
        "def cordic_accumulator (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    state_accumulator = {\n",
        "    \"denominator_sf\": 0,\n",
        "    \"accumulator_SEL\": 0,\n",
        "    \"valid\": [0] * 11,\n",
        "    \"sum1\": 0,\n",
        "    \"sum2\": 0\n",
        "}\n",
        "\n",
        "    exp_fraction[0]=  accumulator(1,data_list[0],state_accumulator)\n",
        "    exp_fraction[1]=  accumulator( 0 ,data_list[1],state_accumulator)\n",
        "    exp_fraction[2]=  accumulator( 0 ,data_list[2],state_accumulator)\n",
        "    exp_fraction[3]=  accumulator( 0 ,data_list[3],state_accumulator)\n",
        "    exp_fraction[4]=  accumulator( 0 ,data_list[4],state_accumulator)\n",
        "    exp_fraction[5]=  accumulator( 0 ,data_list[5],state_accumulator)\n",
        "    exp_fraction[6]=  accumulator( 0 ,data_list[6],state_accumulator)\n",
        "    exp_fraction[7]=  accumulator( 0 ,data_list[7],state_accumulator)\n",
        "    exp_fraction[8]=  accumulator( 0 ,data_list[8],state_accumulator)\n",
        "    exp_fraction[9]=  accumulator( 0 ,data_list[9],state_accumulator)\n",
        "    #print(hex(exp_fraction[9]))\n",
        "\n",
        "    return  exp_fraction[9]\n",
        "\n",
        "\n",
        "\n",
        "def fifo(exponential, state):\n",
        "\n",
        "\n",
        "        # FIFO 시프트 (FIFO_reg[i] = FIFO_reg[i-1])\n",
        "    state[\"FIFO_reg\"] = [exponential] + state[\"FIFO_reg\"][:-1]\n",
        "\n",
        "    numerator_sf = state[\"FIFO_reg\"][9]\n",
        "\n",
        "\n",
        "    return numerator_sf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def div_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if y_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "\n",
        "\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "    else:\n",
        "\n",
        "\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return  y_cal, z_cal\n",
        "def cordic_diu_pipe(x_in, y_in):\n",
        "    y_1, z_1 = div_unit(x_in, y_in, 0, 0b0010000000000000, 0)\n",
        "    y_2, z_2 = div_unit(x_in, y_1, z_1, 0b0001000000000000, 1)\n",
        "    y_3, z_3 = div_unit(x_in, y_2, z_2, 0b0000100000000000, 2)\n",
        "    y_4, z_4 = div_unit(x_in, y_3, z_3, 0b0000010000000000, 3)\n",
        "    y_5, z_5 = div_unit(x_in, y_4, z_4, 0b0000001000000000, 4)\n",
        "    y_6, z_6 = div_unit(x_in, y_5, z_5, 0b0000000100000000, 5)\n",
        "    y_7, z_7 = div_unit(x_in, y_6, z_6, 0b0000000010000000, 6)\n",
        "    y_8, z_8 = div_unit(x_in, y_7, z_7, 0b0000000001000000, 7)\n",
        "    y_9, z_9 = div_unit(x_in, y_8, z_8, 0b0000000000100000, 8)\n",
        "    y_10, z_10 = div_unit(x_in, y_9, z_9, 0b0000000000010000, 9)\n",
        "    y_11, z_11 = div_unit(x_in, y_10, z_10, 0b0000000000001000, 10)\n",
        "    y_12, z_12 = div_unit(x_in, y_11, z_11, 0b0000000000000100, 11)\n",
        "    y_13, z_13 = div_unit(x_in, y_12, z_12, 0b0000000000000010, 12)\n",
        "    y_14, z_14 = div_unit(x_in, y_13, z_13, 0b0000000000000001, 13)\n",
        "    divided_val = z_14  # Final z value representing atan(y/x)\n",
        "    return divided_val\n",
        "\n",
        "def cordic_diu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10,data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20):\n",
        "    data_list_x = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_list_y = [data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_diu_pipe(data_list_x[i],data_list_y[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "top(1.548621,\n",
        "2.154268,\n",
        "0.15548,\n",
        "1.358942,\n",
        "3.54985,\n",
        "2.1578712,\n",
        "-1.58952,\n",
        "-0.248623,\n",
        "1.6685123,\n",
        "-2.68856\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JuXVpD_Fjj7q",
        "outputId": "64c79585-e718-4f97-d2e0-7117fc3aa8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0528564453125,\n",
              " 0.0977783203125,\n",
              " 0.0128173828125,\n",
              " 0.0438232421875,\n",
              " 0.3963623046875,\n",
              " 0.0980224609375,\n",
              " 0.0552978515625,\n",
              " 0.0140380859375,\n",
              " 0.0599365234375,\n",
              " 0.1666259765625)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 수정 및 적용 코드"
      ],
      "metadata": {
        "id": "o19OXrVjjoaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "scMdkDy7tAqU",
        "outputId": "94a5a623-027d-4e54-beea-13ed430b7aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassificationModified(BertForSequenceClassification):\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Softmax 대신 CORDIC을 적용\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        return logits, probs\n"
      ],
      "metadata": {
        "id": "XySJ0nStsisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # 여기서 원래 softmax를 적용하는 대신 CORDIC 기반 함수를 사용합니다.\n",
        "        # 예를 들어, top_1200_input(attention_scores)를 사용하여 softmax 결과를 근사합니다.\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        # 만약 반환된 결과가 numpy 형태라면, torch.tensor로 변환해주어야 합니다.\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 원래 출력은 (context_layer, attention_probs) 또는 (context_layer,)인데,\n",
        "        # 필요에 따라 raw attention scores도 반환하도록 할 수 있습니다.\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "# 만약 학습된 가중치를 로드해야 한다면 로드합니다.\n",
        "# model.load_state_dict(torch.load(\"your_checkpoint.pt\"), strict=False)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# 이제 이 모델은 CoLA나 다른 영어 문장 분류 작업에 사용할 수 있습니다.\n",
        "# 예시 추론 함수:\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 문장으로 테스트\n",
        "sentence = \"This is a grammatically acceptable for sentence.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "itlqawMcjmJ3",
        "outputId": "284e5f8a-d05a-4a2e-92d3-7ebec63eaa6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10545\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 22688\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5433707936\n",
            "[TOP] exp_trunc[9] = 331647\n",
            "[TOP] exp_accum_input[9] = 20727\n",
            "[TOP] x_divider = 13294\n",
            "[TOP] y_dividend = 1295\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 18051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4323160347\n",
            "[TOP] exp_trunc[9] = 263864\n",
            "[TOP] exp_accum_input[9] = 16491\n",
            "[TOP] x_divider = 10986\n",
            "[TOP] y_dividend = 1030\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4733418708\n",
            "[TOP] exp_trunc[9] = 288904\n",
            "[TOP] exp_accum_input[9] = 18056\n",
            "[TOP] x_divider = 11030\n",
            "[TOP] y_dividend = 1128\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 36626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8771817122\n",
            "[TOP] exp_trunc[9] = 535389\n",
            "[TOP] exp_accum_input[9] = 33461\n",
            "[TOP] x_divider = 16307\n",
            "[TOP] y_dividend = 2091\n",
            "[TOP] data_out[9] = 1049\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11950\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 18976\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4544695072\n",
            "[TOP] exp_trunc[9] = 277386\n",
            "[TOP] exp_accum_input[9] = 17336\n",
            "[TOP] x_divider = 11789\n",
            "[TOP] y_dividend = 1083\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10760\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 18398\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4406265806\n",
            "[TOP] exp_trunc[9] = 268937\n",
            "[TOP] exp_accum_input[9] = 16808\n",
            "[TOP] x_divider = 10597\n",
            "[TOP] y_dividend = 1050\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 20790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4979142630\n",
            "[TOP] exp_trunc[9] = 303902\n",
            "[TOP] exp_accum_input[9] = 18993\n",
            "[TOP] x_divider = 12566\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10485\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 18111\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4337530167\n",
            "[TOP] exp_trunc[9] = 264741\n",
            "[TOP] exp_accum_input[9] = 16546\n",
            "[TOP] x_divider = 11099\n",
            "[TOP] y_dividend = 1034\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 24696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5914617912\n",
            "[TOP] exp_trunc[9] = 360999\n",
            "[TOP] exp_accum_input[9] = 22562\n",
            "[TOP] x_divider = 12992\n",
            "[TOP] y_dividend = 1410\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 17635\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4223529595\n",
            "[TOP] exp_trunc[9] = 257783\n",
            "[TOP] exp_accum_input[9] = 16111\n",
            "[TOP] x_divider = 11887\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4239336397\n",
            "[TOP] exp_trunc[9] = 258748\n",
            "[TOP] exp_accum_input[9] = 16171\n",
            "[TOP] x_divider = 10796\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 21152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5065840544\n",
            "[TOP] exp_trunc[9] = 309194\n",
            "[TOP] exp_accum_input[9] = 19324\n",
            "[TOP] x_divider = 13778\n",
            "[TOP] y_dividend = 1207\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 25950\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6214947150\n",
            "[TOP] exp_trunc[9] = 379330\n",
            "[TOP] exp_accum_input[9] = 23708\n",
            "[TOP] x_divider = 11736\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 1031\n",
            "[TOP] exp_fraction_output[9] = 21684\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5193252948\n",
            "[TOP] exp_trunc[9] = 316971\n",
            "[TOP] exp_accum_input[9] = 19810\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 33292\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2933224952\n",
            "[TOP] exp_trunc[9] = 179029\n",
            "[TOP] exp_accum_input[9] = 11189\n",
            "[TOP] x_divider = 6314\n",
            "[TOP] y_dividend = 699\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25942\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6213031174\n",
            "[TOP] exp_trunc[9] = 379213\n",
            "[TOP] exp_accum_input[9] = 23700\n",
            "[TOP] x_divider = 15370\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18622\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4459913134\n",
            "[TOP] exp_trunc[9] = 272211\n",
            "[TOP] exp_accum_input[9] = 17013\n",
            "[TOP] x_divider = 11129\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 32763\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7846640211\n",
            "[TOP] exp_trunc[9] = 478920\n",
            "[TOP] exp_accum_input[9] = 29932\n",
            "[TOP] x_divider = 15649\n",
            "[TOP] y_dividend = 1870\n",
            "[TOP] data_out[9] = 979\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 14685\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 581\n",
            "[TOP] exp_fraction_output[9] = 28649\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6861349553\n",
            "[TOP] exp_trunc[9] = 418783\n",
            "[TOP] exp_accum_input[9] = 26173\n",
            "[TOP] x_divider = 12575\n",
            "[TOP] y_dividend = 1635\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 15574\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 22161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5307493017\n",
            "[TOP] exp_trunc[9] = 323943\n",
            "[TOP] exp_accum_input[9] = 20246\n",
            "[TOP] x_divider = 13237\n",
            "[TOP] y_dividend = 1265\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 26216\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6278653352\n",
            "[TOP] exp_trunc[9] = 383218\n",
            "[TOP] exp_accum_input[9] = 23951\n",
            "[TOP] x_divider = 12267\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 42670\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10219336990\n",
            "[TOP] exp_trunc[9] = 623738\n",
            "[TOP] exp_accum_input[9] = 38983\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 2436\n",
            "[TOP] data_out[9] = 1213\n",
            "[TOP] exp_fraction_output[9] = 31661\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7582714517\n",
            "[TOP] exp_trunc[9] = 462812\n",
            "[TOP] exp_accum_input[9] = 28925\n",
            "[TOP] x_divider = 15707\n",
            "[TOP] y_dividend = 1807\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 28966\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6937270102\n",
            "[TOP] exp_trunc[9] = 423417\n",
            "[TOP] exp_accum_input[9] = 26463\n",
            "[TOP] x_divider = 14162\n",
            "[TOP] y_dividend = 1653\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 24792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5937609624\n",
            "[TOP] exp_trunc[9] = 362402\n",
            "[TOP] exp_accum_input[9] = 22650\n",
            "[TOP] x_divider = 11674\n",
            "[TOP] y_dividend = 1415\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 25534\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6115316398\n",
            "[TOP] exp_trunc[9] = 373249\n",
            "[TOP] exp_accum_input[9] = 23328\n",
            "[TOP] x_divider = 12824\n",
            "[TOP] y_dividend = 1458\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 21825\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5227022025\n",
            "[TOP] exp_trunc[9] = 319032\n",
            "[TOP] exp_accum_input[9] = 19939\n",
            "[TOP] x_divider = 12345\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 23341\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5590099477\n",
            "[TOP] exp_trunc[9] = 341192\n",
            "[TOP] exp_accum_input[9] = 21324\n",
            "[TOP] x_divider = 13618\n",
            "[TOP] y_dividend = 1332\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 32814\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7858854558\n",
            "[TOP] exp_trunc[9] = 479666\n",
            "[TOP] exp_accum_input[9] = 29979\n",
            "[TOP] x_divider = 16724\n",
            "[TOP] y_dividend = 1873\n",
            "[TOP] data_out[9] = 917\n",
            "[TOP] exp_fraction_output[9] = 18204\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4359803388\n",
            "[TOP] exp_trunc[9] = 266101\n",
            "[TOP] exp_accum_input[9] = 16631\n",
            "[TOP] x_divider = 11169\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 18150\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4346870550\n",
            "[TOP] exp_trunc[9] = 265311\n",
            "[TOP] exp_accum_input[9] = 16581\n",
            "[TOP] x_divider = 11011\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18631\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4462068607\n",
            "[TOP] exp_trunc[9] = 272343\n",
            "[TOP] exp_accum_input[9] = 17021\n",
            "[TOP] x_divider = 10920\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 19265\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4613909705\n",
            "[TOP] exp_trunc[9] = 281610\n",
            "[TOP] exp_accum_input[9] = 17600\n",
            "[TOP] x_divider = 10973\n",
            "[TOP] y_dividend = 1100\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10828\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18917\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4530564749\n",
            "[TOP] exp_trunc[9] = 276523\n",
            "[TOP] exp_accum_input[9] = 17282\n",
            "[TOP] x_divider = 10840\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 19613\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4697254661\n",
            "[TOP] exp_trunc[9] = 286697\n",
            "[TOP] exp_accum_input[9] = 17918\n",
            "[TOP] x_divider = 11668\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20196\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836881412\n",
            "[TOP] exp_trunc[9] = 295219\n",
            "[TOP] exp_accum_input[9] = 18451\n",
            "[TOP] x_divider = 12199\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 25509\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6109328973\n",
            "[TOP] exp_trunc[9] = 372883\n",
            "[TOP] exp_accum_input[9] = 23305\n",
            "[TOP] x_divider = 13562\n",
            "[TOP] y_dividend = 1456\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4288912276\n",
            "[TOP] exp_trunc[9] = 261774\n",
            "[TOP] exp_accum_input[9] = 16360\n",
            "[TOP] x_divider = 11053\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 18189\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356210933\n",
            "[TOP] exp_trunc[9] = 265882\n",
            "[TOP] exp_accum_input[9] = 16617\n",
            "[TOP] x_divider = 10892\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 779\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 13359\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12605\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 18477\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4425186069\n",
            "[TOP] exp_trunc[9] = 270091\n",
            "[TOP] exp_accum_input[9] = 16880\n",
            "[TOP] x_divider = 14014\n",
            "[TOP] y_dividend = 1055\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 14893\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 20107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4815566179\n",
            "[TOP] exp_trunc[9] = 293918\n",
            "[TOP] exp_accum_input[9] = 18369\n",
            "[TOP] x_divider = 10863\n",
            "[TOP] y_dividend = 1148\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 22856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5473943432\n",
            "[TOP] exp_trunc[9] = 334102\n",
            "[TOP] exp_accum_input[9] = 20881\n",
            "[TOP] x_divider = 14529\n",
            "[TOP] y_dividend = 1305\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 583\n",
            "[TOP] exp_fraction_output[9] = 18548\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4442190356\n",
            "[TOP] exp_trunc[9] = 271129\n",
            "[TOP] exp_accum_input[9] = 16945\n",
            "[TOP] x_divider = 11061\n",
            "[TOP] y_dividend = 1059\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4679531883\n",
            "[TOP] exp_trunc[9] = 285615\n",
            "[TOP] exp_accum_input[9] = 17850\n",
            "[TOP] x_divider = 11685\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 20461\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4900348117\n",
            "[TOP] exp_trunc[9] = 299093\n",
            "[TOP] exp_accum_input[9] = 18693\n",
            "[TOP] x_divider = 11478\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 26894\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6441032318\n",
            "[TOP] exp_trunc[9] = 393129\n",
            "[TOP] exp_accum_input[9] = 24570\n",
            "[TOP] x_divider = 11410\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 19860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4756410420\n",
            "[TOP] exp_trunc[9] = 290308\n",
            "[TOP] exp_accum_input[9] = 18144\n",
            "[TOP] x_divider = 10536\n",
            "[TOP] y_dividend = 1134\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11262\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 839\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11701\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20444\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4896276668\n",
            "[TOP] exp_trunc[9] = 298845\n",
            "[TOP] exp_accum_input[9] = 18677\n",
            "[TOP] x_divider = 11715\n",
            "[TOP] y_dividend = 1167\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 21815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5224627055\n",
            "[TOP] exp_trunc[9] = 318885\n",
            "[TOP] exp_accum_input[9] = 19930\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 21527\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5155651919\n",
            "[TOP] exp_trunc[9] = 314676\n",
            "[TOP] exp_accum_input[9] = 19667\n",
            "[TOP] x_divider = 12489\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185449572\n",
            "[TOP] exp_trunc[9] = 255459\n",
            "[TOP] exp_accum_input[9] = 15966\n",
            "[TOP] x_divider = 11698\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4569123766\n",
            "[TOP] exp_trunc[9] = 278877\n",
            "[TOP] exp_accum_input[9] = 17429\n",
            "[TOP] x_divider = 12182\n",
            "[TOP] y_dividend = 1089\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 17543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4201495871\n",
            "[TOP] exp_trunc[9] = 256438\n",
            "[TOP] exp_accum_input[9] = 16027\n",
            "[TOP] x_divider = 12255\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17766\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254903702\n",
            "[TOP] exp_trunc[9] = 259698\n",
            "[TOP] exp_accum_input[9] = 16231\n",
            "[TOP] x_divider = 10276\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 809\n",
            "[TOP] exp_fraction_output[9] = 20666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949445002\n",
            "[TOP] exp_trunc[9] = 302090\n",
            "[TOP] exp_accum_input[9] = 18880\n",
            "[TOP] x_divider = 10877\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11281\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 26040\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6236501880\n",
            "[TOP] exp_trunc[9] = 380645\n",
            "[TOP] exp_accum_input[9] = 23790\n",
            "[TOP] x_divider = 11992\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 18538\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4439795386\n",
            "[TOP] exp_trunc[9] = 270983\n",
            "[TOP] exp_accum_input[9] = 16936\n",
            "[TOP] x_divider = 11386\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 25020\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5992214940\n",
            "[TOP] exp_trunc[9] = 365735\n",
            "[TOP] exp_accum_input[9] = 22858\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1428\n",
            "[TOP] data_out[9] = 969\n",
            "[TOP] exp_fraction_output[9] = 18292\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380879124\n",
            "[TOP] exp_trunc[9] = 267387\n",
            "[TOP] exp_accum_input[9] = 16711\n",
            "[TOP] x_divider = 11272\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 18294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4381358118\n",
            "[TOP] exp_trunc[9] = 267416\n",
            "[TOP] exp_accum_input[9] = 16713\n",
            "[TOP] x_divider = 11240\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 23762\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5690927714\n",
            "[TOP] exp_trunc[9] = 347346\n",
            "[TOP] exp_accum_input[9] = 21709\n",
            "[TOP] x_divider = 11857\n",
            "[TOP] y_dividend = 1356\n",
            "[TOP] data_out[9] = 937\n",
            "[TOP] exp_fraction_output[9] = 29990\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7182515030\n",
            "[TOP] exp_trunc[9] = 438385\n",
            "[TOP] exp_accum_input[9] = 27399\n",
            "[TOP] x_divider = 12830\n",
            "[TOP] y_dividend = 1712\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 20088\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4811015736\n",
            "[TOP] exp_trunc[9] = 293641\n",
            "[TOP] exp_accum_input[9] = 18352\n",
            "[TOP] x_divider = 11929\n",
            "[TOP] y_dividend = 1147\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 26891\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6440313827\n",
            "[TOP] exp_trunc[9] = 393085\n",
            "[TOP] exp_accum_input[9] = 24567\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1071\n",
            "[TOP] exp_fraction_output[9] = 19436\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4654863692\n",
            "[TOP] exp_trunc[9] = 284110\n",
            "[TOP] exp_accum_input[9] = 17756\n",
            "[TOP] x_divider = 11596\n",
            "[TOP] y_dividend = 1109\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 18565\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4446261805\n",
            "[TOP] exp_trunc[9] = 271378\n",
            "[TOP] exp_accum_input[9] = 16961\n",
            "[TOP] x_divider = 11838\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 11210\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 25440\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6092803680\n",
            "[TOP] exp_trunc[9] = 371875\n",
            "[TOP] exp_accum_input[9] = 23242\n",
            "[TOP] x_divider = 12640\n",
            "[TOP] y_dividend = 1452\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 12939\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 1003\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11779\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4664443572\n",
            "[TOP] exp_trunc[9] = 284695\n",
            "[TOP] exp_accum_input[9] = 17793\n",
            "[TOP] x_divider = 12994\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17528\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4197903416\n",
            "[TOP] exp_trunc[9] = 256219\n",
            "[TOP] exp_accum_input[9] = 16013\n",
            "[TOP] x_divider = 12928\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 22511\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5391316967\n",
            "[TOP] exp_trunc[9] = 329059\n",
            "[TOP] exp_accum_input[9] = 20566\n",
            "[TOP] x_divider = 13605\n",
            "[TOP] y_dividend = 1285\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 19154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4587325538\n",
            "[TOP] exp_trunc[9] = 279988\n",
            "[TOP] exp_accum_input[9] = 17499\n",
            "[TOP] x_divider = 11381\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4191915991\n",
            "[TOP] exp_trunc[9] = 255854\n",
            "[TOP] exp_accum_input[9] = 15990\n",
            "[TOP] x_divider = 10757\n",
            "[TOP] y_dividend = 999\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 18288\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4379921136\n",
            "[TOP] exp_trunc[9] = 267329\n",
            "[TOP] exp_accum_input[9] = 16708\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 19604\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4695099188\n",
            "[TOP] exp_trunc[9] = 286566\n",
            "[TOP] exp_accum_input[9] = 17910\n",
            "[TOP] x_divider = 11040\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18929\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4533438713\n",
            "[TOP] exp_trunc[9] = 276699\n",
            "[TOP] exp_accum_input[9] = 17293\n",
            "[TOP] x_divider = 11820\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 19510\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4672586470\n",
            "[TOP] exp_trunc[9] = 285192\n",
            "[TOP] exp_accum_input[9] = 17824\n",
            "[TOP] x_divider = 10950\n",
            "[TOP] y_dividend = 1114\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 23850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5712003450\n",
            "[TOP] exp_trunc[9] = 348633\n",
            "[TOP] exp_accum_input[9] = 21789\n",
            "[TOP] x_divider = 12004\n",
            "[TOP] y_dividend = 1361\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 17605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4216344685\n",
            "[TOP] exp_trunc[9] = 257345\n",
            "[TOP] exp_accum_input[9] = 16084\n",
            "[TOP] x_divider = 10933\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4255861690\n",
            "[TOP] exp_trunc[9] = 259757\n",
            "[TOP] exp_accum_input[9] = 16234\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21731\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5204509307\n",
            "[TOP] exp_trunc[9] = 317658\n",
            "[TOP] exp_accum_input[9] = 19853\n",
            "[TOP] x_divider = 11488\n",
            "[TOP] y_dividend = 1240\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 17835\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271428995\n",
            "[TOP] exp_trunc[9] = 260707\n",
            "[TOP] exp_accum_input[9] = 16294\n",
            "[TOP] x_divider = 11475\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4513081468\n",
            "[TOP] exp_trunc[9] = 275456\n",
            "[TOP] exp_accum_input[9] = 17216\n",
            "[TOP] x_divider = 11140\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585170065\n",
            "[TOP] exp_trunc[9] = 279856\n",
            "[TOP] exp_accum_input[9] = 17491\n",
            "[TOP] x_divider = 12620\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11781\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12720\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10502\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 26868\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6434805396\n",
            "[TOP] exp_trunc[9] = 392749\n",
            "[TOP] exp_accum_input[9] = 24546\n",
            "[TOP] x_divider = 16212\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 17779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4258017163\n",
            "[TOP] exp_trunc[9] = 259888\n",
            "[TOP] exp_accum_input[9] = 16243\n",
            "[TOP] x_divider = 11161\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 19338\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4631392986\n",
            "[TOP] exp_trunc[9] = 282677\n",
            "[TOP] exp_accum_input[9] = 17667\n",
            "[TOP] x_divider = 12405\n",
            "[TOP] y_dividend = 1104\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 24236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5804449292\n",
            "[TOP] exp_trunc[9] = 354275\n",
            "[TOP] exp_accum_input[9] = 22142\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 1383\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 23326\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586507022\n",
            "[TOP] exp_trunc[9] = 340973\n",
            "[TOP] exp_accum_input[9] = 21310\n",
            "[TOP] x_divider = 12019\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 907\n",
            "[TOP] exp_fraction_output[9] = 17198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4118869406\n",
            "[TOP] exp_trunc[9] = 251395\n",
            "[TOP] exp_accum_input[9] = 15712\n",
            "[TOP] x_divider = 12154\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 19165\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4589960005\n",
            "[TOP] exp_trunc[9] = 280148\n",
            "[TOP] exp_accum_input[9] = 17509\n",
            "[TOP] x_divider = 13570\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 19628\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4700847116\n",
            "[TOP] exp_trunc[9] = 286916\n",
            "[TOP] exp_accum_input[9] = 17932\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1120\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 19049\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562178353\n",
            "[TOP] exp_trunc[9] = 278453\n",
            "[TOP] exp_accum_input[9] = 17403\n",
            "[TOP] x_divider = 13609\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 23612\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5655003164\n",
            "[TOP] exp_trunc[9] = 345154\n",
            "[TOP] exp_accum_input[9] = 21572\n",
            "[TOP] x_divider = 11212\n",
            "[TOP] y_dividend = 1348\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 26603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6371338691\n",
            "[TOP] exp_trunc[9] = 388875\n",
            "[TOP] exp_accum_input[9] = 24304\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 1519\n",
            "[TOP] data_out[9] = 1093\n",
            "[TOP] exp_fraction_output[9] = 19570\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4686956290\n",
            "[TOP] exp_trunc[9] = 286069\n",
            "[TOP] exp_accum_input[9] = 17879\n",
            "[TOP] x_divider = 11083\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17699\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4238857403\n",
            "[TOP] exp_trunc[9] = 258719\n",
            "[TOP] exp_accum_input[9] = 16169\n",
            "[TOP] x_divider = 11586\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20194\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836402418\n",
            "[TOP] exp_trunc[9] = 295190\n",
            "[TOP] exp_accum_input[9] = 18449\n",
            "[TOP] x_divider = 11269\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4200537883\n",
            "[TOP] exp_trunc[9] = 256380\n",
            "[TOP] exp_accum_input[9] = 16023\n",
            "[TOP] x_divider = 12494\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 24900\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963475300\n",
            "[TOP] exp_trunc[9] = 363981\n",
            "[TOP] exp_accum_input[9] = 22748\n",
            "[TOP] x_divider = 12659\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 25175\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6029336975\n",
            "[TOP] exp_trunc[9] = 368001\n",
            "[TOP] exp_accum_input[9] = 23000\n",
            "[TOP] x_divider = 13321\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 17880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4282206360\n",
            "[TOP] exp_trunc[9] = 261365\n",
            "[TOP] exp_accum_input[9] = 16335\n",
            "[TOP] x_divider = 11318\n",
            "[TOP] y_dividend = 1020\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 23240\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5565910280\n",
            "[TOP] exp_trunc[9] = 339716\n",
            "[TOP] exp_accum_input[9] = 21232\n",
            "[TOP] x_divider = 11637\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 29592\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7087195224\n",
            "[TOP] exp_trunc[9] = 432568\n",
            "[TOP] exp_accum_input[9] = 27035\n",
            "[TOP] x_divider = 14586\n",
            "[TOP] y_dividend = 1689\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 19549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4681926853\n",
            "[TOP] exp_trunc[9] = 285762\n",
            "[TOP] exp_accum_input[9] = 17860\n",
            "[TOP] x_divider = 12147\n",
            "[TOP] y_dividend = 1116\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4974592187\n",
            "[TOP] exp_trunc[9] = 303625\n",
            "[TOP] exp_accum_input[9] = 18976\n",
            "[TOP] x_divider = 13006\n",
            "[TOP] y_dividend = 1186\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12027\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 32790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7853106630\n",
            "[TOP] exp_trunc[9] = 479315\n",
            "[TOP] exp_accum_input[9] = 29957\n",
            "[TOP] x_divider = 15772\n",
            "[TOP] y_dividend = 1872\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 17904\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4287954288\n",
            "[TOP] exp_trunc[9] = 261715\n",
            "[TOP] exp_accum_input[9] = 16357\n",
            "[TOP] x_divider = 12088\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 18765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4494161205\n",
            "[TOP] exp_trunc[9] = 274301\n",
            "[TOP] exp_accum_input[9] = 17143\n",
            "[TOP] x_divider = 12011\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 14106\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 18408\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4408660776\n",
            "[TOP] exp_trunc[9] = 269083\n",
            "[TOP] exp_accum_input[9] = 16817\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 23801\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5700268097\n",
            "[TOP] exp_trunc[9] = 347916\n",
            "[TOP] exp_accum_input[9] = 21744\n",
            "[TOP] x_divider = 12166\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20608\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4935554176\n",
            "[TOP] exp_trunc[9] = 301242\n",
            "[TOP] exp_accum_input[9] = 18827\n",
            "[TOP] x_divider = 10942\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17837\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271907989\n",
            "[TOP] exp_trunc[9] = 260736\n",
            "[TOP] exp_accum_input[9] = 16296\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 18665\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4470211505\n",
            "[TOP] exp_trunc[9] = 272840\n",
            "[TOP] exp_accum_input[9] = 17052\n",
            "[TOP] x_divider = 13420\n",
            "[TOP] y_dividend = 1065\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12270\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 651\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10312\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 20152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4826343544\n",
            "[TOP] exp_trunc[9] = 294576\n",
            "[TOP] exp_accum_input[9] = 18411\n",
            "[TOP] x_divider = 11172\n",
            "[TOP] y_dividend = 1150\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17318\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4147609046\n",
            "[TOP] exp_trunc[9] = 253149\n",
            "[TOP] exp_accum_input[9] = 15821\n",
            "[TOP] x_divider = 10474\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18540\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4440274380\n",
            "[TOP] exp_trunc[9] = 271012\n",
            "[TOP] exp_accum_input[9] = 16938\n",
            "[TOP] x_divider = 10600\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22703\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5437300391\n",
            "[TOP] exp_trunc[9] = 331866\n",
            "[TOP] exp_accum_input[9] = 20741\n",
            "[TOP] x_divider = 11384\n",
            "[TOP] y_dividend = 1296\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 18629\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4461589613\n",
            "[TOP] exp_trunc[9] = 272313\n",
            "[TOP] exp_accum_input[9] = 17019\n",
            "[TOP] x_divider = 11241\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18758\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4492484726\n",
            "[TOP] exp_trunc[9] = 274199\n",
            "[TOP] exp_accum_input[9] = 17137\n",
            "[TOP] x_divider = 11123\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 17259\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4133478723\n",
            "[TOP] exp_trunc[9] = 252287\n",
            "[TOP] exp_accum_input[9] = 15767\n",
            "[TOP] x_divider = 11648\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 20459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4899869123\n",
            "[TOP] exp_trunc[9] = 299064\n",
            "[TOP] exp_accum_input[9] = 18691\n",
            "[TOP] x_divider = 11372\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 19391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4644086327\n",
            "[TOP] exp_trunc[9] = 283452\n",
            "[TOP] exp_accum_input[9] = 17715\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1107\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 601\n",
            "[TOP] exp_fraction_output[9] = 23924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5729726228\n",
            "[TOP] exp_trunc[9] = 349714\n",
            "[TOP] exp_accum_input[9] = 21857\n",
            "[TOP] x_divider = 15161\n",
            "[TOP] y_dividend = 1366\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18084\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4331063748\n",
            "[TOP] exp_trunc[9] = 264347\n",
            "[TOP] exp_accum_input[9] = 16521\n",
            "[TOP] x_divider = 11549\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20389\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4883104333\n",
            "[TOP] exp_trunc[9] = 298041\n",
            "[TOP] exp_accum_input[9] = 18627\n",
            "[TOP] x_divider = 12473\n",
            "[TOP] y_dividend = 1164\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 11803\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12347\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 19005\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4551640485\n",
            "[TOP] exp_trunc[9] = 277810\n",
            "[TOP] exp_accum_input[9] = 17363\n",
            "[TOP] x_divider = 10852\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20587\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4930524739\n",
            "[TOP] exp_trunc[9] = 300935\n",
            "[TOP] exp_accum_input[9] = 18808\n",
            "[TOP] x_divider = 11912\n",
            "[TOP] y_dividend = 1175\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 18433\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4414648201\n",
            "[TOP] exp_trunc[9] = 269448\n",
            "[TOP] exp_accum_input[9] = 16840\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1052\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11200\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 715\n",
            "[TOP] exp_fraction_output[9] = 19320\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627082040\n",
            "[TOP] exp_trunc[9] = 282414\n",
            "[TOP] exp_accum_input[9] = 17650\n",
            "[TOP] x_divider = 10945\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229996014\n",
            "[TOP] exp_trunc[9] = 258178\n",
            "[TOP] exp_accum_input[9] = 16136\n",
            "[TOP] x_divider = 12042\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17425\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173235225\n",
            "[TOP] exp_trunc[9] = 254714\n",
            "[TOP] exp_accum_input[9] = 15919\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 994\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 18411\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4409379267\n",
            "[TOP] exp_trunc[9] = 269127\n",
            "[TOP] exp_accum_input[9] = 16820\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 21515\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5152777955\n",
            "[TOP] exp_trunc[9] = 314500\n",
            "[TOP] exp_accum_input[9] = 19656\n",
            "[TOP] x_divider = 11338\n",
            "[TOP] y_dividend = 1228\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 21738\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5206185786\n",
            "[TOP] exp_trunc[9] = 317760\n",
            "[TOP] exp_accum_input[9] = 19860\n",
            "[TOP] x_divider = 10818\n",
            "[TOP] y_dividend = 1241\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 27987\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6702802539\n",
            "[TOP] exp_trunc[9] = 409106\n",
            "[TOP] exp_accum_input[9] = 25569\n",
            "[TOP] x_divider = 12986\n",
            "[TOP] y_dividend = 1598\n",
            "[TOP] data_out[9] = 1009\n",
            "[TOP] exp_fraction_output[9] = 18858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4516434426\n",
            "[TOP] exp_trunc[9] = 275661\n",
            "[TOP] exp_accum_input[9] = 17228\n",
            "[TOP] x_divider = 10406\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12028\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 18833\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4510447001\n",
            "[TOP] exp_trunc[9] = 275295\n",
            "[TOP] exp_accum_input[9] = 17205\n",
            "[TOP] x_divider = 11231\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 19012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553316964\n",
            "[TOP] exp_trunc[9] = 277912\n",
            "[TOP] exp_accum_input[9] = 17369\n",
            "[TOP] x_divider = 11886\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 18965\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4542060605\n",
            "[TOP] exp_trunc[9] = 277225\n",
            "[TOP] exp_accum_input[9] = 17326\n",
            "[TOP] x_divider = 11516\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 19802\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4742519594\n",
            "[TOP] exp_trunc[9] = 289460\n",
            "[TOP] exp_accum_input[9] = 18091\n",
            "[TOP] x_divider = 10443\n",
            "[TOP] y_dividend = 1130\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 17212\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122222364\n",
            "[TOP] exp_trunc[9] = 251600\n",
            "[TOP] exp_accum_input[9] = 15725\n",
            "[TOP] x_divider = 10064\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13551\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 589\n",
            "[TOP] exp_fraction_output[9] = 21101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5053626197\n",
            "[TOP] exp_trunc[9] = 308448\n",
            "[TOP] exp_accum_input[9] = 19278\n",
            "[TOP] x_divider = 12093\n",
            "[TOP] y_dividend = 1204\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 19855\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4755212935\n",
            "[TOP] exp_trunc[9] = 290235\n",
            "[TOP] exp_accum_input[9] = 18139\n",
            "[TOP] x_divider = 11830\n",
            "[TOP] y_dividend = 1133\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10872\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914478440\n",
            "[TOP] exp_trunc[9] = 299955\n",
            "[TOP] exp_accum_input[9] = 18747\n",
            "[TOP] x_divider = 12614\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17681\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4234546457\n",
            "[TOP] exp_trunc[9] = 258456\n",
            "[TOP] exp_accum_input[9] = 16153\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 17618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4219458146\n",
            "[TOP] exp_trunc[9] = 257535\n",
            "[TOP] exp_accum_input[9] = 16095\n",
            "[TOP] x_divider = 10811\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 23311\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5582914567\n",
            "[TOP] exp_trunc[9] = 340754\n",
            "[TOP] exp_accum_input[9] = 21297\n",
            "[TOP] x_divider = 12055\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 903\n",
            "[TOP] exp_fraction_output[9] = 27549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6597902853\n",
            "[TOP] exp_trunc[9] = 402704\n",
            "[TOP] exp_accum_input[9] = 25169\n",
            "[TOP] x_divider = 11877\n",
            "[TOP] y_dividend = 1573\n",
            "[TOP] data_out[9] = 1085\n",
            "[TOP] exp_fraction_output[9] = 23100\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5532380700\n",
            "[TOP] exp_trunc[9] = 337669\n",
            "[TOP] exp_accum_input[9] = 21104\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 11257\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12119\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11278\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12672\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 18290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380400130\n",
            "[TOP] exp_trunc[9] = 267358\n",
            "[TOP] exp_accum_input[9] = 16709\n",
            "[TOP] x_divider = 15782\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 541\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 15434\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 519\n",
            "[TOP] exp_fraction_output[9] = 23491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626024027\n",
            "[TOP] exp_trunc[9] = 343385\n",
            "[TOP] exp_accum_input[9] = 21461\n",
            "[TOP] x_divider = 11613\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6140463583\n",
            "[TOP] exp_trunc[9] = 374784\n",
            "[TOP] exp_accum_input[9] = 23424\n",
            "[TOP] x_divider = 11406\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 1051\n",
            "[TOP] exp_fraction_output[9] = 27184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6510486448\n",
            "[TOP] exp_trunc[9] = 397368\n",
            "[TOP] exp_accum_input[9] = 24835\n",
            "[TOP] x_divider = 10939\n",
            "[TOP] y_dividend = 1552\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 38909\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 9318588773\n",
            "[TOP] exp_trunc[9] = 568761\n",
            "[TOP] exp_accum_input[9] = 35547\n",
            "[TOP] x_divider = 14650\n",
            "[TOP] y_dividend = 2221\n",
            "[TOP] data_out[9] = 1241\n",
            "[TOP] exp_fraction_output[9] = 18880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4521703360\n",
            "[TOP] exp_trunc[9] = 275982\n",
            "[TOP] exp_accum_input[9] = 17248\n",
            "[TOP] x_divider = 11160\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18898\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4526014306\n",
            "[TOP] exp_trunc[9] = 276245\n",
            "[TOP] exp_accum_input[9] = 17265\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11882\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6030534460\n",
            "[TOP] exp_trunc[9] = 368074\n",
            "[TOP] exp_accum_input[9] = 23004\n",
            "[TOP] x_divider = 11702\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 1007\n",
            "[TOP] exp_fraction_output[9] = 33438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8008300686\n",
            "[TOP] exp_trunc[9] = 488787\n",
            "[TOP] exp_accum_input[9] = 30549\n",
            "[TOP] x_divider = 13486\n",
            "[TOP] y_dividend = 1909\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13624\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 26312\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6301645064\n",
            "[TOP] exp_trunc[9] = 384621\n",
            "[TOP] exp_accum_input[9] = 24038\n",
            "[TOP] x_divider = 13254\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 26719\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6399120343\n",
            "[TOP] exp_trunc[9] = 390571\n",
            "[TOP] exp_accum_input[9] = 24410\n",
            "[TOP] x_divider = 13958\n",
            "[TOP] y_dividend = 1525\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 21453\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5137929141\n",
            "[TOP] exp_trunc[9] = 313594\n",
            "[TOP] exp_accum_input[9] = 19599\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1224\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 20994\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5028000018\n",
            "[TOP] exp_trunc[9] = 306884\n",
            "[TOP] exp_accum_input[9] = 19180\n",
            "[TOP] x_divider = 12802\n",
            "[TOP] y_dividend = 1198\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 11437\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11430\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4270471007\n",
            "[TOP] exp_trunc[9] = 260648\n",
            "[TOP] exp_accum_input[9] = 16290\n",
            "[TOP] x_divider = 10452\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 21132\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5061050604\n",
            "[TOP] exp_trunc[9] = 308902\n",
            "[TOP] exp_accum_input[9] = 19306\n",
            "[TOP] x_divider = 12565\n",
            "[TOP] y_dividend = 1206\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20685\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4953995445\n",
            "[TOP] exp_trunc[9] = 302367\n",
            "[TOP] exp_accum_input[9] = 18897\n",
            "[TOP] x_divider = 11166\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 19007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4552119479\n",
            "[TOP] exp_trunc[9] = 277839\n",
            "[TOP] exp_accum_input[9] = 17364\n",
            "[TOP] x_divider = 10533\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 27234\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6522461298\n",
            "[TOP] exp_trunc[9] = 398099\n",
            "[TOP] exp_accum_input[9] = 24881\n",
            "[TOP] x_divider = 14731\n",
            "[TOP] y_dividend = 1555\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 18810\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4504938570\n",
            "[TOP] exp_trunc[9] = 274959\n",
            "[TOP] exp_accum_input[9] = 17184\n",
            "[TOP] x_divider = 12703\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 19595\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4692943715\n",
            "[TOP] exp_trunc[9] = 286434\n",
            "[TOP] exp_accum_input[9] = 17902\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1118\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 24646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5902643062\n",
            "[TOP] exp_trunc[9] = 360268\n",
            "[TOP] exp_accum_input[9] = 22516\n",
            "[TOP] x_divider = 11314\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 1021\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4217781667\n",
            "[TOP] exp_trunc[9] = 257432\n",
            "[TOP] exp_accum_input[9] = 16089\n",
            "[TOP] x_divider = 10966\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 13581\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 607\n",
            "[TOP] exp_fraction_output[9] = 19494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668754518\n",
            "[TOP] exp_trunc[9] = 284958\n",
            "[TOP] exp_accum_input[9] = 17809\n",
            "[TOP] x_divider = 12558\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 19147\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585649059\n",
            "[TOP] exp_trunc[9] = 279885\n",
            "[TOP] exp_accum_input[9] = 17492\n",
            "[TOP] x_divider = 12142\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4792813964\n",
            "[TOP] exp_trunc[9] = 292530\n",
            "[TOP] exp_accum_input[9] = 18283\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1142\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 18371\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4399799387\n",
            "[TOP] exp_trunc[9] = 268542\n",
            "[TOP] exp_accum_input[9] = 16783\n",
            "[TOP] x_divider = 10865\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19498\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4669712506\n",
            "[TOP] exp_trunc[9] = 285016\n",
            "[TOP] exp_accum_input[9] = 17813\n",
            "[TOP] x_divider = 12894\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 23607\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653805679\n",
            "[TOP] exp_trunc[9] = 345080\n",
            "[TOP] exp_accum_input[9] = 21567\n",
            "[TOP] x_divider = 12525\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 11684\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 41979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10053844563\n",
            "[TOP] exp_trunc[9] = 613637\n",
            "[TOP] exp_accum_input[9] = 38352\n",
            "[TOP] x_divider = 19046\n",
            "[TOP] y_dividend = 2397\n",
            "[TOP] data_out[9] = 1029\n",
            "[TOP] exp_fraction_output[9] = 27869\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6674541893\n",
            "[TOP] exp_trunc[9] = 407381\n",
            "[TOP] exp_accum_input[9] = 25461\n",
            "[TOP] x_divider = 15936\n",
            "[TOP] y_dividend = 1591\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 18222\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4364114334\n",
            "[TOP] exp_trunc[9] = 266364\n",
            "[TOP] exp_accum_input[9] = 16647\n",
            "[TOP] x_divider = 12675\n",
            "[TOP] y_dividend = 1040\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 23031\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2029169286\n",
            "[TOP] exp_trunc[9] = 123850\n",
            "[TOP] exp_accum_input[9] = 7740\n",
            "[TOP] x_divider = 6434\n",
            "[TOP] y_dividend = 483\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 28910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6923858270\n",
            "[TOP] exp_trunc[9] = 422598\n",
            "[TOP] exp_accum_input[9] = 26412\n",
            "[TOP] x_divider = 14346\n",
            "[TOP] y_dividend = 1650\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 26208\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6276737376\n",
            "[TOP] exp_trunc[9] = 383101\n",
            "[TOP] exp_accum_input[9] = 23943\n",
            "[TOP] x_divider = 16608\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 739\n",
            "[TOP] exp_fraction_output[9] = 22906\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5485918282\n",
            "[TOP] exp_trunc[9] = 334833\n",
            "[TOP] exp_accum_input[9] = 20927\n",
            "[TOP] x_divider = 11539\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19317\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4626363549\n",
            "[TOP] exp_trunc[9] = 282370\n",
            "[TOP] exp_accum_input[9] = 17648\n",
            "[TOP] x_divider = 11482\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 22958\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5498372126\n",
            "[TOP] exp_trunc[9] = 335594\n",
            "[TOP] exp_accum_input[9] = 20974\n",
            "[TOP] x_divider = 13617\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 20526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4915915422\n",
            "[TOP] exp_trunc[9] = 300043\n",
            "[TOP] exp_accum_input[9] = 18752\n",
            "[TOP] x_divider = 11517\n",
            "[TOP] y_dividend = 1172\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 19491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668036027\n",
            "[TOP] exp_trunc[9] = 284914\n",
            "[TOP] exp_accum_input[9] = 17807\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 21679\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5192055463\n",
            "[TOP] exp_trunc[9] = 316897\n",
            "[TOP] exp_accum_input[9] = 19806\n",
            "[TOP] x_divider = 12265\n",
            "[TOP] y_dividend = 1237\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 27746\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6645083762\n",
            "[TOP] exp_trunc[9] = 405583\n",
            "[TOP] exp_accum_input[9] = 25348\n",
            "[TOP] x_divider = 11529\n",
            "[TOP] y_dividend = 1584\n",
            "[TOP] data_out[9] = 1125\n",
            "[TOP] exp_fraction_output[9] = 26184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6270989448\n",
            "[TOP] exp_trunc[9] = 382750\n",
            "[TOP] exp_accum_input[9] = 23921\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1069\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11261\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 17854\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275979438\n",
            "[TOP] exp_trunc[9] = 260985\n",
            "[TOP] exp_accum_input[9] = 16311\n",
            "[TOP] x_divider = 13483\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 17520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195987440\n",
            "[TOP] exp_trunc[9] = 256102\n",
            "[TOP] exp_accum_input[9] = 16006\n",
            "[TOP] x_divider = 11555\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 28729\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6880509313\n",
            "[TOP] exp_trunc[9] = 419952\n",
            "[TOP] exp_accum_input[9] = 26247\n",
            "[TOP] x_divider = 13307\n",
            "[TOP] y_dividend = 1640\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10854\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 22146\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5303900562\n",
            "[TOP] exp_trunc[9] = 323724\n",
            "[TOP] exp_accum_input[9] = 20232\n",
            "[TOP] x_divider = 13327\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 19051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562657347\n",
            "[TOP] exp_trunc[9] = 278482\n",
            "[TOP] exp_accum_input[9] = 17405\n",
            "[TOP] x_divider = 12252\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 21537\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5158046889\n",
            "[TOP] exp_trunc[9] = 314822\n",
            "[TOP] exp_accum_input[9] = 19676\n",
            "[TOP] x_divider = 11224\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 21039\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5038777383\n",
            "[TOP] exp_trunc[9] = 307542\n",
            "[TOP] exp_accum_input[9] = 19221\n",
            "[TOP] x_divider = 13680\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4390937998\n",
            "[TOP] exp_trunc[9] = 268001\n",
            "[TOP] exp_accum_input[9] = 16750\n",
            "[TOP] x_divider = 11223\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12122\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 17458\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4181138626\n",
            "[TOP] exp_trunc[9] = 255196\n",
            "[TOP] exp_accum_input[9] = 15949\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4247000301\n",
            "[TOP] exp_trunc[9] = 259216\n",
            "[TOP] exp_accum_input[9] = 16201\n",
            "[TOP] x_divider = 11681\n",
            "[TOP] y_dividend = 1012\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17714\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4242449858\n",
            "[TOP] exp_trunc[9] = 258938\n",
            "[TOP] exp_accum_input[9] = 16183\n",
            "[TOP] x_divider = 11219\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 24467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5859773099\n",
            "[TOP] exp_trunc[9] = 357652\n",
            "[TOP] exp_accum_input[9] = 22353\n",
            "[TOP] x_divider = 13017\n",
            "[TOP] y_dividend = 1397\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 17936\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4295618192\n",
            "[TOP] exp_trunc[9] = 262183\n",
            "[TOP] exp_accum_input[9] = 16386\n",
            "[TOP] x_divider = 11293\n",
            "[TOP] y_dividend = 1024\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 20737\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4966449289\n",
            "[TOP] exp_trunc[9] = 303128\n",
            "[TOP] exp_accum_input[9] = 18945\n",
            "[TOP] x_divider = 11408\n",
            "[TOP] y_dividend = 1184\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 18908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4528409276\n",
            "[TOP] exp_trunc[9] = 276392\n",
            "[TOP] exp_accum_input[9] = 17274\n",
            "[TOP] x_divider = 12646\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 21331\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5108710507\n",
            "[TOP] exp_trunc[9] = 311810\n",
            "[TOP] exp_accum_input[9] = 19488\n",
            "[TOP] x_divider = 12179\n",
            "[TOP] y_dividend = 1218\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 26471\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6339725087\n",
            "[TOP] exp_trunc[9] = 386946\n",
            "[TOP] exp_accum_input[9] = 24184\n",
            "[TOP] x_divider = 14785\n",
            "[TOP] y_dividend = 1511\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 29843\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7147308971\n",
            "[TOP] exp_trunc[9] = 436237\n",
            "[TOP] exp_accum_input[9] = 27264\n",
            "[TOP] x_divider = 12719\n",
            "[TOP] y_dividend = 1704\n",
            "[TOP] data_out[9] = 1095\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 13229\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 32217\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2838511002\n",
            "[TOP] exp_trunc[9] = 173248\n",
            "[TOP] exp_accum_input[9] = 10828\n",
            "[TOP] x_divider = 7613\n",
            "[TOP] y_dividend = 676\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12065\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 22273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5334316681\n",
            "[TOP] exp_trunc[9] = 325580\n",
            "[TOP] exp_accum_input[9] = 20348\n",
            "[TOP] x_divider = 13500\n",
            "[TOP] y_dividend = 1271\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 17379\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4162218363\n",
            "[TOP] exp_trunc[9] = 254041\n",
            "[TOP] exp_accum_input[9] = 15877\n",
            "[TOP] x_divider = 11481\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5789121484\n",
            "[TOP] exp_trunc[9] = 353339\n",
            "[TOP] exp_accum_input[9] = 22083\n",
            "[TOP] x_divider = 16257\n",
            "[TOP] y_dividend = 1380\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17671\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4232151487\n",
            "[TOP] exp_trunc[9] = 258310\n",
            "[TOP] exp_accum_input[9] = 16144\n",
            "[TOP] x_divider = 11825\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 23637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5660990589\n",
            "[TOP] exp_trunc[9] = 345519\n",
            "[TOP] exp_accum_input[9] = 21594\n",
            "[TOP] x_divider = 12438\n",
            "[TOP] y_dividend = 1349\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 12744\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5048117766\n",
            "[TOP] exp_trunc[9] = 308112\n",
            "[TOP] exp_accum_input[9] = 19257\n",
            "[TOP] x_divider = 13302\n",
            "[TOP] y_dividend = 1203\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 27363\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6553356411\n",
            "[TOP] exp_trunc[9] = 399985\n",
            "[TOP] exp_accum_input[9] = 24999\n",
            "[TOP] x_divider = 16020\n",
            "[TOP] y_dividend = 1562\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 22149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5304619053\n",
            "[TOP] exp_trunc[9] = 323768\n",
            "[TOP] exp_accum_input[9] = 20235\n",
            "[TOP] x_divider = 11791\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 877\n",
            "[TOP] exp_fraction_output[9] = 19322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627561034\n",
            "[TOP] exp_trunc[9] = 282443\n",
            "[TOP] exp_accum_input[9] = 17652\n",
            "[TOP] x_divider = 11394\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11163\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 12123\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 18896\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4525535312\n",
            "[TOP] exp_trunc[9] = 276216\n",
            "[TOP] exp_accum_input[9] = 17263\n",
            "[TOP] x_divider = 11031\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 18569\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4447219793\n",
            "[TOP] exp_trunc[9] = 271436\n",
            "[TOP] exp_accum_input[9] = 16964\n",
            "[TOP] x_divider = 12091\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 19172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4591636484\n",
            "[TOP] exp_trunc[9] = 280251\n",
            "[TOP] exp_accum_input[9] = 17515\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 18519\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4435244943\n",
            "[TOP] exp_trunc[9] = 270705\n",
            "[TOP] exp_accum_input[9] = 16919\n",
            "[TOP] x_divider = 12249\n",
            "[TOP] y_dividend = 1057\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 26186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6271468442\n",
            "[TOP] exp_trunc[9] = 382780\n",
            "[TOP] exp_accum_input[9] = 23923\n",
            "[TOP] x_divider = 11360\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1077\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10860\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18776\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4496795672\n",
            "[TOP] exp_trunc[9] = 274462\n",
            "[TOP] exp_accum_input[9] = 17153\n",
            "[TOP] x_divider = 11153\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18724\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4484341828\n",
            "[TOP] exp_trunc[9] = 273702\n",
            "[TOP] exp_accum_input[9] = 17106\n",
            "[TOP] x_divider = 11072\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11000\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 28198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6753336406\n",
            "[TOP] exp_trunc[9] = 412190\n",
            "[TOP] exp_accum_input[9] = 25761\n",
            "[TOP] x_divider = 15249\n",
            "[TOP] y_dividend = 1610\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 28591\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6847458727\n",
            "[TOP] exp_trunc[9] = 417935\n",
            "[TOP] exp_accum_input[9] = 26120\n",
            "[TOP] x_divider = 13872\n",
            "[TOP] y_dividend = 1632\n",
            "[TOP] data_out[9] = 965\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20319\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4866339543\n",
            "[TOP] exp_trunc[9] = 297017\n",
            "[TOP] exp_accum_input[9] = 18563\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 1160\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13427\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 22601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412871697\n",
            "[TOP] exp_trunc[9] = 330375\n",
            "[TOP] exp_accum_input[9] = 20648\n",
            "[TOP] x_divider = 13130\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 37387\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8954074339\n",
            "[TOP] exp_trunc[9] = 546513\n",
            "[TOP] exp_accum_input[9] = 34157\n",
            "[TOP] x_divider = 16787\n",
            "[TOP] y_dividend = 2134\n",
            "[TOP] data_out[9] = 1039\n",
            "[TOP] exp_fraction_output[9] = 25840\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6188602480\n",
            "[TOP] exp_trunc[9] = 377722\n",
            "[TOP] exp_accum_input[9] = 23607\n",
            "[TOP] x_divider = 12098\n",
            "[TOP] y_dividend = 1475\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 35844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8584530468\n",
            "[TOP] exp_trunc[9] = 523958\n",
            "[TOP] exp_accum_input[9] = 32747\n",
            "[TOP] x_divider = 13716\n",
            "[TOP] y_dividend = 2046\n",
            "[TOP] data_out[9] = 1221\n",
            "[TOP] exp_fraction_output[9] = 25658\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6145014026\n",
            "[TOP] exp_trunc[9] = 375061\n",
            "[TOP] exp_accum_input[9] = 23441\n",
            "[TOP] x_divider = 13429\n",
            "[TOP] y_dividend = 1465\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20183\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4833767951\n",
            "[TOP] exp_trunc[9] = 295029\n",
            "[TOP] exp_accum_input[9] = 18439\n",
            "[TOP] x_divider = 10613\n",
            "[TOP] y_dividend = 1152\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 36993\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8859712521\n",
            "[TOP] exp_trunc[9] = 540753\n",
            "[TOP] exp_accum_input[9] = 33797\n",
            "[TOP] x_divider = 18570\n",
            "[TOP] y_dividend = 2112\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 43948\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 3872082488\n",
            "[TOP] exp_trunc[9] = 236333\n",
            "[TOP] exp_accum_input[9] = 14770\n",
            "[TOP] x_divider = 7486\n",
            "[TOP] y_dividend = 923\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 24121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5776907137\n",
            "[TOP] exp_trunc[9] = 352594\n",
            "[TOP] exp_accum_input[9] = 22037\n",
            "[TOP] x_divider = 13245\n",
            "[TOP] y_dividend = 1377\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 7897\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 32030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7671088910\n",
            "[TOP] exp_trunc[9] = 468206\n",
            "[TOP] exp_accum_input[9] = 29262\n",
            "[TOP] x_divider = 12925\n",
            "[TOP] y_dividend = 1828\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24213\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5798940861\n",
            "[TOP] exp_trunc[9] = 353939\n",
            "[TOP] exp_accum_input[9] = 22121\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1382\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 10858\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 27273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6531801681\n",
            "[TOP] exp_trunc[9] = 398669\n",
            "[TOP] exp_accum_input[9] = 24916\n",
            "[TOP] x_divider = 12112\n",
            "[TOP] y_dividend = 1557\n",
            "[TOP] data_out[9] = 1053\n",
            "[TOP] exp_fraction_output[9] = 21581\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5168584757\n",
            "[TOP] exp_trunc[9] = 315465\n",
            "[TOP] exp_accum_input[9] = 19716\n",
            "[TOP] x_divider = 11560\n",
            "[TOP] y_dividend = 1232\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6104778530\n",
            "[TOP] exp_trunc[9] = 372606\n",
            "[TOP] exp_accum_input[9] = 23287\n",
            "[TOP] x_divider = 11984\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 25647\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6142379559\n",
            "[TOP] exp_trunc[9] = 374901\n",
            "[TOP] exp_accum_input[9] = 23431\n",
            "[TOP] x_divider = 15670\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 31798\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7615525606\n",
            "[TOP] exp_trunc[9] = 464814\n",
            "[TOP] exp_accum_input[9] = 29050\n",
            "[TOP] x_divider = 12708\n",
            "[TOP] y_dividend = 1815\n",
            "[TOP] data_out[9] = 1169\n",
            "[TOP] exp_fraction_output[9] = 25853\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6191715941\n",
            "[TOP] exp_trunc[9] = 377912\n",
            "[TOP] exp_accum_input[9] = 23619\n",
            "[TOP] x_divider = 13472\n",
            "[TOP] y_dividend = 1476\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 28599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6849374703\n",
            "[TOP] exp_trunc[9] = 418052\n",
            "[TOP] exp_accum_input[9] = 26128\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1633\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 18328\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4389501016\n",
            "[TOP] exp_trunc[9] = 267913\n",
            "[TOP] exp_accum_input[9] = 16744\n",
            "[TOP] x_divider = 11117\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 22925\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5490468725\n",
            "[TOP] exp_trunc[9] = 335111\n",
            "[TOP] exp_accum_input[9] = 20944\n",
            "[TOP] x_divider = 12102\n",
            "[TOP] y_dividend = 1309\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4411774237\n",
            "[TOP] exp_trunc[9] = 269273\n",
            "[TOP] exp_accum_input[9] = 16829\n",
            "[TOP] x_divider = 12442\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 28633\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6857517601\n",
            "[TOP] exp_trunc[9] = 418549\n",
            "[TOP] exp_accum_input[9] = 26159\n",
            "[TOP] x_divider = 15349\n",
            "[TOP] y_dividend = 1634\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 33114\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7930703658\n",
            "[TOP] exp_trunc[9] = 484051\n",
            "[TOP] exp_accum_input[9] = 30253\n",
            "[TOP] x_divider = 16206\n",
            "[TOP] y_dividend = 1890\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21768\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213370696\n",
            "[TOP] exp_trunc[9] = 318198\n",
            "[TOP] exp_accum_input[9] = 19887\n",
            "[TOP] x_divider = 15445\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 32959\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7893581623\n",
            "[TOP] exp_trunc[9] = 481785\n",
            "[TOP] exp_accum_input[9] = 30111\n",
            "[TOP] x_divider = 16141\n",
            "[TOP] y_dividend = 1881\n",
            "[TOP] data_out[9] = 955\n",
            "[TOP] exp_fraction_output[9] = 33686\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8067695942\n",
            "[TOP] exp_trunc[9] = 492413\n",
            "[TOP] exp_accum_input[9] = 30775\n",
            "[TOP] x_divider = 16475\n",
            "[TOP] y_dividend = 1923\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5081407849\n",
            "[TOP] exp_trunc[9] = 310144\n",
            "[TOP] exp_accum_input[9] = 19384\n",
            "[TOP] x_divider = 12340\n",
            "[TOP] y_dividend = 1211\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5497174641\n",
            "[TOP] exp_trunc[9] = 335520\n",
            "[TOP] exp_accum_input[9] = 20970\n",
            "[TOP] x_divider = 13633\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 24526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5873903422\n",
            "[TOP] exp_trunc[9] = 358514\n",
            "[TOP] exp_accum_input[9] = 22407\n",
            "[TOP] x_divider = 12220\n",
            "[TOP] y_dividend = 1400\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 20601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4933877697\n",
            "[TOP] exp_trunc[9] = 301139\n",
            "[TOP] exp_accum_input[9] = 18821\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 25485\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6103581045\n",
            "[TOP] exp_trunc[9] = 372533\n",
            "[TOP] exp_accum_input[9] = 23283\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 835\n",
            "[TOP] exp_fraction_output[9] = 20494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4908251518\n",
            "[TOP] exp_trunc[9] = 299575\n",
            "[TOP] exp_accum_input[9] = 18723\n",
            "[TOP] x_divider = 12187\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 25333\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6067177501\n",
            "[TOP] exp_trunc[9] = 370311\n",
            "[TOP] exp_accum_input[9] = 23144\n",
            "[TOP] x_divider = 15042\n",
            "[TOP] y_dividend = 1446\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6027420999\n",
            "[TOP] exp_trunc[9] = 367884\n",
            "[TOP] exp_accum_input[9] = 22992\n",
            "[TOP] x_divider = 14310\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 11863\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 23952\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2110314912\n",
            "[TOP] exp_trunc[9] = 128803\n",
            "[TOP] exp_accum_input[9] = 8050\n",
            "[TOP] x_divider = 5423\n",
            "[TOP] y_dividend = 503\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 21903\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5245702791\n",
            "[TOP] exp_trunc[9] = 320172\n",
            "[TOP] exp_accum_input[9] = 20010\n",
            "[TOP] x_divider = 11453\n",
            "[TOP] y_dividend = 1250\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20171\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4830893987\n",
            "[TOP] exp_trunc[9] = 294854\n",
            "[TOP] exp_accum_input[9] = 18428\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 23420\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5609019740\n",
            "[TOP] exp_trunc[9] = 342347\n",
            "[TOP] exp_accum_input[9] = 21396\n",
            "[TOP] x_divider = 13078\n",
            "[TOP] y_dividend = 1337\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 14252\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 567\n",
            "[TOP] exp_fraction_output[9] = 24856\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2189962736\n",
            "[TOP] exp_trunc[9] = 133664\n",
            "[TOP] exp_accum_input[9] = 8354\n",
            "[TOP] x_divider = 7201\n",
            "[TOP] y_dividend = 522\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 18889\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1664234234\n",
            "[TOP] exp_trunc[9] = 101576\n",
            "[TOP] exp_accum_input[9] = 6348\n",
            "[TOP] x_divider = 5850\n",
            "[TOP] y_dividend = 396\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 17772\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4256340684\n",
            "[TOP] exp_trunc[9] = 259786\n",
            "[TOP] exp_accum_input[9] = 16236\n",
            "[TOP] x_divider = 11906\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 631\n",
            "[TOP] exp_fraction_output[9] = 17468\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4183533596\n",
            "[TOP] exp_trunc[9] = 255342\n",
            "[TOP] exp_accum_input[9] = 15958\n",
            "[TOP] x_divider = 13071\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 13266\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 24616\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5895458152\n",
            "[TOP] exp_trunc[9] = 359830\n",
            "[TOP] exp_accum_input[9] = 22489\n",
            "[TOP] x_divider = 17693\n",
            "[TOP] y_dividend = 1405\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 20926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5011714222\n",
            "[TOP] exp_trunc[9] = 305890\n",
            "[TOP] exp_accum_input[9] = 19118\n",
            "[TOP] x_divider = 11780\n",
            "[TOP] y_dividend = 1194\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 18754\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4491526738\n",
            "[TOP] exp_trunc[9] = 274141\n",
            "[TOP] exp_accum_input[9] = 17133\n",
            "[TOP] x_divider = 12039\n",
            "[TOP] y_dividend = 1070\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17214\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122701358\n",
            "[TOP] exp_trunc[9] = 251629\n",
            "[TOP] exp_accum_input[9] = 15726\n",
            "[TOP] x_divider = 10739\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17226\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4125575322\n",
            "[TOP] exp_trunc[9] = 251805\n",
            "[TOP] exp_accum_input[9] = 15737\n",
            "[TOP] x_divider = 10355\n",
            "[TOP] y_dividend = 983\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 21770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213849690\n",
            "[TOP] exp_trunc[9] = 318228\n",
            "[TOP] exp_accum_input[9] = 19889\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 18831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4509968007\n",
            "[TOP] exp_trunc[9] = 275266\n",
            "[TOP] exp_accum_input[9] = 17204\n",
            "[TOP] x_divider = 12274\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 18953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4539186641\n",
            "[TOP] exp_trunc[9] = 277049\n",
            "[TOP] exp_accum_input[9] = 17315\n",
            "[TOP] x_divider = 10512\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17518\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195508446\n",
            "[TOP] exp_trunc[9] = 256073\n",
            "[TOP] exp_accum_input[9] = 16004\n",
            "[TOP] x_divider = 11380\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 19217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4602413849\n",
            "[TOP] exp_trunc[9] = 280909\n",
            "[TOP] exp_accum_input[9] = 17556\n",
            "[TOP] x_divider = 10585\n",
            "[TOP] y_dividend = 1097\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 20644\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4944176068\n",
            "[TOP] exp_trunc[9] = 301768\n",
            "[TOP] exp_accum_input[9] = 18860\n",
            "[TOP] x_divider = 10861\n",
            "[TOP] y_dividend = 1178\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19199\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4598102903\n",
            "[TOP] exp_trunc[9] = 280645\n",
            "[TOP] exp_accum_input[9] = 17540\n",
            "[TOP] x_divider = 10850\n",
            "[TOP] y_dividend = 1096\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 25224\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6041072328\n",
            "[TOP] exp_trunc[9] = 368717\n",
            "[TOP] exp_accum_input[9] = 23044\n",
            "[TOP] x_divider = 11252\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 1047\n",
            "[TOP] exp_fraction_output[9] = 23170\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5549145490\n",
            "[TOP] exp_trunc[9] = 338692\n",
            "[TOP] exp_accum_input[9] = 21168\n",
            "[TOP] x_divider = 11840\n",
            "[TOP] y_dividend = 1323\n",
            "[TOP] data_out[9] = 915\n",
            "[TOP] exp_fraction_output[9] = 21353\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5113979441\n",
            "[TOP] exp_trunc[9] = 312132\n",
            "[TOP] exp_accum_input[9] = 19508\n",
            "[TOP] x_divider = 11460\n",
            "[TOP] y_dividend = 1219\n",
            "[TOP] data_out[9] = 871\n",
            "[TOP] exp_fraction_output[9] = 21785\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5217442145\n",
            "[TOP] exp_trunc[9] = 318447\n",
            "[TOP] exp_accum_input[9] = 19902\n",
            "[TOP] x_divider = 10963\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 23107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5534057179\n",
            "[TOP] exp_trunc[9] = 337772\n",
            "[TOP] exp_accum_input[9] = 21110\n",
            "[TOP] x_divider = 11337\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 953\n",
            "[TOP] exp_fraction_output[9] = 22112\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5295757664\n",
            "[TOP] exp_trunc[9] = 323227\n",
            "[TOP] exp_accum_input[9] = 20201\n",
            "[TOP] x_divider = 12541\n",
            "[TOP] y_dividend = 1262\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 21790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5218639630\n",
            "[TOP] exp_trunc[9] = 318520\n",
            "[TOP] exp_accum_input[9] = 19907\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1244\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 25459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6097354123\n",
            "[TOP] exp_trunc[9] = 372152\n",
            "[TOP] exp_accum_input[9] = 23259\n",
            "[TOP] x_divider = 12078\n",
            "[TOP] y_dividend = 1453\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 28295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6776567615\n",
            "[TOP] exp_trunc[9] = 413608\n",
            "[TOP] exp_accum_input[9] = 25850\n",
            "[TOP] x_divider = 13416\n",
            "[TOP] y_dividend = 1615\n",
            "[TOP] data_out[9] = 987\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 27047\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6477675359\n",
            "[TOP] exp_trunc[9] = 395365\n",
            "[TOP] exp_accum_input[9] = 24710\n",
            "[TOP] x_divider = 16395\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 22813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5463645061\n",
            "[TOP] exp_trunc[9] = 333474\n",
            "[TOP] exp_accum_input[9] = 20842\n",
            "[TOP] x_divider = 13514\n",
            "[TOP] y_dividend = 1302\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6156509882\n",
            "[TOP] exp_trunc[9] = 375763\n",
            "[TOP] exp_accum_input[9] = 23485\n",
            "[TOP] x_divider = 13073\n",
            "[TOP] y_dividend = 1467\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 17850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275021450\n",
            "[TOP] exp_trunc[9] = 260926\n",
            "[TOP] exp_accum_input[9] = 16307\n",
            "[TOP] x_divider = 11368\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17443\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4177546171\n",
            "[TOP] exp_trunc[9] = 254977\n",
            "[TOP] exp_accum_input[9] = 15936\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10931\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 29143\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6979661071\n",
            "[TOP] exp_trunc[9] = 426004\n",
            "[TOP] exp_accum_input[9] = 26625\n",
            "[TOP] x_divider = 14588\n",
            "[TOP] y_dividend = 1664\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12761\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 629\n",
            "[TOP] exp_fraction_output[9] = 30236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7241431292\n",
            "[TOP] exp_trunc[9] = 441981\n",
            "[TOP] exp_accum_input[9] = 27623\n",
            "[TOP] x_divider = 15270\n",
            "[TOP] y_dividend = 1726\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 20490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907293530\n",
            "[TOP] exp_trunc[9] = 299517\n",
            "[TOP] exp_accum_input[9] = 18719\n",
            "[TOP] x_divider = 13310\n",
            "[TOP] y_dividend = 1169\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4500627624\n",
            "[TOP] exp_trunc[9] = 274696\n",
            "[TOP] exp_accum_input[9] = 17168\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1073\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 27416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6566049752\n",
            "[TOP] exp_trunc[9] = 400759\n",
            "[TOP] exp_accum_input[9] = 25047\n",
            "[TOP] x_divider = 11998\n",
            "[TOP] y_dividend = 1565\n",
            "[TOP] data_out[9] = 1067\n",
            "[TOP] exp_fraction_output[9] = 28860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6911883420\n",
            "[TOP] exp_trunc[9] = 421867\n",
            "[TOP] exp_accum_input[9] = 26366\n",
            "[TOP] x_divider = 13191\n",
            "[TOP] y_dividend = 1647\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 29779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7131981163\n",
            "[TOP] exp_trunc[9] = 435301\n",
            "[TOP] exp_accum_input[9] = 27206\n",
            "[TOP] x_divider = 13079\n",
            "[TOP] y_dividend = 1700\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 17366\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4159104902\n",
            "[TOP] exp_trunc[9] = 253851\n",
            "[TOP] exp_accum_input[9] = 15865\n",
            "[TOP] x_divider = 11259\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 26536\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6355292392\n",
            "[TOP] exp_trunc[9] = 387896\n",
            "[TOP] exp_accum_input[9] = 24243\n",
            "[TOP] x_divider = 11466\n",
            "[TOP] y_dividend = 1515\n",
            "[TOP] data_out[9] = 1081\n",
            "[TOP] exp_fraction_output[9] = 23193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5554653921\n",
            "[TOP] exp_trunc[9] = 339029\n",
            "[TOP] exp_accum_input[9] = 21189\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 1324\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23577\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5646620769\n",
            "[TOP] exp_trunc[9] = 344642\n",
            "[TOP] exp_accum_input[9] = 21540\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 1346\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 31609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7570260673\n",
            "[TOP] exp_trunc[9] = 462052\n",
            "[TOP] exp_accum_input[9] = 28878\n",
            "[TOP] x_divider = 13385\n",
            "[TOP] y_dividend = 1804\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 22186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5313480442\n",
            "[TOP] exp_trunc[9] = 324309\n",
            "[TOP] exp_accum_input[9] = 20269\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1266\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 17474\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4184970578\n",
            "[TOP] exp_trunc[9] = 255430\n",
            "[TOP] exp_accum_input[9] = 15964\n",
            "[TOP] x_divider = 10496\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4312622479\n",
            "[TOP] exp_trunc[9] = 263221\n",
            "[TOP] exp_accum_input[9] = 16451\n",
            "[TOP] x_divider = 11097\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11796\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 17161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4110008017\n",
            "[TOP] exp_trunc[9] = 250854\n",
            "[TOP] exp_accum_input[9] = 15678\n",
            "[TOP] x_divider = 12331\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11544\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18015\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4314538455\n",
            "[TOP] exp_trunc[9] = 263338\n",
            "[TOP] exp_accum_input[9] = 16458\n",
            "[TOP] x_divider = 10713\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17275\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4137310675\n",
            "[TOP] exp_trunc[9] = 252521\n",
            "[TOP] exp_accum_input[9] = 15782\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 986\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17552\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4203651344\n",
            "[TOP] exp_trunc[9] = 256570\n",
            "[TOP] exp_accum_input[9] = 16035\n",
            "[TOP] x_divider = 10350\n",
            "[TOP] y_dividend = 1002\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 22462\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5379581614\n",
            "[TOP] exp_trunc[9] = 328343\n",
            "[TOP] exp_accum_input[9] = 20521\n",
            "[TOP] x_divider = 11290\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 18009\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4313101473\n",
            "[TOP] exp_trunc[9] = 263250\n",
            "[TOP] exp_accum_input[9] = 16453\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18206\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4360282382\n",
            "[TOP] exp_trunc[9] = 266130\n",
            "[TOP] exp_accum_input[9] = 16633\n",
            "[TOP] x_divider = 11258\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4148567034\n",
            "[TOP] exp_trunc[9] = 253208\n",
            "[TOP] exp_accum_input[9] = 15825\n",
            "[TOP] x_divider = 10461\n",
            "[TOP] y_dividend = 989\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11719\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 683\n",
            "[TOP] exp_fraction_output[9] = 21032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5037100904\n",
            "[TOP] exp_trunc[9] = 307440\n",
            "[TOP] exp_accum_input[9] = 19215\n",
            "[TOP] x_divider = 11626\n",
            "[TOP] y_dividend = 1200\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 23243\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5566628771\n",
            "[TOP] exp_trunc[9] = 339760\n",
            "[TOP] exp_accum_input[9] = 21235\n",
            "[TOP] x_divider = 12278\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18821\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4507573037\n",
            "[TOP] exp_trunc[9] = 275120\n",
            "[TOP] exp_accum_input[9] = 17195\n",
            "[TOP] x_divider = 13368\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 22599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412392703\n",
            "[TOP] exp_trunc[9] = 330346\n",
            "[TOP] exp_accum_input[9] = 20646\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 26391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6320565327\n",
            "[TOP] exp_trunc[9] = 385776\n",
            "[TOP] exp_accum_input[9] = 24111\n",
            "[TOP] x_divider = 14123\n",
            "[TOP] y_dividend = 1506\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25188\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6032450436\n",
            "[TOP] exp_trunc[9] = 368191\n",
            "[TOP] exp_accum_input[9] = 23011\n",
            "[TOP] x_divider = 14432\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 21416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5129067752\n",
            "[TOP] exp_trunc[9] = 313053\n",
            "[TOP] exp_accum_input[9] = 19565\n",
            "[TOP] x_divider = 12398\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 29409\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7043367273\n",
            "[TOP] exp_trunc[9] = 429893\n",
            "[TOP] exp_accum_input[9] = 26868\n",
            "[TOP] x_divider = 14547\n",
            "[TOP] y_dividend = 1679\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25837\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2276394722\n",
            "[TOP] exp_trunc[9] = 138940\n",
            "[TOP] exp_accum_input[9] = 8683\n",
            "[TOP] x_divider = 6291\n",
            "[TOP] y_dividend = 542\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 24815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5943118055\n",
            "[TOP] exp_trunc[9] = 362739\n",
            "[TOP] exp_accum_input[9] = 22671\n",
            "[TOP] x_divider = 14134\n",
            "[TOP] y_dividend = 1416\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 23324\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586028028\n",
            "[TOP] exp_trunc[9] = 340944\n",
            "[TOP] exp_accum_input[9] = 21309\n",
            "[TOP] x_divider = 11095\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 11195\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 24045\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5758705365\n",
            "[TOP] exp_trunc[9] = 351483\n",
            "[TOP] exp_accum_input[9] = 21967\n",
            "[TOP] x_divider = 13000\n",
            "[TOP] y_dividend = 1372\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 29003\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6946131491\n",
            "[TOP] exp_trunc[9] = 423958\n",
            "[TOP] exp_accum_input[9] = 26497\n",
            "[TOP] x_divider = 15963\n",
            "[TOP] y_dividend = 1656\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 27073\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6483902281\n",
            "[TOP] exp_trunc[9] = 395745\n",
            "[TOP] exp_accum_input[9] = 24734\n",
            "[TOP] x_divider = 14354\n",
            "[TOP] y_dividend = 1545\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 27050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6478393850\n",
            "[TOP] exp_trunc[9] = 395409\n",
            "[TOP] exp_accum_input[9] = 24713\n",
            "[TOP] x_divider = 12938\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 977\n",
            "[TOP] exp_fraction_output[9] = 22122\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5298152634\n",
            "[TOP] exp_trunc[9] = 323373\n",
            "[TOP] exp_accum_input[9] = 20210\n",
            "[TOP] x_divider = 13012\n",
            "[TOP] y_dividend = 1263\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 21822\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5226303534\n",
            "[TOP] exp_trunc[9] = 318988\n",
            "[TOP] exp_accum_input[9] = 19936\n",
            "[TOP] x_divider = 12487\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12393\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17660\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229517020\n",
            "[TOP] exp_trunc[9] = 258149\n",
            "[TOP] exp_accum_input[9] = 16134\n",
            "[TOP] x_divider = 12694\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 23205\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5557527885\n",
            "[TOP] exp_trunc[9] = 339204\n",
            "[TOP] exp_accum_input[9] = 21200\n",
            "[TOP] x_divider = 12836\n",
            "[TOP] y_dividend = 1325\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 14111\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14478\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 921\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 18624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4460392128\n",
            "[TOP] exp_trunc[9] = 272240\n",
            "[TOP] exp_accum_input[9] = 17015\n",
            "[TOP] x_divider = 11611\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 11091\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18191\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356689927\n",
            "[TOP] exp_trunc[9] = 265911\n",
            "[TOP] exp_accum_input[9] = 16619\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 26258\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6288712226\n",
            "[TOP] exp_trunc[9] = 383832\n",
            "[TOP] exp_accum_input[9] = 23989\n",
            "[TOP] x_divider = 12630\n",
            "[TOP] y_dividend = 1499\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 22637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5421493589\n",
            "[TOP] exp_trunc[9] = 330901\n",
            "[TOP] exp_accum_input[9] = 20681\n",
            "[TOP] x_divider = 11913\n",
            "[TOP] y_dividend = 1292\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 11733\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 20618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4937949146\n",
            "[TOP] exp_trunc[9] = 301388\n",
            "[TOP] exp_accum_input[9] = 18836\n",
            "[TOP] x_divider = 12339\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 23709\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5678234373\n",
            "[TOP] exp_trunc[9] = 346571\n",
            "[TOP] exp_accum_input[9] = 21660\n",
            "[TOP] x_divider = 12533\n",
            "[TOP] y_dividend = 1353\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 18351\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4395009447\n",
            "[TOP] exp_trunc[9] = 268250\n",
            "[TOP] exp_accum_input[9] = 16765\n",
            "[TOP] x_divider = 11976\n",
            "[TOP] y_dividend = 1047\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 27286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6534915142\n",
            "[TOP] exp_trunc[9] = 398859\n",
            "[TOP] exp_accum_input[9] = 24928\n",
            "[TOP] x_divider = 12974\n",
            "[TOP] y_dividend = 1558\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 21316\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5105118052\n",
            "[TOP] exp_trunc[9] = 311591\n",
            "[TOP] exp_accum_input[9] = 19474\n",
            "[TOP] x_divider = 12248\n",
            "[TOP] y_dividend = 1217\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 23018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5512741946\n",
            "[TOP] exp_trunc[9] = 336471\n",
            "[TOP] exp_accum_input[9] = 21029\n",
            "[TOP] x_divider = 11794\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 23817\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5704100049\n",
            "[TOP] exp_trunc[9] = 348150\n",
            "[TOP] exp_accum_input[9] = 21759\n",
            "[TOP] x_divider = 11722\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 951\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 11659\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 22490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5386287530\n",
            "[TOP] exp_trunc[9] = 328752\n",
            "[TOP] exp_accum_input[9] = 20547\n",
            "[TOP] x_divider = 10790\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5719188360\n",
            "[TOP] exp_trunc[9] = 349071\n",
            "[TOP] exp_accum_input[9] = 21816\n",
            "[TOP] x_divider = 11254\n",
            "[TOP] y_dividend = 1363\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 19652\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4706595044\n",
            "[TOP] exp_trunc[9] = 287267\n",
            "[TOP] exp_accum_input[9] = 17954\n",
            "[TOP] x_divider = 11006\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 10769\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 17397\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4166529309\n",
            "[TOP] exp_trunc[9] = 254304\n",
            "[TOP] exp_accum_input[9] = 15894\n",
            "[TOP] x_divider = 11350\n",
            "[TOP] y_dividend = 993\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11572\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22901\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5484720797\n",
            "[TOP] exp_trunc[9] = 334760\n",
            "[TOP] exp_accum_input[9] = 20922\n",
            "[TOP] x_divider = 10916\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 23824\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5705776528\n",
            "[TOP] exp_trunc[9] = 348252\n",
            "[TOP] exp_accum_input[9] = 21765\n",
            "[TOP] x_divider = 11600\n",
            "[TOP] y_dividend = 1360\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4305916563\n",
            "[TOP] exp_trunc[9] = 262812\n",
            "[TOP] exp_accum_input[9] = 16425\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1026\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 22322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5346052034\n",
            "[TOP] exp_trunc[9] = 326297\n",
            "[TOP] exp_accum_input[9] = 20393\n",
            "[TOP] x_divider = 12478\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22922\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5489750234\n",
            "[TOP] exp_trunc[9] = 335067\n",
            "[TOP] exp_accum_input[9] = 20941\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1308\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10837\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10951\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 18715\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4482186355\n",
            "[TOP] exp_trunc[9] = 273570\n",
            "[TOP] exp_accum_input[9] = 17098\n",
            "[TOP] x_divider = 13642\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 10978\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 17427\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173714219\n",
            "[TOP] exp_trunc[9] = 254743\n",
            "[TOP] exp_accum_input[9] = 15921\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 995\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 20290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4859394130\n",
            "[TOP] exp_trunc[9] = 296593\n",
            "[TOP] exp_accum_input[9] = 18537\n",
            "[TOP] x_divider = 11075\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 28032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6713579904\n",
            "[TOP] exp_trunc[9] = 409764\n",
            "[TOP] exp_accum_input[9] = 25610\n",
            "[TOP] x_divider = 17852\n",
            "[TOP] y_dividend = 1600\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 18082\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4330584754\n",
            "[TOP] exp_trunc[9] = 264317\n",
            "[TOP] exp_accum_input[9] = 16519\n",
            "[TOP] x_divider = 11156\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13246\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 605\n",
            "[TOP] exp_fraction_output[9] = 36600\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8765590200\n",
            "[TOP] exp_trunc[9] = 535009\n",
            "[TOP] exp_accum_input[9] = 33438\n",
            "[TOP] x_divider = 19558\n",
            "[TOP] y_dividend = 2089\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11546\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 21696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5196126912\n",
            "[TOP] exp_trunc[9] = 317146\n",
            "[TOP] exp_accum_input[9] = 19821\n",
            "[TOP] x_divider = 10919\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 24254\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5808760238\n",
            "[TOP] exp_trunc[9] = 354538\n",
            "[TOP] exp_accum_input[9] = 22158\n",
            "[TOP] x_divider = 12597\n",
            "[TOP] y_dividend = 1384\n",
            "[TOP] data_out[9] = 899\n",
            "[TOP] exp_fraction_output[9] = 25193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6033647921\n",
            "[TOP] exp_trunc[9] = 368264\n",
            "[TOP] exp_accum_input[9] = 23016\n",
            "[TOP] x_divider = 13186\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 23121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5537410137\n",
            "[TOP] exp_trunc[9] = 337976\n",
            "[TOP] exp_accum_input[9] = 21123\n",
            "[TOP] x_divider = 13934\n",
            "[TOP] y_dividend = 1320\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 18167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4350941999\n",
            "[TOP] exp_trunc[9] = 265560\n",
            "[TOP] exp_accum_input[9] = 16597\n",
            "[TOP] x_divider = 11412\n",
            "[TOP] y_dividend = 1037\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 24070\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5764692790\n",
            "[TOP] exp_trunc[9] = 351848\n",
            "[TOP] exp_accum_input[9] = 21990\n",
            "[TOP] x_divider = 14412\n",
            "[TOP] y_dividend = 1374\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 17346\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4154314962\n",
            "[TOP] exp_trunc[9] = 253559\n",
            "[TOP] exp_accum_input[9] = 15847\n",
            "[TOP] x_divider = 10781\n",
            "[TOP] y_dividend = 990\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4696296673\n",
            "[TOP] exp_trunc[9] = 286639\n",
            "[TOP] exp_accum_input[9] = 17914\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4224487583\n",
            "[TOP] exp_trunc[9] = 257842\n",
            "[TOP] exp_accum_input[9] = 16115\n",
            "[TOP] x_divider = 11369\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 21242\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5087395274\n",
            "[TOP] exp_trunc[9] = 310509\n",
            "[TOP] exp_accum_input[9] = 19406\n",
            "[TOP] x_divider = 11535\n",
            "[TOP] y_dividend = 1212\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 18787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4499430139\n",
            "[TOP] exp_trunc[9] = 274623\n",
            "[TOP] exp_accum_input[9] = 17163\n",
            "[TOP] x_divider = 11035\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 17706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4240533882\n",
            "[TOP] exp_trunc[9] = 258821\n",
            "[TOP] exp_accum_input[9] = 16176\n",
            "[TOP] x_divider = 11067\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 24642\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5901685074\n",
            "[TOP] exp_trunc[9] = 360210\n",
            "[TOP] exp_accum_input[9] = 22513\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 30984\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7420575048\n",
            "[TOP] exp_trunc[9] = 452915\n",
            "[TOP] exp_accum_input[9] = 28307\n",
            "[TOP] x_divider = 12499\n",
            "[TOP] y_dividend = 1769\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24348\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5831272956\n",
            "[TOP] exp_trunc[9] = 355912\n",
            "[TOP] exp_accum_input[9] = 22244\n",
            "[TOP] x_divider = 12721\n",
            "[TOP] y_dividend = 1390\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 28716\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6877395852\n",
            "[TOP] exp_trunc[9] = 419762\n",
            "[TOP] exp_accum_input[9] = 26235\n",
            "[TOP] x_divider = 12710\n",
            "[TOP] y_dividend = 1639\n",
            "[TOP] data_out[9] = 1055\n",
            "[TOP] exp_fraction_output[9] = 25229\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6042269813\n",
            "[TOP] exp_trunc[9] = 368790\n",
            "[TOP] exp_accum_input[9] = 23049\n",
            "[TOP] x_divider = 12639\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 933\n",
            "[TOP] exp_fraction_output[9] = 23605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653326685\n",
            "[TOP] exp_trunc[9] = 345051\n",
            "[TOP] exp_accum_input[9] = 21565\n",
            "[TOP] x_divider = 10784\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 10622\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 27720\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6638856840\n",
            "[TOP] exp_trunc[9] = 405203\n",
            "[TOP] exp_accum_input[9] = 25325\n",
            "[TOP] x_divider = 13212\n",
            "[TOP] y_dividend = 1582\n",
            "[TOP] data_out[9] = 981\n",
            "[TOP] exp_fraction_output[9] = 22973\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5501964581\n",
            "[TOP] exp_trunc[9] = 335813\n",
            "[TOP] exp_accum_input[9] = 20988\n",
            "[TOP] x_divider = 11058\n",
            "[TOP] y_dividend = 1311\n",
            "[TOP] data_out[9] = 971\n",
            "[TOP] exp_fraction_output[9] = 22503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5389400991\n",
            "[TOP] exp_trunc[9] = 328942\n",
            "[TOP] exp_accum_input[9] = 20558\n",
            "[TOP] x_divider = 11890\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 26910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6444864270\n",
            "[TOP] exp_trunc[9] = 393363\n",
            "[TOP] exp_accum_input[9] = 24585\n",
            "[TOP] x_divider = 13004\n",
            "[TOP] y_dividend = 1536\n",
            "[TOP] data_out[9] = 967\n",
            "[TOP] exp_fraction_output[9] = 21042\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5039495874\n",
            "[TOP] exp_trunc[9] = 307586\n",
            "[TOP] exp_accum_input[9] = 19224\n",
            "[TOP] x_divider = 15418\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 639\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1551634766\n",
            "[TOP] exp_trunc[9] = 94704\n",
            "[TOP] exp_accum_input[9] = 5919\n",
            "[TOP] x_divider = 6579\n",
            "[TOP] y_dividend = 369\n",
            "[TOP] data_out[9] = 457\n",
            "[TOP] exp_fraction_output[9] = 19103\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4575111191\n",
            "[TOP] exp_trunc[9] = 279242\n",
            "[TOP] exp_accum_input[9] = 17452\n",
            "[TOP] x_divider = 14043\n",
            "[TOP] y_dividend = 1090\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12901\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 21849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5232769953\n",
            "[TOP] exp_trunc[9] = 319382\n",
            "[TOP] exp_accum_input[9] = 19961\n",
            "[TOP] x_divider = 12718\n",
            "[TOP] y_dividend = 1247\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 493\n",
            "[TOP] exp_fraction_output[9] = 17358\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4157188926\n",
            "[TOP] exp_trunc[9] = 253734\n",
            "[TOP] exp_accum_input[9] = 15858\n",
            "[TOP] x_divider = 14744\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 22095\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5291686215\n",
            "[TOP] exp_trunc[9] = 322978\n",
            "[TOP] exp_accum_input[9] = 20186\n",
            "[TOP] x_divider = 14721\n",
            "[TOP] y_dividend = 1261\n",
            "[TOP] data_out[9] = 703\n",
            "[TOP] exp_fraction_output[9] = 26061\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6241531317\n",
            "[TOP] exp_trunc[9] = 380952\n",
            "[TOP] exp_accum_input[9] = 23809\n",
            "[TOP] x_divider = 16576\n",
            "[TOP] y_dividend = 1488\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 26286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6295418142\n",
            "[TOP] exp_trunc[9] = 384241\n",
            "[TOP] exp_accum_input[9] = 24015\n",
            "[TOP] x_divider = 15505\n",
            "[TOP] y_dividend = 1500\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 18251\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4371059747\n",
            "[TOP] exp_trunc[9] = 266788\n",
            "[TOP] exp_accum_input[9] = 16674\n",
            "[TOP] x_divider = 11178\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17155\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4108571035\n",
            "[TOP] exp_trunc[9] = 250767\n",
            "[TOP] exp_accum_input[9] = 15672\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21783\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5216963151\n",
            "[TOP] exp_trunc[9] = 318418\n",
            "[TOP] exp_accum_input[9] = 19901\n",
            "[TOP] x_divider = 13081\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 24438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5852827686\n",
            "[TOP] exp_trunc[9] = 357228\n",
            "[TOP] exp_accum_input[9] = 22326\n",
            "[TOP] x_divider = 12876\n",
            "[TOP] y_dividend = 1395\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 22203\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5317551891\n",
            "[TOP] exp_trunc[9] = 324557\n",
            "[TOP] exp_accum_input[9] = 20284\n",
            "[TOP] x_divider = 11374\n",
            "[TOP] y_dividend = 1267\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20522\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914957434\n",
            "[TOP] exp_trunc[9] = 299985\n",
            "[TOP] exp_accum_input[9] = 18749\n",
            "[TOP] x_divider = 12385\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 24902\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963954294\n",
            "[TOP] exp_trunc[9] = 364010\n",
            "[TOP] exp_accum_input[9] = 22750\n",
            "[TOP] x_divider = 12596\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 925\n",
            "[TOP] exp_fraction_output[9] = 21406\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5126672782\n",
            "[TOP] exp_trunc[9] = 312907\n",
            "[TOP] exp_accum_input[9] = 19556\n",
            "[TOP] x_divider = 11043\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 19576\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4688393272\n",
            "[TOP] exp_trunc[9] = 286156\n",
            "[TOP] exp_accum_input[9] = 17884\n",
            "[TOP] x_divider = 11828\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 35963\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8613030611\n",
            "[TOP] exp_trunc[9] = 525697\n",
            "[TOP] exp_accum_input[9] = 32856\n",
            "[TOP] x_divider = 15390\n",
            "[TOP] y_dividend = 2053\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 32277\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7730244669\n",
            "[TOP] exp_trunc[9] = 471816\n",
            "[TOP] exp_accum_input[9] = 29488\n",
            "[TOP] x_divider = 12203\n",
            "[TOP] y_dividend = 1843\n",
            "[TOP] data_out[9] = 1237\n",
            "[TOP] exp_fraction_output[9] = 23105\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5533578185\n",
            "[TOP] exp_trunc[9] = 337742\n",
            "[TOP] exp_accum_input[9] = 21108\n",
            "[TOP] x_divider = 10676\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 1015\n",
            "[TOP] exp_fraction_output[9] = 30295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7255561615\n",
            "[TOP] exp_trunc[9] = 442844\n",
            "[TOP] exp_accum_input[9] = 27677\n",
            "[TOP] x_divider = 11700\n",
            "[TOP] y_dividend = 1729\n",
            "[TOP] data_out[9] = 1211\n",
            "[TOP] exp_fraction_output[9] = 22467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5380779099\n",
            "[TOP] exp_trunc[9] = 328416\n",
            "[TOP] exp_accum_input[9] = 20526\n",
            "[TOP] x_divider = 11189\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 26878\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6437200366\n",
            "[TOP] exp_trunc[9] = 392895\n",
            "[TOP] exp_accum_input[9] = 24555\n",
            "[TOP] x_divider = 10882\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 1153\n",
            "[TOP] exp_fraction_output[9] = 22050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5280908850\n",
            "[TOP] exp_trunc[9] = 322321\n",
            "[TOP] exp_accum_input[9] = 20145\n",
            "[TOP] x_divider = 11313\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 30634\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7336751098\n",
            "[TOP] exp_trunc[9] = 447799\n",
            "[TOP] exp_accum_input[9] = 27987\n",
            "[TOP] x_divider = 12590\n",
            "[TOP] y_dividend = 1749\n",
            "[TOP] data_out[9] = 1137\n",
            "[TOP] exp_fraction_output[9] = 35180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8425504460\n",
            "[TOP] exp_trunc[9] = 514251\n",
            "[TOP] exp_accum_input[9] = 32140\n",
            "[TOP] x_divider = 12922\n",
            "[TOP] y_dividend = 2008\n",
            "[TOP] data_out[9] = 1273\n",
            "[TOP] exp_fraction_output[9] = 17157\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4109050029\n",
            "[TOP] exp_trunc[9] = 250796\n",
            "[TOP] exp_accum_input[9] = 15674\n",
            "[TOP] x_divider = 10220\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19405\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4647439285\n",
            "[TOP] exp_trunc[9] = 283657\n",
            "[TOP] exp_accum_input[9] = 17728\n",
            "[TOP] x_divider = 11390\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10802\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 17255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4132520735\n",
            "[TOP] exp_trunc[9] = 252229\n",
            "[TOP] exp_accum_input[9] = 15764\n",
            "[TOP] x_divider = 10370\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10791\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 18706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4480030882\n",
            "[TOP] exp_trunc[9] = 273439\n",
            "[TOP] exp_accum_input[9] = 17089\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 22063\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5284022311\n",
            "[TOP] exp_trunc[9] = 322511\n",
            "[TOP] exp_accum_input[9] = 20156\n",
            "[TOP] x_divider = 11646\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 20899\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5005247803\n",
            "[TOP] exp_trunc[9] = 305496\n",
            "[TOP] exp_accum_input[9] = 19093\n",
            "[TOP] x_divider = 11405\n",
            "[TOP] y_dividend = 1193\n",
            "[TOP] data_out[9] = 857\n",
            "[TOP] exp_fraction_output[9] = 18669\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4471169493\n",
            "[TOP] exp_trunc[9] = 272898\n",
            "[TOP] exp_accum_input[9] = 17056\n",
            "[TOP] x_divider = 10642\n",
            "[TOP] y_dividend = 1066\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10875\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17654\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4228080038\n",
            "[TOP] exp_trunc[9] = 258061\n",
            "[TOP] exp_accum_input[9] = 16128\n",
            "[TOP] x_divider = 10935\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 21467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5141282099\n",
            "[TOP] exp_trunc[9] = 313798\n",
            "[TOP] exp_accum_input[9] = 19612\n",
            "[TOP] x_divider = 12696\n",
            "[TOP] y_dividend = 1225\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19676\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4712342972\n",
            "[TOP] exp_trunc[9] = 287618\n",
            "[TOP] exp_accum_input[9] = 17976\n",
            "[TOP] x_divider = 10868\n",
            "[TOP] y_dividend = 1123\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 19307\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4623968579\n",
            "[TOP] exp_trunc[9] = 282224\n",
            "[TOP] exp_accum_input[9] = 17639\n",
            "[TOP] x_divider = 12526\n",
            "[TOP] y_dividend = 1102\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 18101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4335135197\n",
            "[TOP] exp_trunc[9] = 264595\n",
            "[TOP] exp_accum_input[9] = 16537\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 613\n",
            "[TOP] exp_fraction_output[9] = 26035\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6235304395\n",
            "[TOP] exp_trunc[9] = 380572\n",
            "[TOP] exp_accum_input[9] = 23785\n",
            "[TOP] x_divider = 15724\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25004\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5988382988\n",
            "[TOP] exp_trunc[9] = 365501\n",
            "[TOP] exp_accum_input[9] = 22843\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1427\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 26321\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6303800537\n",
            "[TOP] exp_trunc[9] = 384753\n",
            "[TOP] exp_accum_input[9] = 24047\n",
            "[TOP] x_divider = 14992\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 20624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4939386128\n",
            "[TOP] exp_trunc[9] = 301476\n",
            "[TOP] exp_accum_input[9] = 18842\n",
            "[TOP] x_divider = 11761\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20166\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4829696502\n",
            "[TOP] exp_trunc[9] = 294781\n",
            "[TOP] exp_accum_input[9] = 18423\n",
            "[TOP] x_divider = 10880\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 23156\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5545792532\n",
            "[TOP] exp_trunc[9] = 338488\n",
            "[TOP] exp_accum_input[9] = 21155\n",
            "[TOP] x_divider = 11724\n",
            "[TOP] y_dividend = 1322\n",
            "[TOP] data_out[9] = 923\n",
            "[TOP] exp_fraction_output[9] = 23385\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5600637345\n",
            "[TOP] exp_trunc[9] = 341835\n",
            "[TOP] exp_accum_input[9] = 21364\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1335\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 11092\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 17708\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4241012876\n",
            "[TOP] exp_trunc[9] = 258850\n",
            "[TOP] exp_accum_input[9] = 16178\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11296\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 18255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4372017735\n",
            "[TOP] exp_trunc[9] = 266846\n",
            "[TOP] exp_accum_input[9] = 16677\n",
            "[TOP] x_divider = 10387\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 19926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4772217222\n",
            "[TOP] exp_trunc[9] = 291273\n",
            "[TOP] exp_accum_input[9] = 18204\n",
            "[TOP] x_divider = 11048\n",
            "[TOP] y_dividend = 1137\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17804\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4264004588\n",
            "[TOP] exp_trunc[9] = 260254\n",
            "[TOP] exp_accum_input[9] = 16265\n",
            "[TOP] x_divider = 12129\n",
            "[TOP] y_dividend = 1016\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 19811\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4744675067\n",
            "[TOP] exp_trunc[9] = 289591\n",
            "[TOP] exp_accum_input[9] = 18099\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1131\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 10601\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 22496\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5387724512\n",
            "[TOP] exp_trunc[9] = 328840\n",
            "[TOP] exp_accum_input[9] = 20552\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 18107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4336572179\n",
            "[TOP] exp_trunc[9] = 264683\n",
            "[TOP] exp_accum_input[9] = 16542\n",
            "[TOP] x_divider = 12238\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10968\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 24557\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5881327829\n",
            "[TOP] exp_trunc[9] = 358967\n",
            "[TOP] exp_accum_input[9] = 22435\n",
            "[TOP] x_divider = 12595\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 911\n",
            "[TOP] exp_fraction_output[9] = 18136\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4343517592\n",
            "[TOP] exp_trunc[9] = 265107\n",
            "[TOP] exp_accum_input[9] = 16569\n",
            "[TOP] x_divider = 11755\n",
            "[TOP] y_dividend = 1035\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 17764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254424708\n",
            "[TOP] exp_trunc[9] = 259669\n",
            "[TOP] exp_accum_input[9] = 16229\n",
            "[TOP] x_divider = 11186\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 18154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4347828538\n",
            "[TOP] exp_trunc[9] = 265370\n",
            "[TOP] exp_accum_input[9] = 16585\n",
            "[TOP] x_divider = 10758\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10911\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18378\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4401475866\n",
            "[TOP] exp_trunc[9] = 268644\n",
            "[TOP] exp_accum_input[9] = 16790\n",
            "[TOP] x_divider = 10707\n",
            "[TOP] y_dividend = 1049\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 23493\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626503021\n",
            "[TOP] exp_trunc[9] = 343414\n",
            "[TOP] exp_accum_input[9] = 21463\n",
            "[TOP] x_divider = 15737\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 18876\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4520745372\n",
            "[TOP] exp_trunc[9] = 275924\n",
            "[TOP] exp_accum_input[9] = 17245\n",
            "[TOP] x_divider = 13422\n",
            "[TOP] y_dividend = 1077\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 15293\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 611\n",
            "[TOP] exp_fraction_output[9] = 21812\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5223908564\n",
            "[TOP] exp_trunc[9] = 318842\n",
            "[TOP] exp_accum_input[9] = 19927\n",
            "[TOP] x_divider = 14613\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17478\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185928566\n",
            "[TOP] exp_trunc[9] = 255488\n",
            "[TOP] exp_accum_input[9] = 15968\n",
            "[TOP] x_divider = 12454\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 17486\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4187844542\n",
            "[TOP] exp_trunc[9] = 255605\n",
            "[TOP] exp_accum_input[9] = 15975\n",
            "[TOP] x_divider = 12297\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 20813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4984651061\n",
            "[TOP] exp_trunc[9] = 304238\n",
            "[TOP] exp_accum_input[9] = 19014\n",
            "[TOP] x_divider = 12933\n",
            "[TOP] y_dividend = 1188\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4993272953\n",
            "[TOP] exp_trunc[9] = 304765\n",
            "[TOP] exp_accum_input[9] = 19047\n",
            "[TOP] x_divider = 12914\n",
            "[TOP] y_dividend = 1190\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 17167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4111444999\n",
            "[TOP] exp_trunc[9] = 250942\n",
            "[TOP] exp_accum_input[9] = 15683\n",
            "[TOP] x_divider = 11978\n",
            "[TOP] y_dividend = 980\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 19967\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4782036599\n",
            "[TOP] exp_trunc[9] = 291872\n",
            "[TOP] exp_accum_input[9] = 18242\n",
            "[TOP] x_divider = 12874\n",
            "[TOP] y_dividend = 1140\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 18599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4454404703\n",
            "[TOP] exp_trunc[9] = 271875\n",
            "[TOP] exp_accum_input[9] = 16992\n",
            "[TOP] x_divider = 13205\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20372\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4879032884\n",
            "[TOP] exp_trunc[9] = 297792\n",
            "[TOP] exp_accum_input[9] = 18612\n",
            "[TOP] x_divider = 10928\n",
            "[TOP] y_dividend = 1163\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 24270\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5812592190\n",
            "[TOP] exp_trunc[9] = 354772\n",
            "[TOP] exp_accum_input[9] = 22173\n",
            "[TOP] x_divider = 11951\n",
            "[TOP] y_dividend = 1385\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 18605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455841685\n",
            "[TOP] exp_trunc[9] = 271962\n",
            "[TOP] exp_accum_input[9] = 16997\n",
            "[TOP] x_divider = 11656\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20502\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4910167494\n",
            "[TOP] exp_trunc[9] = 299692\n",
            "[TOP] exp_accum_input[9] = 18730\n",
            "[TOP] x_divider = 11903\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10881\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4965491301\n",
            "[TOP] exp_trunc[9] = 303069\n",
            "[TOP] exp_accum_input[9] = 18941\n",
            "[TOP] x_divider = 11822\n",
            "[TOP] y_dividend = 1183\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 17456\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4180659632\n",
            "[TOP] exp_trunc[9] = 255167\n",
            "[TOP] exp_accum_input[9] = 15947\n",
            "[TOP] x_divider = 11571\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 18357\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396446429\n",
            "[TOP] exp_trunc[9] = 268337\n",
            "[TOP] exp_accum_input[9] = 16771\n",
            "[TOP] x_divider = 10651\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4230954002\n",
            "[TOP] exp_trunc[9] = 258236\n",
            "[TOP] exp_accum_input[9] = 16139\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 12828\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4680489871\n",
            "[TOP] exp_trunc[9] = 285674\n",
            "[TOP] exp_accum_input[9] = 17854\n",
            "[TOP] x_divider = 11206\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 18359\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396925423\n",
            "[TOP] exp_trunc[9] = 268367\n",
            "[TOP] exp_accum_input[9] = 16772\n",
            "[TOP] x_divider = 10786\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14967\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 24567\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5883722799\n",
            "[TOP] exp_trunc[9] = 359113\n",
            "[TOP] exp_accum_input[9] = 22444\n",
            "[TOP] x_divider = 15260\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17301\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4143537597\n",
            "[TOP] exp_trunc[9] = 252901\n",
            "[TOP] exp_accum_input[9] = 15806\n",
            "[TOP] x_divider = 12146\n",
            "[TOP] y_dividend = 987\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 12600\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6208720228\n",
            "[TOP] exp_trunc[9] = 378950\n",
            "[TOP] exp_accum_input[9] = 23684\n",
            "[TOP] x_divider = 16238\n",
            "[TOP] y_dividend = 1480\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 19062\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4565291814\n",
            "[TOP] exp_trunc[9] = 278643\n",
            "[TOP] exp_accum_input[9] = 17415\n",
            "[TOP] x_divider = 11347\n",
            "[TOP] y_dividend = 1088\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 20415\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4889331255\n",
            "[TOP] exp_trunc[9] = 298421\n",
            "[TOP] exp_accum_input[9] = 18651\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1165\n",
            "[TOP] data_out[9] = 861\n",
            "[TOP] exp_fraction_output[9] = 18636\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4463266092\n",
            "[TOP] exp_trunc[9] = 272416\n",
            "[TOP] exp_accum_input[9] = 17026\n",
            "[TOP] x_divider = 12609\n",
            "[TOP] y_dividend = 1064\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11264\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 19370\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4639056890\n",
            "[TOP] exp_trunc[9] = 283145\n",
            "[TOP] exp_accum_input[9] = 17696\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1106\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4315256946\n",
            "[TOP] exp_trunc[9] = 263382\n",
            "[TOP] exp_accum_input[9] = 16461\n",
            "[TOP] x_divider = 11415\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12660\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 19701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4718330397\n",
            "[TOP] exp_trunc[9] = 287984\n",
            "[TOP] exp_accum_input[9] = 17999\n",
            "[TOP] x_divider = 10867\n",
            "[TOP] y_dividend = 1124\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 21765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5212652205\n",
            "[TOP] exp_trunc[9] = 318155\n",
            "[TOP] exp_accum_input[9] = 19884\n",
            "[TOP] x_divider = 13401\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 22248\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5328329256\n",
            "[TOP] exp_trunc[9] = 325215\n",
            "[TOP] exp_accum_input[9] = 20325\n",
            "[TOP] x_divider = 15274\n",
            "[TOP] y_dividend = 1270\n",
            "[TOP] data_out[9] = 681\n",
            "[TOP] exp_fraction_output[9] = 20653\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4946331541\n",
            "[TOP] exp_trunc[9] = 301900\n",
            "[TOP] exp_accum_input[9] = 18868\n",
            "[TOP] x_divider = 14548\n",
            "[TOP] y_dividend = 1179\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 17236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4127970292\n",
            "[TOP] exp_trunc[9] = 251951\n",
            "[TOP] exp_accum_input[9] = 15746\n",
            "[TOP] x_divider = 12702\n",
            "[TOP] y_dividend = 984\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 20799\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4981298103\n",
            "[TOP] exp_trunc[9] = 304034\n",
            "[TOP] exp_accum_input[9] = 19002\n",
            "[TOP] x_divider = 13789\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 11303\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22068\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5285219796\n",
            "[TOP] exp_trunc[9] = 322584\n",
            "[TOP] exp_accum_input[9] = 20161\n",
            "[TOP] x_divider = 14163\n",
            "[TOP] y_dividend = 1260\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 19646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4705158062\n",
            "[TOP] exp_trunc[9] = 287180\n",
            "[TOP] exp_accum_input[9] = 17948\n",
            "[TOP] x_divider = 14401\n",
            "[TOP] y_dividend = 1121\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 24771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5932580187\n",
            "[TOP] exp_trunc[9] = 362095\n",
            "[TOP] exp_accum_input[9] = 22630\n",
            "[TOP] x_divider = 18996\n",
            "[TOP] y_dividend = 1414\n",
            "[TOP] data_out[9] = 609\n",
            "[TOP] exp_fraction_output[9] = 17149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4107134053\n",
            "[TOP] exp_trunc[9] = 250679\n",
            "[TOP] exp_accum_input[9] = 15667\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 29382\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7036900854\n",
            "[TOP] exp_trunc[9] = 429498\n",
            "[TOP] exp_accum_input[9] = 26843\n",
            "[TOP] x_divider = 20285\n",
            "[TOP] y_dividend = 1677\n",
            "[TOP] data_out[9] = 677\n",
            "[TOP] exp_fraction_output[9] = 20694\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4956150918\n",
            "[TOP] exp_trunc[9] = 302499\n",
            "[TOP] exp_accum_input[9] = 18906\n",
            "[TOP] x_divider = 14997\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 643\n",
            "[TOP] exp_fraction_output[9] = 19662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4708990014\n",
            "[TOP] exp_trunc[9] = 287413\n",
            "[TOP] exp_accum_input[9] = 17963\n",
            "[TOP] x_divider = 15698\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 585\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12362\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 14012\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13550\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 591\n",
            "[TOP] exp_fraction_output[9] = 21167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5069432999\n",
            "[TOP] exp_trunc[9] = 309413\n",
            "[TOP] exp_accum_input[9] = 19338\n",
            "[TOP] x_divider = 14782\n",
            "[TOP] y_dividend = 1208\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4259933139\n",
            "[TOP] exp_trunc[9] = 260005\n",
            "[TOP] exp_accum_input[9] = 16250\n",
            "[TOP] x_divider = 14554\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 20968\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5021773096\n",
            "[TOP] exp_trunc[9] = 306504\n",
            "[TOP] exp_accum_input[9] = 19156\n",
            "[TOP] x_divider = 14328\n",
            "[TOP] y_dividend = 1197\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20668\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949923996\n",
            "[TOP] exp_trunc[9] = 302119\n",
            "[TOP] exp_accum_input[9] = 18882\n",
            "[TOP] x_divider = 16874\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 573\n",
            "[TOP] exp_fraction_output[9] = 18603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455362691\n",
            "[TOP] exp_trunc[9] = 271933\n",
            "[TOP] exp_accum_input[9] = 16995\n",
            "[TOP] x_divider = 13297\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 18579\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4449614763\n",
            "[TOP] exp_trunc[9] = 271582\n",
            "[TOP] exp_accum_input[9] = 16973\n",
            "[TOP] x_divider = 10571\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 10815\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12919\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 853\n",
            "[TOP] exp_fraction_output[9] = 18064\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4326273808\n",
            "[TOP] exp_trunc[9] = 264054\n",
            "[TOP] exp_accum_input[9] = 16503\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1031\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 12583\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19412\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4649115764\n",
            "[TOP] exp_trunc[9] = 283759\n",
            "[TOP] exp_accum_input[9] = 17734\n",
            "[TOP] x_divider = 10685\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 22334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5348925998\n",
            "[TOP] exp_trunc[9] = 326472\n",
            "[TOP] exp_accum_input[9] = 20404\n",
            "[TOP] x_divider = 10855\n",
            "[TOP] y_dividend = 1275\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5866239518\n",
            "[TOP] exp_trunc[9] = 358046\n",
            "[TOP] exp_accum_input[9] = 22377\n",
            "[TOP] x_divider = 11899\n",
            "[TOP] y_dividend = 1398\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4221374122\n",
            "[TOP] exp_trunc[9] = 257652\n",
            "[TOP] exp_accum_input[9] = 16103\n",
            "[TOP] x_divider = 11279\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 729\n",
            "입력 문장: This is a grammatically acceptable for sentence.\n",
            "예측된 클래스: 0\n",
            "예측 확률: [[0.56147647 0.43852356]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 encoder 내 각 레이어의 self-attention 모듈 타입을 출력하여\n",
        "# 수정된 BertSelfAttentionModified가 적용되었는지 확인합니다.\n",
        "print(\"수정된 Attention Layers 확인:\")\n",
        "for i, layer in enumerate(model.bert.encoder.layer):\n",
        "    attn_layer = layer.attention.self\n",
        "    print(f\"Layer {i} self-attention layer type: {type(attn_layer)}\")\n"
      ],
      "metadata": {
        "id": "8WUtGKLuoW2z",
        "outputId": "5fbb725e-8939-4578-ea68-dc042ec45fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수정된 Attention Layers 확인:\n",
            "Layer 0 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 1 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 2 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 3 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 4 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 5 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 6 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 7 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 8 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 9 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 10 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 11 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 디버깅용 잡동사니코드"
      ],
      "metadata": {
        "id": "H5iP5hhyJuf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\"[TOP] exp_fraction_output[{i}] = {exp_fraction_output[i]}\")\n",
        "    print(f\"[TOP] exp_int_output[{i}] = {exp_int_output[i]}\")\n",
        "    print(f\"[TOP] exp_whole[{i}] = {exp_whole[i]}\")\n",
        "    print(f\"[TOP] exp_trunc[{i}] = {exp_trunc[i]}\")\n",
        "    print(f\"[TOP] exp_accum_input[{i}] = {exp_accum_input[i]}\")\n",
        "    print(f\"[TOP] x_divider = {x_divider[0]}\")\n",
        "    print(f\"[TOP] y_dividend = {y_dividend[i]}\")\n",
        "    print(f\"[TOP] data_out[{i}] = {data_out[i]}\")"
      ],
      "metadata": {
        "id": "rAamcoXWSjnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # ✅ 디버깅용 Q, K 체크\n",
        "        with torch.no_grad():\n",
        "            h = hidden_states.detach().cpu().numpy()\n",
        "            print(\"=== Hidden States 디버깅 ===\")\n",
        "            print(\"min:\", np.min(h), \"max:\", np.max(h))\n",
        "            print(\"NaN 수:\", np.isnan(h).sum(), \"Inf 수:\", np.isinf(h).sum())\n",
        "            q = query_layer.detach().cpu().numpy()\n",
        "            k = key_layer.detach().cpu().numpy()\n",
        "            print(\"=== Q, K 디버깅 ===\")\n",
        "            print(\"Q min/max:\", np.min(q), np.max(q))\n",
        "            print(\"K min/max:\", np.min(k), np.max(k))\n",
        "            print(\"Q NaN 수:\", np.isnan(q).sum(), \"K NaN 수:\", np.isnan(k).sum())\n",
        "            print(\"Q Inf 수:\", np.isinf(q).sum(), \"K Inf 수:\", np.isinf(k).sum())\n",
        "\n",
        "        # attention score 계산 후 clamp\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 계산 직후 ===\")\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item(), \"Inf 수:\", torch.isinf(attention_scores).sum().item())\n",
        "        attention_scores = torch.clamp(attention_scores, min=-10.0, max=10.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 예시 ===\")\n",
        "            print(attention_scores[0, 0, 0, :10])\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item())\n",
        "        # CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "\n",
        "# 2. BertLayerWithNaNCheck: layer 0 내부 모듈 NaN 추적\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n",
        "\n",
        "\n",
        "# 3. BertEncoderModified\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "        self.layer[0] = BertLayerWithNaNCheck(self.layer[0])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_values=None, use_cache=False, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=True):\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            with torch.no_grad():\n",
        "                hs = hidden_states.detach().cpu().numpy()\n",
        "                if np.isnan(hs).sum() > 0:\n",
        "                    print(f\"[NaN DETECTED] ❌ in hidden_states BEFORE layer {i}\")\n",
        "                else:\n",
        "                    print(f\"[OK] ✅ hidden_states BEFORE layer {i}\")\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                head_mask[i] if head_mask is not None else None,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_values[i] if past_key_values is not None else None,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        return (hidden_states,)\n",
        "\n",
        "\n",
        "# 4. BertWithModifiedAttentionForClassification\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 5. Load config and model\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "dTD-BGlHKAh5",
        "outputId": "383af56c-e173-4f09-db40-610c833f616d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayerWithNaNCheck(\n",
              "          (layer): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttentionModified(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-11): 11 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def predict_sentence(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_class = np.argmax(probs)\n",
        "\n",
        "    return pred_class, probs\n"
      ],
      "metadata": {
        "id": "sgc6czs9DqMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_nan(module, input, output):\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        if torch.isnan(output).any():\n",
        "            print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "    elif isinstance(output, (tuple, list)):\n",
        "        for o in output:\n",
        "            if torch.is_tensor(o) and torch.isnan(o).any():\n",
        "                print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    module.register_forward_hook(check_for_nan)\n"
      ],
      "metadata": {
        "id": "2QHFLakIFV8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        # Attention\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        # Intermediate\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        # Output\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n"
      ],
      "metadata": {
        "id": "gAA7SyorIaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentence = \"The cat is sitting on the mat.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "\n",
        "print(\"문장:\", sentence)\n",
        "print(\"예측 클래스:\", pred_class)\n",
        "print(\"클래스별 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "yv0VeRUlDpLp",
        "outputId": "1cd74ea5-ca18-4524-d57c-bed12d1b5f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] ✅ hidden_states BEFORE layer 0\n",
            "=== [Layer 0] BEFORE ===\n",
            "min: -4.571415901184082 max: 3.8990025520324707\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Hidden States 디버깅 ===\n",
            "min: -4.571416 max: 3.8990026\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: -2.1867194 2.275151\n",
            "K min/max: -2.4105966 2.2897592\n",
            "Q NaN 수: 0 K NaN 수: 0\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([ 0.4173, -0.1602, -0.0771,  0.3595,  0.1112, -0.1229, -0.2256, -0.0283,\n",
            "        -0.1147, -0.0984], device='cuda:0')\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0\n",
            "[NaN DETECTED] ❌ after Attention in Layer 0\n",
            "[NaN DETECTED] ❌ after Intermediate in Layer 0\n",
            "[NaN DETECTED] ❌ after Output in Layer 0\n",
            "=== [Layer 0] AFTER ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "[NaN DETECTED] ❌ in hidden_states BEFORE layer 1\n",
            "=== Hidden States 디버깅 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: nan nan\n",
            "K min/max: nan nan\n",
            "Q NaN 수: 49152 K NaN 수: 49152\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
            "min: nan max: nan\n",
            "NaN 수: 49152\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "float_to_fixed_point() - NaN 또는 무한대 값 감지: nan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-8b7cffed0231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The cat is sitting on the mat.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"문장:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-c78350183b4c>\u001b[0m in \u001b[0;36mpredict_sentence\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[OK] ✅ hidden_states BEFORE layer {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN 수:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcordic_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_1200_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop_1200_input\u001b[0;34m(attention_scores)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mresult_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#20bit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdata_22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# ✅ NaN 또는 Inf 값 체크 후 예외 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# ✅ **최대/최소 값 제한 (7비트 표현 범위)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: float_to_fixed_point() - NaN 또는 무한대 값 감지: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        batch_size, num_heads, seq_len, _ = attention_scores.shape\n",
        "        for b in range(batch_size):\n",
        "          for h in range(num_heads):\n",
        "            for row in range(seq_len):\n",
        "              float_row = attention_scores[b, h, row, :10].detach().cpu().numpy().tolist()\n",
        "\n",
        "              if any(np.isnan(f) for f in float_row):\n",
        "                print(f\"[NaN] b={b}, head={h}, row={row} - 입력에 NaN 있음! ❌\")\n",
        "                continue  # 이건 skip하고 다음으로\n",
        "              try:\n",
        "                cordic_attention = top(*float_row)\n",
        "              except Exception as e:\n",
        "                print(f\"[ERROR] top() 실패: {e}\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          sample_row = attention_scores[0, 0, 0, :10]  # [10개 float]\n",
        "          float_row = sample_row.detach().cpu().numpy().tolist()\n",
        "          print(\"[DEBUG] top 입력값:\", float_row)\n",
        "\n",
        "          try:\n",
        "            top_result = top(*float_row)\n",
        "            print(\"[DEBUG] top 출력값:\", top_result)\n",
        "          except Exception as e:\n",
        "            print(\"[ERROR] top에서 예외 발생:\", e)\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top(*float_row)\n",
        "\n",
        "        if any(np.isnan(c) for c in cordic_attention):\n",
        "          print(f\"[NaN DETECTED] ❌ top() 결과에 NaN 존재! input: {float_row}\")\n",
        "\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # ⚠️ 먼저 너의 원본 문장 리스트가 있어야 해!\n",
        "# 예: dataset_sentences = [\"문장1\", \"문장2\", ..., \"문장N\"]\n",
        "        # 🧪 validation_dataloader에서 원문 문장 추출 (예시)\n",
        "        dataset_sentences = []\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "          input_ids = batch[0]\n",
        "          for ids in input_ids:\n",
        "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            dataset_sentences.append(text)\n",
        "\n",
        "        for i, sentence in enumerate(dataset_sentences):\n",
        "          try:\n",
        "            pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "          except Exception as e:\n",
        "            print(f\"[❌ NaN 발생] 문장 index = {i}\")\n",
        "            print(\"문장 내용:\", sentence)\n",
        "            print(\"에러 메시지:\", e)\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "_dPquKTIGbOE",
        "outputId": "54615d57-7130-49f1-b469-d5b229ca4e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 적용 모델 validation"
      ],
      "metadata": {
        "id": "JSRL23_d7voR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "metadata": {
        "id": "h4m08g6HohMF",
        "outputId": "49f4c61d-8044-4532-9d24-432c54e23eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:07:27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 저장 및 로드 코드.\n",
        "path = '/content/model/'\n",
        "torch.save(model.state_dict(), path+\"CoLA.pt\")\n",
        "model.load_state_dict(torch.load(path+\"CoLA.pt\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6f3ddQs0x7V-",
        "outputId": "eeb472a5-b074-4bbe-a958-e6eafda42707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}