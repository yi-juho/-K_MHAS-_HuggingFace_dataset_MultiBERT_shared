{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLA_Document_Classification_BERT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yi-juho/-K_MHAS-_HuggingFace_dataset_MultiBERT_shared/blob/main/CoLA_Document_Classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Classification BERT [CoLA]\n",
        "\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SGcOaw5P769d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg-qog2IyzCc",
        "outputId": "099b09aa-2c0f-41e5-f807-64921cc3cff6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cordic"
      ],
      "metadata": {
        "id": "x6iTkvfbQ6Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def top_1200_input(attention_scores, dim=-1):\n",
        "    \"\"\"\n",
        "    1200개의 attention_scores를 120개의 리스트로 나누어 각 리스트를 top 함수에 전달\n",
        "    \"\"\"\n",
        "    chunk_size = len(attention_scores) // dim\n",
        "    data_list = [attention_scores[i * chunk_size:(i + 1) * chunk_size] for i in range(120)]\n",
        "\n",
        "    # 120개의 결과 리스트 생성\n",
        "    result_arrays = []\n",
        "\n",
        "    # 각 10개씩 top 함수에 전달\n",
        "    for i in range(120):\n",
        "        result = top(*data_list[i])  # 리스트를 개별 인자로 풀어서 전달\n",
        "        result_arrays.append(result)  # 결과 저장\n",
        "\n",
        "    return result_arrays\n",
        "\n",
        "\n",
        "\n",
        "def float_to_fixed_point(value, int_bits=7, frac_bits=13):\n",
        "    \"\"\"\n",
        "    실수를 32비트 Fixed-Point 바이너리로 변환하는 함수.\n",
        "    - 31~20 비트: 항상 0 (상위 12비트)\n",
        "    - 19~13 비트: 정수부 (7비트, Signed, 2의 보수)\n",
        "    - 12~0 비트: 소수부 (13비트, 항상 양수)\n",
        "\n",
        "    정수부는 내림하여 변환하고, 소수부를 정확한 2진 변환 방식으로 변환.\n",
        "    \"\"\"\n",
        "    max_frac_value = 2**frac_bits  # 13비트 소수부 정밀도 (8192)\n",
        "\n",
        "    # ✅ 1. 정수부와 소수부 정확하게 분리 (내림 적용)\n",
        "    int_part = np.floor(value).astype(int)\n",
        "    frac_part = abs(value - int_part)  # 소수부 절댓값 유지\n",
        "\n",
        "    # ✅ 2. 정수부(7비트) 변환 (2의 보수 변환)\n",
        "    if int_part < 0:\n",
        "        int_binary = format((1 << int_bits) + int_part, f'0{int_bits}b')  # 7비트 2의 보수 변환\n",
        "    else:\n",
        "        int_binary = format(int_part, f'0{int_bits}b')  # 7비트 양수 표현\n",
        "\n",
        "    # ✅ 3. 앞 12비트는 항상 0으로 유지\n",
        "    int_binary = \"0\" * 12 + int_binary\n",
        "\n",
        "    # ✅ 4. 소수부 변환 (부동소수점 오차 최소화)\n",
        "    frac_binary = \"\"\n",
        "    frac_value = frac_part  # 소수부 값 유지\n",
        "    for _ in range(frac_bits):  # 13비트 반복\n",
        "        frac_value *= 2\n",
        "        if frac_value >= 1:\n",
        "            frac_binary += \"1\"\n",
        "            frac_value -= 1\n",
        "        else:\n",
        "            frac_binary += \"0\"\n",
        "\n",
        "    # ✅ 5. 최종 32비트 바이너리 생성\n",
        "    fixed_binary = int_binary + frac_binary\n",
        "    fixed_binary_int = int(fixed_binary, 2)\n",
        "    lower_20_bits = fixed_binary_int & 0xFFFFF\n",
        "    return lower_20_bits\n",
        "\n",
        "def binary_to_decimal(binary_str):\n",
        "    # 16비트 중 앞 3비트는 정수부, 뒤 13비트는 소수부\n",
        "    int_part = int(binary_str[:3], 2)  # 정수부\n",
        "    frac_part = int(binary_str[3:], 2) / (2 ** 13)  # 소수부를 2^13으로 나눔\n",
        "    return int_part + frac_part  # 정수부와 소수부 합산\n",
        "\n",
        "def hex_to_binary(hex_str):\n",
        "    \"\"\"\n",
        "    16비트 헥스 값을 16비트 바이너리 문자열로 변환하는 함수\n",
        "\n",
        "    :param hex_str: 변환할 16비트 헥스 값 (예: \"1A2F\" 또는 0x1A2F)\n",
        "    :return: 16비트 바이너리 문자열 (예: \"0001101000101111\")\n",
        "    \"\"\"\n",
        "    # hex_str가 정수형일 경우 문자열로 변환\n",
        "    if isinstance(hex_str, int):\n",
        "        hex_str = format(hex_str, 'X')  # 16진수 대문자로 변환\n",
        "\n",
        "    # 16진수를 10진수로 변환 후, 16비트 이진수로 변환\n",
        "    binary_str = bin(int(hex_str, 16))[2:].zfill(16)\n",
        "    return binary_str\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "def top(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10) : #20bit data\n",
        "\n",
        "    data_11 = float_to_fixed_point(data_1)\n",
        "    data_22 = float_to_fixed_point(data_2)\n",
        "    data_33 = float_to_fixed_point(data_3)\n",
        "    data_44 = float_to_fixed_point(data_4)\n",
        "    data_55 = float_to_fixed_point(data_5)\n",
        "    data_66 = float_to_fixed_point(data_6)\n",
        "    data_77 = float_to_fixed_point(data_7)\n",
        "    data_88 = float_to_fixed_point(data_8)\n",
        "    data_99 = float_to_fixed_point(data_9)\n",
        "    data_1010 = float_to_fixed_point(data_10)\n",
        "\n",
        "\n",
        "    data_list = [data_11, data_22, data_33, data_44, data_55, data_66, data_77, data_88, data_99, data_1010]\n",
        "    #data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    int_list = [0] * 10 #exp_int input\n",
        "    exp_int_output = [0]*10 #exp_int output\n",
        "    fraction_list=[0]*10 #cordic_exu input\n",
        "    exp_fraction_output = [0]*10 #cordic_exu output\n",
        "    exp_whole=[0]*10 #cordic_exu +cordic_int\n",
        "    exp_trunc=[0]*10\n",
        "    exp_accum_input=[0]*10 # accumulator input  (exponential)\n",
        "    exp_accum_output=[0]*1\n",
        "    x_divider = [0] * 10 #cordic diu input\n",
        "    y_dividend = [0] * 10 #cordic diu input\n",
        "    data_out = [0] * 10\n",
        "    binary_values = [0] * 10\n",
        "    int_values = [0.0] * 10\n",
        "    for i in range(10):  # int part\n",
        "        int_list[i] = data_list[i] >> 13\n",
        "    exp_int_output[0], exp_int_output[1], exp_int_output[2], exp_int_output[3], exp_int_output[4], exp_int_output[5], exp_int_output[6], exp_int_output[7], exp_int_output[8], exp_int_output[9]=exp_int(int_list[0], int_list[1], int_list[2], int_list[3], int_list[4], int_list[5], int_list[6], int_list[7], int_list[8], int_list[9])\n",
        "\n",
        "    for i in range(10):  # fraction part\n",
        "        fraction_list[i] = (data_list[i] & 0x1FFF) << 1  # 하위 13비트를 가져오고, 뒤에 0을 추가\n",
        "    exp_fraction_output[0], exp_fraction_output[1], exp_fraction_output[2], exp_fraction_output[3], exp_fraction_output[4], exp_fraction_output[5], exp_fraction_output[6], exp_fraction_output[7], exp_fraction_output[8], exp_fraction_output[9] = cordic_exu(fraction_list[0], fraction_list[1], fraction_list[2], fraction_list[3], fraction_list[4], fraction_list[5], fraction_list[6], fraction_list[7], fraction_list[8], fraction_list[9])\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_whole[i] = exp_int_output[i] * exp_fraction_output[i]\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_trunc[i] = (exp_whole[i] >> 14) & 0xFFFFF  # 33~14 비트를 추출 (20비트)\n",
        "    for i in range(10):\n",
        "       exp_accum_input[i] = (exp_trunc[i] >> 4) & 0xFFFF\n",
        "\n",
        "    exp_accum_output[0] = cordic_accumulator(exp_accum_input[0],exp_accum_input[1],exp_accum_input[2],exp_accum_input[3],exp_accum_input[4],exp_accum_input[5],exp_accum_input[6],exp_accum_input[7],exp_accum_input[8],exp_accum_input[9])\n",
        "\n",
        "\n",
        "\n",
        "    # denominator_sf[i]에서 18~4 비트 추출 후, 앞에 0을 붙여서 x_divider에 할당\n",
        "    x_divider[0] = (exp_accum_output[0] >> 4) & 0x7FFF  # 18~4 비트를 추출 (15비트)\n",
        "\n",
        "    for i in range(10):\n",
        "    # numerator_sf[i]에서 15~4 비트 추출 후, 앞에 4개의 0을 붙여서 y_dividend에 할당\n",
        "        y_dividend[i] = (exp_accum_input[i] >> 4)  # 15~4 비트를 추출 (12비트)\n",
        "\n",
        "    data_out[0],data_out[1],data_out[2],data_out[3],data_out[4],data_out[5],data_out[6],data_out[7],data_out[8],data_out[9], = cordic_diu (x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],y_dividend[0],y_dividend[1],y_dividend[2],y_dividend[3],y_dividend[4],y_dividend[5],y_dividend[6],y_dividend[7],y_dividend[8],y_dividend[9])\n",
        "\n",
        "    for i in range(10):\n",
        "        binary_values[i] = hex_to_binary(data_out[i])\n",
        "       # print(binary_values[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        int_values[i] = binary_to_decimal(binary_values[i])\n",
        "        #print(int_values[i])\n",
        "\n",
        "    return int_values[0],int_values[1],int_values[2],int_values[3],int_values[4],int_values[5],int_values[6],int_values[7],int_values[8],int_values[9]\n",
        "   #return int_values\n",
        "\n",
        "def exp_int(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_to_int = [0]*10\n",
        "    int_output=[0]*10\n",
        "    for i in range(10):\n",
        "        # 이진수로 변환하여 signed_binary_to_int 함수에 전달\n",
        "        binary_str = bin(data_list[i])[2:].zfill(7)  # 20비트 이진수 문자열로 변환\n",
        "        data_to_int[i] = signed_binary_to_int(binary_str)\n",
        "\n",
        "    i_max = max(data_to_int)\n",
        "    offset=signed_binary_to_int(bin(0b0001011-i_max)[2:].zfill(7)) #offset\n",
        "\n",
        "    for i in range(10):\n",
        "        int_reg=data_to_int[i]+offset\n",
        "        #print(int_reg)\n",
        "        if int_reg==0 :\n",
        "            int_output[i]=0b0000_0000_0000_0001_00\n",
        "        elif int_reg==1 :\n",
        "            int_output[i]=0b0000_0000_0000_0010_11\n",
        "        elif int_reg==2 :\n",
        "            int_output[i]=0b0000_0000_0000_0111_10\n",
        "        elif int_reg==3 :\n",
        "            int_output[i]=0b0000_0000_0001_0100_00\n",
        "        elif int_reg==4 :\n",
        "            int_output[i]=0b0000_0000_0011_0110_10\n",
        "        elif int_reg==5 :\n",
        "            int_output[i]=0b0000_0000_1001_0100_10\n",
        "        elif int_reg==6 :\n",
        "            int_output[i]=0b0000_0001_1001_0011_10\n",
        "        elif int_reg==7 :\n",
        "            int_output[i]=0b0000_0100_0100_1000_11\n",
        "        elif int_reg==8 :\n",
        "            int_output[i]=0b0000_1011_1010_0101_00\n",
        "        elif int_reg==9 :\n",
        "            int_output[i]=0b0001_1111_1010_0111_00\n",
        "        elif int_reg==10 :\n",
        "            int_output[i]=0b0101_0110_0000_1010_10\n",
        "        elif int_reg==11 :\n",
        "            int_output[i]=0b1110_1001_1110_0010_01\n",
        "        else :\n",
        "            int_output[i]=0\n",
        "\n",
        "    return  int_output[0], int_output[1], int_output[2], int_output[3], int_output[4], int_output[5], int_output[6], int_output[7], int_output[8], int_output[9]\n",
        "\n",
        "\n",
        "def cordic_exu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_exu_pipe(data_list[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "def cordic_exu_pipe(z_in):\n",
        "    x_1, y_1, z_1=exp_unit(0b1001101010001111, 0, z_in, 0b0010001100100111,1)\n",
        "    x_2, y_2, z_2=exp_unit(x_1, y_1, z_1,0b0001000001011000,2)\n",
        "    x_3, y_3, z_3 = exp_unit(x_2, y_2, z_2, 0b0000100000001010, 3)\n",
        "    x_4, y_4, z_4 = exp_unit(x_3, y_3, z_3, 0b0000010000000001, 4)\n",
        "    x_5, y_5, z_5 = exp_unit(x_4, y_4, z_4, 0b0000001000000000, 5)\n",
        "    x_6, y_6, z_6 = exp_unit(x_5, y_5, z_5, 0b0000000100000000, 6)\n",
        "    x_7, y_7, z_7 = exp_unit(x_6, y_6, z_6, 0b0000000010000000, 7)\n",
        "    x_8, y_8, z_8 = exp_unit(x_7, y_7, z_7, 0b0000000001000000, 8)\n",
        "    x_9, y_9, z_9 = exp_unit(x_8, y_8, z_8, 0b0000000000100000, 9)\n",
        "    x_10, y_10, z_10 = exp_unit(x_9, y_9, z_9, 0b0000000000010000, 10)\n",
        "    x_11, y_11, z_11 = exp_unit(x_10, y_10, z_10, 0b0000000000001000, 11)\n",
        "    x_12, y_12, z_12 = exp_unit(x_11, y_11, z_11, 0b0000000000000100, 12)\n",
        "    x_13, y_13, z_13 = exp_unit(x_12, y_12, z_12, 0b0000000000000010, 13)\n",
        "    x_14, y_14, z_14 = exp_unit(x_13, y_13, z_13, 0b0000000000000010, 14)\n",
        "\n",
        "\n",
        "    exp_frac_reg=x_14+y_14\n",
        "    exp_frac = (exp_frac_reg >> 1) & 0xFFFF\n",
        "    #print(bin(exp_frac))\n",
        "    return exp_frac\n",
        "\n",
        "\n",
        "\n",
        "def exp_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "    x_cal = 0\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if z_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "        # 시계방향 (downward)\n",
        "        x_cal = x_in - (y_in >> pipe_num)\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "    else:\n",
        "        # 반시계방향 (upward)\n",
        "        x_cal = x_in + (y_in >> pipe_num)\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return x_cal, y_cal, z_cal\n",
        "\n",
        "\n",
        "def signed_binary_to_int(binary_str):\n",
        "    # 이진수의 길이\n",
        "    n = len(binary_str)\n",
        "\n",
        "    # 부호 비트 확인 (최상위 비트가 1이면 음수)\n",
        "    if binary_str[0] == '1':\n",
        "        # 2의 보수 방법으로 음수 변환\n",
        "        return int(binary_str, 2) - (1 << n)\n",
        "    else:\n",
        "        # 양수는 그냥 변환\n",
        "        return int(binary_str, 2)\n",
        "\n",
        "def accumulator(sequence_in, exponential, state):\n",
        "    \"\"\"\n",
        "    입력값을 받아 sum1, sum2, denominator_sf 값을 업데이트하고 출력.\n",
        "    \"\"\"\n",
        "\n",
        "    # accumulator_SEL 토글\n",
        "    state[\"accumulator_SEL\"] = (state[\"accumulator_SEL\"] + sequence_in) & 1\n",
        "\n",
        "    # valid 시프트 레지스터 갱신\n",
        "    state[\"valid\"] = [sequence_in] + state[\"valid\"][:-1]\n",
        "\n",
        "    if state[\"accumulator_SEL\"]:  # accumulator 1 사용\n",
        "        state[\"sum1\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum2\"] = exponential  # sum2 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum1\"]  # denominator_sf 업데이트\n",
        "    else:  # accumulator 2 사용\n",
        "        state[\"sum2\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum1\"] = exponential  # sum1 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum2\"]  # denominator_sf 업데이트\n",
        "\n",
        "    sequence_out = state[\"valid\"][0]\n",
        "\n",
        "    return  state[\"denominator_sf\"]\n",
        "\n",
        "\n",
        "def cordic_accumulator (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    state_accumulator = {\n",
        "    \"denominator_sf\": 0,\n",
        "    \"accumulator_SEL\": 0,\n",
        "    \"valid\": [0] * 11,\n",
        "    \"sum1\": 0,\n",
        "    \"sum2\": 0\n",
        "}\n",
        "\n",
        "    exp_fraction[0]=  accumulator(1,data_list[0],state_accumulator)\n",
        "    exp_fraction[1]=  accumulator( 0 ,data_list[1],state_accumulator)\n",
        "    exp_fraction[2]=  accumulator( 0 ,data_list[2],state_accumulator)\n",
        "    exp_fraction[3]=  accumulator( 0 ,data_list[3],state_accumulator)\n",
        "    exp_fraction[4]=  accumulator( 0 ,data_list[4],state_accumulator)\n",
        "    exp_fraction[5]=  accumulator( 0 ,data_list[5],state_accumulator)\n",
        "    exp_fraction[6]=  accumulator( 0 ,data_list[6],state_accumulator)\n",
        "    exp_fraction[7]=  accumulator( 0 ,data_list[7],state_accumulator)\n",
        "    exp_fraction[8]=  accumulator( 0 ,data_list[8],state_accumulator)\n",
        "    exp_fraction[9]=  accumulator( 0 ,data_list[9],state_accumulator)\n",
        "    #print(hex(exp_fraction[9]))\n",
        "\n",
        "    return  exp_fraction[9]\n",
        "\n",
        "\n",
        "\n",
        "def fifo(exponential, state):\n",
        "\n",
        "\n",
        "        # FIFO 시프트 (FIFO_reg[i] = FIFO_reg[i-1])\n",
        "    state[\"FIFO_reg\"] = [exponential] + state[\"FIFO_reg\"][:-1]\n",
        "\n",
        "    numerator_sf = state[\"FIFO_reg\"][9]\n",
        "\n",
        "\n",
        "    return numerator_sf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def div_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if y_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "\n",
        "\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "    else:\n",
        "\n",
        "\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return  y_cal, z_cal\n",
        "def cordic_diu_pipe(x_in, y_in):\n",
        "    y_1, z_1 = div_unit(x_in, y_in, 0, 0b0010000000000000, 0)\n",
        "    y_2, z_2 = div_unit(x_in, y_1, z_1, 0b0001000000000000, 1)\n",
        "    y_3, z_3 = div_unit(x_in, y_2, z_2, 0b0000100000000000, 2)\n",
        "    y_4, z_4 = div_unit(x_in, y_3, z_3, 0b0000010000000000, 3)\n",
        "    y_5, z_5 = div_unit(x_in, y_4, z_4, 0b0000001000000000, 4)\n",
        "    y_6, z_6 = div_unit(x_in, y_5, z_5, 0b0000000100000000, 5)\n",
        "    y_7, z_7 = div_unit(x_in, y_6, z_6, 0b0000000010000000, 6)\n",
        "    y_8, z_8 = div_unit(x_in, y_7, z_7, 0b0000000001000000, 7)\n",
        "    y_9, z_9 = div_unit(x_in, y_8, z_8, 0b0000000000100000, 8)\n",
        "    y_10, z_10 = div_unit(x_in, y_9, z_9, 0b0000000000010000, 9)\n",
        "    y_11, z_11 = div_unit(x_in, y_10, z_10, 0b0000000000001000, 10)\n",
        "    y_12, z_12 = div_unit(x_in, y_11, z_11, 0b0000000000000100, 11)\n",
        "    y_13, z_13 = div_unit(x_in, y_12, z_12, 0b0000000000000010, 12)\n",
        "    y_14, z_14 = div_unit(x_in, y_13, z_13, 0b0000000000000001, 13)\n",
        "    divided_val = z_14  # Final z value representing atan(y/x)\n",
        "    return divided_val\n",
        "\n",
        "def cordic_diu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10,data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20):\n",
        "    data_list_x = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_list_y = [data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_diu_pipe(data_list_x[i],data_list_y[i])\n",
        "        #print(bin(data_list[i]))\n",
        "        print(hex(exp_fraction[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "'''\n",
        "top(\n",
        "    0b00000011000110001110,  # 첫 번째 data_in 값\n",
        "    0b00000100010011110000,  # 두 번째 data_in 값\n",
        "    0b00000000010011111010,  # 세 번째 data_in 값\n",
        "    0b00000010101101111100,  # 네 번째 data_in 값\n",
        "    0b00000111000110011000,  # 다섯 번째 data_in 값\n",
        "    0b00000100010100001101,  # 여섯 번째 data_in 값\n",
        "    0b11111100110100100011,  # 일곱 번째 data_in 값\n",
        "    0b11111111100000001011,  # 여덟 번째 data_in 값\n",
        "    0b00000011010101100100,  # 아홉 번째 data_in 값\n",
        "    0b11111010100111110111   # 열 번째 data_in 값\n",
        ")\n",
        "'''\n",
        "top(1.548621,\n",
        "2.154268,\n",
        "0.15548,\n",
        "1.358942,\n",
        "3.54985,\n",
        "2.1578712,\n",
        "-1.58952,\n",
        "-0.248623,\n",
        "1.6685123,\n",
        "-2.68856\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # ✅ 기존 Softmax -> Sigmoid Normalization 적용\n",
        "        attention_probs = top(attention_scores)\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "HM1AXhHcQ04X",
        "outputId": "70e00c87-b52d-4802-d298-8efd7feceac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0x22f\n",
            "0x401\n",
            "0x87\n",
            "0x1d1\n",
            "0x1059\n",
            "0x401\n",
            "0xf\n",
            "0x57\n",
            "0x279\n",
            "0x1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 구조 적용,, 재시작"
      ],
      "metadata": {
        "id": "eA7KlC3ftQ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None,\n",
        "                past_key_value=None,\n",
        "                output_attentions=False):\n",
        "\n",
        "        # 1) Query/Key/Value 계산 (원본과 동일)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        key_layer   = self.transpose_for_scores(\n",
        "                          self.key(encoder_hidden_states\n",
        "                                   if encoder_hidden_states is not None\n",
        "                                   else hidden_states)\n",
        "                      )\n",
        "        value_layer = self.transpose_for_scores(\n",
        "                          self.value(encoder_hidden_states\n",
        "                                     if encoder_hidden_states is not None\n",
        "                                     else hidden_states)\n",
        "                      )\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 2) Attention score 계산 & scaling\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2)\n",
        "        ) / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        # 3) CORDIC-Softmax: top()을 120번 자동 호출\n",
        "        #    attention_scores.shape == [B, H, L, L] with L=10\n",
        "        # ---------------------------------------------------\n",
        "        B, H, L, _ = attention_scores.size()             # B=1, H=12, L=10\n",
        "        flat = attention_scores.view(-1, L)              # shape = [B*H, 10]\n",
        "        rows = []\n",
        "        for row in flat:                                 # 자동으로 1*12 = 12 행 × 10 쿼리 = 120 호출\n",
        "            # row.tolist() → Python float 리스트 길이 10\n",
        "            top_out = top(*row.tolist())                 # 여러분의 top(data1…data10)\n",
        "            # 다시 tensor 로 만들 때, dtype/device 일치시키기\n",
        "            rows.append(torch.tensor(\n",
        "                top_out,\n",
        "                dtype=attention_scores.dtype,\n",
        "                device=attention_scores.device\n",
        "            ))\n",
        "        attention_probs = torch.stack(rows)              # shape = [B*H, 10]\n",
        "        attention_probs = attention_probs.view(B, H, L, L)\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        # 4) Dropout & Context 계산 (원본과 동일)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) \\\n",
        "                  if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "zfSHmJVQtUqh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
        "\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None,\n",
        "                past_key_value=None,\n",
        "                output_attentions=False):\n",
        "\n",
        "        # 1) query/key/value (원본과 동일)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        key_layer   = self.transpose_for_scores(\n",
        "                          self.key(encoder_hidden_states\n",
        "                                   if encoder_hidden_states is not None\n",
        "                                   else hidden_states)\n",
        "                      )\n",
        "        value_layer = self.transpose_for_scores(\n",
        "                          self.value(encoder_hidden_states\n",
        "                                     if encoder_hidden_states is not None\n",
        "                                     else hidden_states)\n",
        "                      )\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 2) 스코어 계산 및 scaling\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2)\n",
        "        ) / (self.attention_head_size ** 0.5)\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # ─────────────────────────────────────────────\n",
        "        # 3) CORDIC-Softmax 교체: 여기에 아래 코드를 넣으세요\n",
        "        # ─────────────────────────────────────────────\n",
        "        B, H, L, _ = attention_scores.size()           # e.g. 1,12,10,10\n",
        "        flat = attention_scores.view(-1, L)            # [B*H, L]\n",
        "        rows = []\n",
        "\n",
        "        for row in flat:\n",
        "            # (1) 안정화: max 빼기\n",
        "            row = row - row.max()\n",
        "            # (2) 정수부 범위 클램핑 (–63.999 ~ +63.999)\n",
        "            row = row.clamp(min=-63.999, max=63.999)\n",
        "            # (3) Python float 리스트로 변환 → top()\n",
        "            out = top(*row.tolist())\n",
        "            # (4) 다시 tensor 생성\n",
        "            rows.append(torch.tensor(\n",
        "                out,\n",
        "                dtype=attention_scores.dtype,\n",
        "                device=attention_scores.device\n",
        "            ))\n",
        "\n",
        "        attention_probs = torch.stack(rows).view(B, H, L, L)\n",
        "        # ─────────────────────────────────────────────\n",
        "\n",
        "        # 4) dropout & context 계산 (원본과 동일)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) \\\n",
        "                  if output_attentions else (context_layer,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "JxoOuCl6wyU6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "for layer in model.bert.encoder.layer:\n",
        "    orig = layer.attention.self\n",
        "    # 1) Modified 모듈 생성\n",
        "    mod = BertSelfAttentionModified(config)\n",
        "    # 2) 기존 가중치 로드\n",
        "    mod.load_state_dict(orig.state_dict(), strict=False)\n",
        "    # 3) GPU로 보내기\n",
        "    mod.to(device)          # ← 이 줄을 추가!\n",
        "    # 4) 교체\n",
        "    layer.attention.self = mod\n"
      ],
      "metadata": {
        "id": "peAPhZdDvZzN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, layer in enumerate(model.bert.encoder.layer):\n",
        "    attn = layer.attention.self\n",
        "    print(f\"Layer {idx:2d}: {attn.__class__.__name__:<30} (module: {attn.__class__.__module__})\")\n"
      ],
      "metadata": {
        "id": "huL4Du0TtzwD",
        "outputId": "d1e82704-bb3b-47dc-8e07-9a5a0445ccb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer  0: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  1: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  2: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  3: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  4: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  5: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  6: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  7: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  8: BertSelfAttentionModified      (module: __main__)\n",
            "Layer  9: BertSelfAttentionModified      (module: __main__)\n",
            "Layer 10: BertSelfAttentionModified      (module: __main__)\n",
            "Layer 11: BertSelfAttentionModified      (module: __main__)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        b_input_ids,\n",
        "        attention_mask=b_input_mask,\n",
        "        output_attentions=True\n",
        "    )\n",
        "\n",
        "# 2) 튜플에서 마지막 레이어 확률만 꺼내기\n",
        "all_attentions = outputs.attentions    # tuple of length 12, each is [B, H, L, L]\n",
        "attention_probs = all_attentions[-1]   # 마지막 레이어의 [B, H, L, L] 텐서\n",
        "\n",
        "# 3) 마지막 축(키 방향)으로 합이 1인지 확인\n",
        "sums = attention_probs.sum(dim=-1)     # shape = [B, H, L]\n",
        "print(\"어텐션 확률 합:\", sums)\n"
      ],
      "metadata": {
        "id": "m2Fs1SMmxKoy",
        "outputId": "11b4a620-dc65-4f06-bb45-f272e73e2a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "0x1\n",
            "0x131\n",
            "0x1df\n",
            "0x611\n",
            "0x987\n",
            "0x25f\n",
            "0x65f\n",
            "0x1df\n",
            "0x127\n",
            "0x17f\n",
            "0x1\n",
            "0x121\n",
            "0x171\n",
            "0x721\n",
            "0x2d9\n",
            "0x18f\n",
            "0x871\n",
            "0x587\n",
            "0x2b7\n",
            "0x121\n",
            "0x1\n",
            "0xbf\n",
            "0xc1\n",
            "0x241\n",
            "0x1a1\n",
            "0x2cf\n",
            "0xff1\n",
            "0x5df\n",
            "0x15f\n",
            "0x8f\n",
            "0x1\n",
            "0x10f\n",
            "0x1f1\n",
            "0x4a1\n",
            "0x197\n",
            "0x419\n",
            "0xca1\n",
            "0x359\n",
            "0x1c7\n",
            "0xb1\n",
            "0x1\n",
            "0x171\n",
            "0x1b1\n",
            "0x497\n",
            "0x36f\n",
            "0x2b1\n",
            "0x7f1\n",
            "0x5e9\n",
            "0x301\n",
            "0x129\n",
            "0x1\n",
            "0x91\n",
            "0x23f\n",
            "0x4d1\n",
            "0x2b7\n",
            "0x3ff\n",
            "0x9ff\n",
            "0x4cf\n",
            "0x1af\n",
            "0x111\n",
            "0x1\n",
            "0x11b\n",
            "0x28f\n",
            "0x6f5\n",
            "0x135\n",
            "0x4a9\n",
            "0x789\n",
            "0x415\n",
            "0x269\n",
            "0x14f\n",
            "0x1\n",
            "0x1a1\n",
            "0x209\n",
            "0x7df\n",
            "0x2cf\n",
            "0x1c7\n",
            "0x869\n",
            "0x4f1\n",
            "0x181\n",
            "0xe1\n",
            "0x1\n",
            "0xdf\n",
            "0x431\n",
            "0x9c1\n",
            "0x1ef\n",
            "0x471\n",
            "0x469\n",
            "0x329\n",
            "0x187\n",
            "0x17f\n",
            "0x1\n",
            "0x107\n",
            "0xd9\n",
            "0x689\n",
            "0x4a9\n",
            "0x229\n",
            "0x9ff\n",
            "0x2df\n",
            "0x21f\n",
            "0x189\n",
            "0x1\n",
            "0x54d\n",
            "0x475\n",
            "0x3ff\n",
            "0x481\n",
            "0x151\n",
            "0x141\n",
            "0x2cb\n",
            "0x11b\n",
            "0x73b\n",
            "0x1\n",
            "0x611\n",
            "0x22f\n",
            "0x451\n",
            "0x313\n",
            "0x21f\n",
            "0x26f\n",
            "0x27f\n",
            "0x2c3\n",
            "0x675\n",
            "0x1\n",
            "0x55f\n",
            "0xc7\n",
            "0x27b\n",
            "0x207\n",
            "0x3b5\n",
            "0x42b\n",
            "0x4f9\n",
            "0x1f1\n",
            "0x67f\n",
            "0x1\n",
            "0x743\n",
            "0x20f\n",
            "0x2df\n",
            "0x435\n",
            "0x1a7\n",
            "0x181\n",
            "0x1e9\n",
            "0x3c3\n",
            "0x6bb\n",
            "0x1\n",
            "0x6a9\n",
            "0x121\n",
            "0x1e7\n",
            "0x369\n",
            "0x2ff\n",
            "0x1e9\n",
            "0x1b7\n",
            "0x1bf\n",
            "0xa91\n",
            "0x1\n",
            "0x5e1\n",
            "0x109\n",
            "0x241\n",
            "0x47f\n",
            "0x22f\n",
            "0x1e1\n",
            "0x1d9\n",
            "0x1b1\n",
            "0xac1\n",
            "0x1\n",
            "0x801\n",
            "0x129\n",
            "0x2b1\n",
            "0x3c1\n",
            "0x131\n",
            "0x141\n",
            "0x1bf\n",
            "0x1d1\n",
            "0xa2f\n",
            "0x1\n",
            "0x62f\n",
            "0x231\n",
            "0x1ff\n",
            "0x371\n",
            "0x201\n",
            "0x211\n",
            "0x249\n",
            "0x401\n",
            "0x7cf\n",
            "0x1\n",
            "0x477\n",
            "0x2c5\n",
            "0x381\n",
            "0x43f\n",
            "0x1e5\n",
            "0x263\n",
            "0x31f\n",
            "0x2b3\n",
            "0x6e3\n",
            "0x1\n",
            "0x857\n",
            "0x1f1\n",
            "0x30f\n",
            "0x367\n",
            "0x19f\n",
            "0x25f\n",
            "0x27f\n",
            "0x361\n",
            "0x541\n",
            "0x1\n",
            "0x3cd\n",
            "0x53b\n",
            "0x3bf\n",
            "0x269\n",
            "0x1e1\n",
            "0x2ab\n",
            "0x275\n",
            "0x32d\n",
            "0x691\n",
            "0x1\n",
            "0x5e7\n",
            "0x1f7\n",
            "0x4a1\n",
            "0x281\n",
            "0xc1\n",
            "0x18f\n",
            "0x81\n",
            "0x2e9\n",
            "0xb01\n",
            "0x1\n",
            "0xa37\n",
            "0x2ff\n",
            "0x33f\n",
            "0x169\n",
            "0xc1\n",
            "0x1c9\n",
            "0xc7\n",
            "0x64f\n",
            "0x479\n",
            "0x1\n",
            "0x901\n",
            "0x2df\n",
            "0x411\n",
            "0x2a1\n",
            "0xaf\n",
            "0x171\n",
            "0xc9\n",
            "0x27f\n",
            "0x7e9\n",
            "0x1\n",
            "0xb0f\n",
            "0x139\n",
            "0x377\n",
            "0x1ff\n",
            "0xf1\n",
            "0x17f\n",
            "0xe7\n",
            "0x4c1\n",
            "0x631\n",
            "0x1\n",
            "0x9bf\n",
            "0x19f\n",
            "0x371\n",
            "0x1e9\n",
            "0x151\n",
            "0x1ff\n",
            "0x189\n",
            "0x57f\n",
            "0x4f7\n",
            "0x1\n",
            "0x9af\n",
            "0x251\n",
            "0x3b1\n",
            "0x159\n",
            "0x14f\n",
            "0x1e1\n",
            "0x129\n",
            "0x559\n",
            "0x527\n",
            "0x1\n",
            "0x891\n",
            "0x1b1\n",
            "0x4ff\n",
            "0xef\n",
            "0x17f\n",
            "0x2bf\n",
            "0x10f\n",
            "0x4d7\n",
            "0x599\n",
            "0x1\n",
            "0x58d\n",
            "0x3d7\n",
            "0x37b\n",
            "0x1ff\n",
            "0x137\n",
            "0x239\n",
            "0x143\n",
            "0x551\n",
            "0x723\n",
            "0x1\n",
            "0x6f5\n",
            "0x2ef\n",
            "0x541\n",
            "0x201\n",
            "0x113\n",
            "0x2ff\n",
            "0xdb\n",
            "0x3a1\n",
            "0x637\n",
            "0x1\n",
            "0x1eb\n",
            "0x431\n",
            "0x3ff\n",
            "0x32f\n",
            "0x291\n",
            "0x331\n",
            "0x2f1\n",
            "0x52b\n",
            "0x4cb\n",
            "0x1\n",
            "0x1d3\n",
            "0x48f\n",
            "0x4ad\n",
            "0x425\n",
            "0x119\n",
            "0x291\n",
            "0x1a5\n",
            "0x3fd\n",
            "0x75f\n",
            "0x1\n",
            "0x261\n",
            "0x361\n",
            "0x41f\n",
            "0x3e1\n",
            "0x191\n",
            "0x31f\n",
            "0x1a1\n",
            "0x38f\n",
            "0x857\n",
            "0x1\n",
            "0x1d1\n",
            "0x3d5\n",
            "0x2cd\n",
            "0x6c1\n",
            "0x167\n",
            "0x2ff\n",
            "0x227\n",
            "0x5a7\n",
            "0x481\n",
            "0x1\n",
            "0x34f\n",
            "0x365\n",
            "0x2e1\n",
            "0x3bf\n",
            "0x24f\n",
            "0x3a9\n",
            "0x253\n",
            "0x5e9\n",
            "0x477\n",
            "0x1\n",
            "0x2cf\n",
            "0x427\n",
            "0x25f\n",
            "0x513\n",
            "0x1ff\n",
            "0x363\n",
            "0x24f\n",
            "0x569\n",
            "0x469\n",
            "0x1\n",
            "0x2b3\n",
            "0x407\n",
            "0x381\n",
            "0x47f\n",
            "0x101\n",
            "0x291\n",
            "0x1c3\n",
            "0x575\n",
            "0x65b\n",
            "0x1\n",
            "0x21f\n",
            "0x457\n",
            "0x375\n",
            "0x289\n",
            "0x171\n",
            "0x323\n",
            "0x219\n",
            "0x561\n",
            "0x771\n",
            "0x1\n",
            "0x11b\n",
            "0x481\n",
            "0x543\n",
            "0x40f\n",
            "0x22b\n",
            "0x31f\n",
            "0x2a1\n",
            "0x1eb\n",
            "0x729\n",
            "0x1\n",
            "0x2b1\n",
            "0x49d\n",
            "0x3f1\n",
            "0x3a1\n",
            "0x101\n",
            "0x269\n",
            "0x1db\n",
            "0x56d\n",
            "0x669\n",
            "0x1\n",
            "0x2f7\n",
            "0x309\n",
            "0x359\n",
            "0xc69\n",
            "0x13f\n",
            "0x1b7\n",
            "0x1f1\n",
            "0x20f\n",
            "0x32f\n",
            "0x1\n",
            "0x36f\n",
            "0x33f\n",
            "0x181\n",
            "0xb67\n",
            "0x11f\n",
            "0x101\n",
            "0xcf\n",
            "0x19f\n",
            "0x7b7\n",
            "0x1\n",
            "0x68f\n",
            "0x2a1\n",
            "0x171\n",
            "0x927\n",
            "0x13f\n",
            "0x121\n",
            "0xef\n",
            "0x327\n",
            "0x5c1\n",
            "0x1\n",
            "0x21f\n",
            "0x131\n",
            "0x141\n",
            "0xf9f\n",
            "0x27f\n",
            "0x1b1\n",
            "0xef\n",
            "0xc1\n",
            "0x5df\n",
            "0x1\n",
            "0x501\n",
            "0x31f\n",
            "0x271\n",
            "0x7b1\n",
            "0x301\n",
            "0x1ff\n",
            "0x13f\n",
            "0x271\n",
            "0x4f1\n",
            "0x1\n",
            "0x4b1\n",
            "0x389\n",
            "0x1a1\n",
            "0x95f\n",
            "0x17f\n",
            "0x13f\n",
            "0x107\n",
            "0x231\n",
            "0x6af\n",
            "0x1\n",
            "0x43f\n",
            "0x1c1\n",
            "0x1b7\n",
            "0xc97\n",
            "0x227\n",
            "0x1b7\n",
            "0x1ef\n",
            "0x1c1\n",
            "0x407\n",
            "0x1\n",
            "0x669\n",
            "0x205\n",
            "0x2fb\n",
            "0x769\n",
            "0x229\n",
            "0x1df\n",
            "0x231\n",
            "0x3c9\n",
            "0x32d\n",
            "0x1\n",
            "0x411\n",
            "0x251\n",
            "0x211\n",
            "0x861\n",
            "0xe7\n",
            "0x12f\n",
            "0x12f\n",
            "0x2e1\n",
            "0x8df\n",
            "0x1\n",
            "0x437\n",
            "0x401\n",
            "0x33f\n",
            "0xab9\n",
            "0x239\n",
            "0x181\n",
            "0x13f\n",
            "0x201\n",
            "0x2a9\n",
            "0x1\n",
            "0xdc7\n",
            "0x201\n",
            "0x111\n",
            "0x637\n",
            "0xa9\n",
            "0xc1\n",
            "0x217\n",
            "0x247\n",
            "0x2e7\n",
            "0x1\n",
            "0x10f1\n",
            "0x1bf\n",
            "0x301\n",
            "0x1df\n",
            "0x10f\n",
            "0xc1\n",
            "0x101\n",
            "0x28f\n",
            "0x2bf\n",
            "0x1\n",
            "0x158f\n",
            "0x9f\n",
            "0x221\n",
            "0xc1\n",
            "0xf1\n",
            "0xcf\n",
            "0x15f\n",
            "0x1d1\n",
            "0x1b1\n",
            "0x1\n",
            "0xc57\n",
            "0x12f\n",
            "0x2ff\n",
            "0x3f7\n",
            "0x219\n",
            "0x1bf\n",
            "0x2ff\n",
            "0x261\n",
            "0x247\n",
            "0x1\n",
            "0x127f\n",
            "0x81\n",
            "0x361\n",
            "0xff\n",
            "0xc1\n",
            "0x81\n",
            "0xff\n",
            "0x361\n",
            "0x2a1\n",
            "0x1\n",
            "0x1221\n",
            "0xbf\n",
            "0x2ef\n",
            "0x121\n",
            "0xff\n",
            "0xaf\n",
            "0x17f\n",
            "0x2df\n",
            "0x2c1\n",
            "0x1\n",
            "0x15ef\n",
            "0x8f\n",
            "0x26f\n",
            "0x91\n",
            "0x101\n",
            "0x81\n",
            "0x13f\n",
            "0x1c1\n",
            "0x181\n",
            "0x1\n",
            "0x1e81\n",
            "0x1\n",
            "0x2f\n",
            "0xf\n",
            "0x1\n",
            "0x1\n",
            "0xf\n",
            "0x6f\n",
            "0x41\n",
            "0x1\n",
            "0xcb9\n",
            "0x1d1\n",
            "0x159\n",
            "0x4a9\n",
            "0x97\n",
            "0x87\n",
            "0x187\n",
            "0x2ff\n",
            "0x587\n",
            "0x1\n",
            "0x1361\n",
            "0x221\n",
            "0x14f\n",
            "0x221\n",
            "0x7f\n",
            "0x81\n",
            "0xaf\n",
            "0x181\n",
            "0x381\n",
            "0x1\n",
            "0x361\n",
            "0x7b1\n",
            "0x149\n",
            "0xc01\n",
            "0xaf\n",
            "0x81\n",
            "0x1a7\n",
            "0xa1\n",
            "0x3ff\n",
            "0x1\n",
            "0x771\n",
            "0x901\n",
            "0x137\n",
            "0x65f\n",
            "0xc1\n",
            "0x99\n",
            "0x16f\n",
            "0x29f\n",
            "0x27f\n",
            "0x1\n",
            "0x3cf\n",
            "0x191\n",
            "0x9f9\n",
            "0x271\n",
            "0x4e7\n",
            "0x1c9\n",
            "0x3c1\n",
            "0x291\n",
            "0x111\n",
            "0x1\n",
            "0x24f\n",
            "0x92f\n",
            "0x79\n",
            "0xd51\n",
            "0xa1\n",
            "0xa9\n",
            "0x2af\n",
            "0x179\n",
            "0xef\n",
            "0x1\n",
            "0x62d\n",
            "0x3a9\n",
            "0x467\n",
            "0x457\n",
            "0x25f\n",
            "0x19b\n",
            "0x341\n",
            "0x41f\n",
            "0x1ff\n",
            "0x1\n",
            "0x4a5\n",
            "0x29b\n",
            "0x405\n",
            "0x5ff\n",
            "0x2b1\n",
            "0x1ef\n",
            "0x571\n",
            "0x355\n",
            "0x133\n",
            "0x1\n",
            "0x337\n",
            "0x2e7\n",
            "0x19f\n",
            "0xb57\n",
            "0x1f1\n",
            "0x181\n",
            "0x647\n",
            "0x1f7\n",
            "0x131\n",
            "0x1\n",
            "0x731\n",
            "0x539\n",
            "0x32d\n",
            "0x57d\n",
            "0x1a1\n",
            "0xbf\n",
            "0x177\n",
            "0x42d\n",
            "0x2ed\n",
            "0x1\n",
            "0x5ff\n",
            "0x5e1\n",
            "0x15f\n",
            "0x4df\n",
            "0xff\n",
            "0xd7\n",
            "0x13f\n",
            "0x2cf\n",
            "0x7ff\n",
            "0x1\n",
            "0x3e1\n",
            "0x98f\n",
            "0xb7\n",
            "0xdf7\n",
            "0x37\n",
            "0x39\n",
            "0x101\n",
            "0x117\n",
            "0x147\n",
            "0x1\n",
            "0x727\n",
            "0x337\n",
            "0x8bf\n",
            "0x2bf\n",
            "0x159\n",
            "0x127\n",
            "0x101\n",
            "0x101\n",
            "0x579\n",
            "0x1\n",
            "0x927\n",
            "0x297\n",
            "0x291\n",
            "0x18f\n",
            "0x59\n",
            "0x81\n",
            "0x9f\n",
            "0x171\n",
            "0xce7\n",
            "0x1\n",
            "0xbb7\n",
            "0x2b1\n",
            "0x349\n",
            "0x11f\n",
            "0x71\n",
            "0xb9\n",
            "0x9f\n",
            "0x15f\n",
            "0x9ff\n",
            "0x1\n",
            "0xa5f\n",
            "0x2af\n",
            "0x321\n",
            "0x121\n",
            "0x49\n",
            "0x87\n",
            "0xa9\n",
            "0x14f\n",
            "0xb9f\n",
            "0x1\n",
            "0x891\n",
            "0x201\n",
            "0x58f\n",
            "0xf9\n",
            "0x111\n",
            "0xdf\n",
            "0xc7\n",
            "0x1e1\n",
            "0xa47\n",
            "0x1\n",
            "0xbf9\n",
            "0x2a1\n",
            "0x401\n",
            "0x101\n",
            "0x79\n",
            "0x8f\n",
            "0x81\n",
            "0x1d1\n",
            "0x8c1\n",
            "0x1\n",
            "0xb4f\n",
            "0x2b9\n",
            "0x4c1\n",
            "0xaf\n",
            "0xc1\n",
            "0xdf\n",
            "0xcf\n",
            "0x1a1\n",
            "0x861\n",
            "0x1\n",
            "0x77f\n",
            "0x2c9\n",
            "0x3e1\n",
            "0x141\n",
            "0x81\n",
            "0x9f\n",
            "0x87\n",
            "0x1f1\n",
            "0xcd1\n",
            "0x1\n",
            "0x821\n",
            "0x51f\n",
            "0x3df\n",
            "0x36f\n",
            "0xaf\n",
            "0xff\n",
            "0x131\n",
            "0x1c1\n",
            "0x6bf\n",
            "0x1\n",
            "0x7bf\n",
            "0x211\n",
            "0x1ff\n",
            "0x1a1\n",
            "0x31\n",
            "0x51\n",
            "0xbf\n",
            "0x12f\n",
            "0xfdf\n",
            "0x1\n",
            "0x35f\n",
            "0x90f\n",
            "0x241\n",
            "0x4d9\n",
            "0x127\n",
            "0x121\n",
            "0x14f\n",
            "0x359\n",
            "0x579\n",
            "0x1\n",
            "0x765\n",
            "0x529\n",
            "0x267\n",
            "0x409\n",
            "0x1f5\n",
            "0x1b5\n",
            "0x1cb\n",
            "0x3d7\n",
            "0x3bb\n",
            "0x1\n",
            "0x4b7\n",
            "0x399\n",
            "0x2f1\n",
            "0x5ef\n",
            "0x32d\n",
            "0x22f\n",
            "0x50b\n",
            "0x2e7\n",
            "0x17d\n",
            "0x1\n",
            "0x771\n",
            "0x585\n",
            "0x1eb\n",
            "0x5ed\n",
            "0x129\n",
            "0x123\n",
            "0x1f1\n",
            "0x2c1\n",
            "0x42f\n",
            "0x1\n",
            "0x68f\n",
            "0x34d\n",
            "0x273\n",
            "0x63d\n",
            "0x2a1\n",
            "0x1eb\n",
            "0x3a5\n",
            "0x37f\n",
            "0x1a3\n",
            "0x1\n",
            "0x4ad\n",
            "0x515\n",
            "0x1e7\n",
            "0x76d\n",
            "0x2e3\n",
            "0x1a1\n",
            "0x3b5\n",
            "0x2bb\n",
            "0x1eb\n",
            "0x1\n",
            "0x4e1\n",
            "0x891\n",
            "0x1bf\n",
            "0x761\n",
            "0x12f\n",
            "0x101\n",
            "0x2cf\n",
            "0x237\n",
            "0x217\n",
            "0x1\n",
            "0x653\n",
            "0x357\n",
            "0x361\n",
            "0x2a1\n",
            "0x2f3\n",
            "0x301\n",
            "0x451\n",
            "0x391\n",
            "0x271\n",
            "0x1\n",
            "0x5b5\n",
            "0x48b\n",
            "0x209\n",
            "0x247\n",
            "0x1bf\n",
            "0x1a3\n",
            "0x1cf\n",
            "0x569\n",
            "0x6bb\n",
            "0x1\n",
            "0x5e9\n",
            "0x759\n",
            "0x19f\n",
            "0x8a1\n",
            "0xd9\n",
            "0xdf\n",
            "0x151\n",
            "0x19f\n",
            "0x3bf\n",
            "0x1\n",
            "0xa3f\n",
            "0xe1\n",
            "0x55f\n",
            "0x91\n",
            "0x24f\n",
            "0x421\n",
            "0x241\n",
            "0x567\n",
            "0xc1\n",
            "0x1\n",
            "0x891\n",
            "0x359\n",
            "0x29f\n",
            "0x1d1\n",
            "0x121\n",
            "0x347\n",
            "0x22f\n",
            "0x521\n",
            "0x3cf\n",
            "0x1\n",
            "0xc1\n",
            "0x2f\n",
            "0x16ff\n",
            "0x2f\n",
            "0x1bf\n",
            "0x2d1\n",
            "0xff\n",
            "0x17f\n",
            "0x7f\n",
            "0x1\n",
            "0x4c3\n",
            "0x2d9\n",
            "0x1b9\n",
            "0x761\n",
            "0x16f\n",
            "0x301\n",
            "0x30d\n",
            "0x5c9\n",
            "0x1ef\n",
            "0x1\n",
            "0xc1\n",
            "0xf\n",
            "0x3d1\n",
            "0x11\n",
            "0x11c1\n",
            "0x691\n",
            "0x8f\n",
            "0x1cf\n",
            "0x6f\n",
            "0x1\n",
            "0x201\n",
            "0x81\n",
            "0x567\n",
            "0x7f\n",
            "0x6c9\n",
            "0xac7\n",
            "0x19f\n",
            "0x369\n",
            "0xb9\n",
            "0x1\n",
            "0x437\n",
            "0x17f\n",
            "0x291\n",
            "0x141\n",
            "0x1bf\n",
            "0x511\n",
            "0x8df\n",
            "0x561\n",
            "0x141\n",
            "0x1\n",
            "0x319\n",
            "0x199\n",
            "0x511\n",
            "0xd7\n",
            "0x25f\n",
            "0x271\n",
            "0x179\n",
            "0xd59\n",
            "0x1a1\n",
            "0x1\n",
            "0x76f\n",
            "0x16b\n",
            "0x463\n",
            "0x10b\n",
            "0x207\n",
            "0x4d7\n",
            "0x1b5\n",
            "0x611\n",
            "0x2f3\n",
            "0x1\n",
            "0x5f9\n",
            "0x92f\n",
            "0xff\n",
            "0x5e1\n",
            "0x6f\n",
            "0x111\n",
            "0x15f\n",
            "0x381\n",
            "0x369\n",
            "0x1\n",
            "0x4c9\n",
            "0x3ff\n",
            "0x2c1\n",
            "0x4cf\n",
            "0x591\n",
            "0x211\n",
            "0x3d3\n",
            "0x12f\n",
            "0x301\n",
            "0x1\n",
            "0x457\n",
            "0xcdf\n",
            "0xf1\n",
            "0x209\n",
            "0x261\n",
            "0x1ef\n",
            "0x12f\n",
            "0x311\n",
            "0x337\n",
            "0x1\n",
            "0x723\n",
            "0x26f\n",
            "0x227\n",
            "0x91\n",
            "0x4e3\n",
            "0x319\n",
            "0x115\n",
            "0x5d9\n",
            "0x4c9\n",
            "0x1\n",
            "0x759\n",
            "0x45f\n",
            "0x93\n",
            "0x391\n",
            "0x2b7\n",
            "0x1d5\n",
            "0x111\n",
            "0x539\n",
            "0x545\n",
            "0x1\n",
            "0x757\n",
            "0x1fd\n",
            "0x1f5\n",
            "0xc3\n",
            "0x5f1\n",
            "0x2d5\n",
            "0x159\n",
            "0x5ab\n",
            "0x437\n",
            "0x1\n",
            "0x559\n",
            "0x26f\n",
            "0x1fd\n",
            "0x139\n",
            "0x457\n",
            "0x493\n",
            "0x317\n",
            "0x629\n",
            "0x2d1\n",
            "0x1\n",
            "0x607\n",
            "0x1bf\n",
            "0x17b\n",
            "0xef\n",
            "0x37b\n",
            "0x3ff\n",
            "0x401\n",
            "0x6ff\n",
            "0x343\n",
            "0x1\n",
            "0xaa9\n",
            "0x1ff\n",
            "0xf1\n",
            "0x11f\n",
            "0x1c7\n",
            "0xe7\n",
            "0x13f\n",
            "0x8c7\n",
            "0x461\n",
            "0x1\n",
            "0x3df\n",
            "0x3ef\n",
            "0x263\n",
            "0x1d1\n",
            "0x5c1\n",
            "0x2bf\n",
            "0x2dd\n",
            "0x427\n",
            "0x46f\n",
            "0x1\n",
            "0x5e1\n",
            "0x7ff\n",
            "0x1d1\n",
            "0x23f\n",
            "0x1f1\n",
            "0x211\n",
            "0x19f\n",
            "0x3f9\n",
            "0x479\n",
            "0x1\n",
            "0x157\n",
            "0x52f\n",
            "0x349\n",
            "0x3f7\n",
            "0x157\n",
            "0x169\n",
            "0x301\n",
            "0x97f\n",
            "0x2f1\n",
            "0x1\n",
            "0x3d1\n",
            "0x591\n",
            "0x1f9\n",
            "0x627\n",
            "0x205\n",
            "0x201\n",
            "0x433\n",
            "0x27d\n",
            "0x3ad\n",
            "0x1\n",
            "0x527\n",
            "0x32f\n",
            "0x171\n",
            "0x281\n",
            "0x241\n",
            "0x29f\n",
            "0x9bf\n",
            "0x3a7\n",
            "0x14f\n",
            "0x1\n",
            "0x299\n",
            "0x601\n",
            "0x157\n",
            "0xb71\n",
            "0x1f1\n",
            "0x1ff\n",
            "0x321\n",
            "0x181\n",
            "0x1e9\n",
            "0x1\n",
            "0x129\n",
            "0x1b1\n",
            "0x131\n",
            "0x279\n",
            "0x3df\n",
            "0x2f1\n",
            "0xf09\n",
            "0x311\n",
            "0x6f\n",
            "0x1\n",
            "0x261\n",
            "0x351\n",
            "0x109\n",
            "0x3df\n",
            "0x2a1\n",
            "0x267\n",
            "0xbe9\n",
            "0x36f\n",
            "0xdf\n",
            "0x1\n",
            "0x201\n",
            "0x30f\n",
            "0x177\n",
            "0x60f\n",
            "0x2c1\n",
            "0x271\n",
            "0x971\n",
            "0x2f1\n",
            "0x1c7\n",
            "0x1\n",
            "0x219\n",
            "0x39f\n",
            "0x16f\n",
            "0x2e7\n",
            "0x34f\n",
            "0x37f\n",
            "0xa7f\n",
            "0x327\n",
            "0x167\n",
            "0x1\n",
            "0x381\n",
            "0x6bd\n",
            "0x1a1\n",
            "0x469\n",
            "0xc9\n",
            "0x111\n",
            "0x377\n",
            "0x5ed\n",
            "0x461\n",
            "0x1\n",
            "0x3c1\n",
            "0x5fd\n",
            "0x167\n",
            "0x65b\n",
            "0x2bf\n",
            "0x281\n",
            "0x475\n",
            "0x2dd\n",
            "0x1e3\n",
            "0x1\n",
            "0x1b1\n",
            "0x6df\n",
            "0x3e9\n",
            "0x67f\n",
            "0xd9\n",
            "0x11f\n",
            "0x17f\n",
            "0x3ff\n",
            "0x59b\n",
            "0x1\n",
            "0x281\n",
            "0x5b1\n",
            "0x20f\n",
            "0xdf1\n",
            "0xcf\n",
            "0xbf\n",
            "0xa9\n",
            "0x139\n",
            "0x439\n",
            "0x1\n",
            "0x341\n",
            "0x5bf\n",
            "0x309\n",
            "0xb11\n",
            "0x1f9\n",
            "0x18f\n",
            "0x147\n",
            "0x107\n",
            "0x2e1\n",
            "0x1\n",
            "0x181\n",
            "0x52f\n",
            "0xbf\n",
            "0x13e1\n",
            "0x31\n",
            "0x5f\n",
            "0x101\n",
            "0xa1\n",
            "0x261\n",
            "0x1\n",
            "0x1ff\n",
            "0x54f\n",
            "0x1f7\n",
            "0xe77\n",
            "0x14f\n",
            "0x169\n",
            "0x189\n",
            "0xf9\n",
            "0x2df\n",
            "0x1\n",
            "0x2bf\n",
            "0x4c1\n",
            "0x201\n",
            "0xbb9\n",
            "0x1e9\n",
            "0x239\n",
            "0x241\n",
            "0x121\n",
            "0x311\n",
            "0x1\n",
            "0x33f\n",
            "0x61f\n",
            "0x241\n",
            "0x8c1\n",
            "0x18f\n",
            "0x259\n",
            "0x26f\n",
            "0x169\n",
            "0x3c1\n",
            "0x1\n",
            "0x27f\n",
            "0x73f\n",
            "0x201\n",
            "0xc81\n",
            "0xd9\n",
            "0x127\n",
            "0xc7\n",
            "0x17f\n",
            "0x37f\n",
            "0x1\n",
            "0x327\n",
            "0x5dd\n",
            "0x2bf\n",
            "0x61f\n",
            "0x11d\n",
            "0x121\n",
            "0x121\n",
            "0x333\n",
            "0x775\n",
            "0x1\n",
            "0x211\n",
            "0x571\n",
            "0x17f\n",
            "0x1121\n",
            "0x61\n",
            "0x8f\n",
            "0xbf\n",
            "0xbf\n",
            "0x32f\n",
            "0x1\n",
            "0x9a1\n",
            "0x4cf\n",
            "0x3b9\n",
            "0x43f\n",
            "0x1d1\n",
            "0xe7\n",
            "0x25f\n",
            "0x2e1\n",
            "0xd7\n",
            "0x99\n",
            "0x87f\n",
            "0x4a1\n",
            "0x329\n",
            "0x4d9\n",
            "0x1a1\n",
            "0x19f\n",
            "0x2f7\n",
            "0x367\n",
            "0xa1\n",
            "0x91\n",
            "0x481\n",
            "0x191\n",
            "0x261\n",
            "0x7b1\n",
            "0x301\n",
            "0x1ff\n",
            "0x281\n",
            "0x637\n",
            "0x129\n",
            "0xb9\n",
            "0x373\n",
            "0x147\n",
            "0x217\n",
            "0x78d\n",
            "0x423\n",
            "0x1ff\n",
            "0x285\n",
            "0x68f\n",
            "0x161\n",
            "0xef\n",
            "0x399\n",
            "0x101\n",
            "0x1a9\n",
            "0x769\n",
            "0x2c1\n",
            "0x199\n",
            "0x3af\n",
            "0x811\n",
            "0x101\n",
            "0x111\n",
            "0x44f\n",
            "0x149\n",
            "0x1bf\n",
            "0x881\n",
            "0x32f\n",
            "0x2c1\n",
            "0x2c7\n",
            "0x5ff\n",
            "0xc7\n",
            "0xa1\n",
            "0x531\n",
            "0x303\n",
            "0x299\n",
            "0x6a9\n",
            "0x217\n",
            "0x1ef\n",
            "0x4c1\n",
            "0x439\n",
            "0x9b\n",
            "0xcf\n",
            "0x507\n",
            "0x27f\n",
            "0x1ff\n",
            "0x43b\n",
            "0x231\n",
            "0x26b\n",
            "0x501\n",
            "0x5cd\n",
            "0x1c9\n",
            "0x109\n",
            "0x561\n",
            "0x2b3\n",
            "0x1e1\n",
            "0x55f\n",
            "0x315\n",
            "0x191\n",
            "0x35f\n",
            "0x605\n",
            "0x145\n",
            "0x143\n",
            "0x63f\n",
            "0x425\n",
            "0x19f\n",
            "0x58b\n",
            "0x2c1\n",
            "0x175\n",
            "0x33f\n",
            "0x503\n",
            "0xdf\n",
            "0x101\n",
            "0xa57\n",
            "0x609\n",
            "0x35f\n",
            "0x2e9\n",
            "0x1bf\n",
            "0x1e1\n",
            "0x161\n",
            "0x1ff\n",
            "0x9f\n",
            "0x18f\n",
            "0x9cf\n",
            "0x571\n",
            "0x309\n",
            "0x3e7\n",
            "0x201\n",
            "0x2cf\n",
            "0x15f\n",
            "0x209\n",
            "0x51\n",
            "0x10f\n",
            "0x94f\n",
            "0x639\n",
            "0x2c9\n",
            "0x3af\n",
            "0x1c7\n",
            "0x227\n",
            "0x1d9\n",
            "0x28f\n",
            "0x51\n",
            "0x141\n",
            "0x4eb\n",
            "0x387\n",
            "0x473\n",
            "0x499\n",
            "0x36f\n",
            "0x36f\n",
            "0x23f\n",
            "0x25f\n",
            "0x5f\n",
            "0x295\n",
            "0x497\n",
            "0x2e5\n",
            "0x573\n",
            "0x76d\n",
            "0x2df\n",
            "0x305\n",
            "0x17d\n",
            "0x1a1\n",
            "0x49\n",
            "0x231\n",
            "0x4c3\n",
            "0x3a3\n",
            "0x321\n",
            "0x55f\n",
            "0x29d\n",
            "0x44f\n",
            "0x2c1\n",
            "0x31b\n",
            "0x6d\n",
            "0x1e1\n",
            "0x36f\n",
            "0x291\n",
            "0x241\n",
            "0x2e9\n",
            "0x141\n",
            "0xa2f\n",
            "0x321\n",
            "0x401\n",
            "0x7f\n",
            "0x187\n",
            "0x331\n",
            "0x1a9\n",
            "0x141\n",
            "0x209\n",
            "0x181\n",
            "0xd3f\n",
            "0x261\n",
            "0x2df\n",
            "0x13f\n",
            "0x259\n",
            "0x57f\n",
            "0x2bf\n",
            "0x1dd\n",
            "0x36b\n",
            "0x1bf\n",
            "0x2c7\n",
            "0x39d\n",
            "0x5c9\n",
            "0x139\n",
            "0x345\n",
            "0x3c1\n",
            "0x2b3\n",
            "0x1f5\n",
            "0x381\n",
            "0x1bb\n",
            "0x501\n",
            "0x3b3\n",
            "0x677\n",
            "0xc1\n",
            "0x25f\n",
            "0x541\n",
            "0x56f\n",
            "0x45f\n",
            "0x8af\n",
            "0x259\n",
            "0xbf\n",
            "0x1d1\n",
            "0x19f\n",
            "0x51\n",
            "0x12f\n",
            "0x4f9\n",
            "0x95f\n",
            "0x2f1\n",
            "0x5ef\n",
            "0x171\n",
            "0x99\n",
            "0x331\n",
            "0x191\n",
            "0x5f\n",
            "0x16f\n",
            "0x2e7\n",
            "0x377\n",
            "0x447\n",
            "0xbbf\n",
            "0x2d7\n",
            "0x16f\n",
            "0x2a7\n",
            "0x161\n",
            "0x1f\n",
            "0xe1\n",
            "0x151\n",
            "0x141\n",
            "0x281\n",
            "0x1141\n",
            "0x3c1\n",
            "0x26f\n",
            "0x111\n",
            "0xff\n",
            "0x21\n",
            "0xa1\n",
            "0x3bf\n",
            "0x37f\n",
            "0x321\n",
            "0xb19\n",
            "0x481\n",
            "0x17f\n",
            "0x27f\n",
            "0xd9\n",
            "0x21\n",
            "0xf1\n",
            "0x2f7\n",
            "0x26f\n",
            "0x251\n",
            "0x997\n",
            "0x301\n",
            "0x387\n",
            "0x4ef\n",
            "0x1f7\n",
            "0x1f\n",
            "0xf7\n",
            "0x327\n",
            "0x51f\n",
            "0x237\n",
            "0x795\n",
            "0x1df\n",
            "0x105\n",
            "0x723\n",
            "0x1b5\n",
            "0x63\n",
            "0x1c5\n",
            "0x37f\n",
            "0x40d\n",
            "0x1a3\n",
            "0x6e1\n",
            "0x401\n",
            "0x1f9\n",
            "0x36f\n",
            "0x2eb\n",
            "0xaf\n",
            "0x2d9\n",
            "0x4ab\n",
            "0x3cb\n",
            "0x1c1\n",
            "0x675\n",
            "0x2a3\n",
            "0x141\n",
            "0x4a1\n",
            "0x2bb\n",
            "0xad\n",
            "0x363\n",
            "0x3b1\n",
            "0x319\n",
            "0x1cf\n",
            "0x5ff\n",
            "0x2f5\n",
            "0x251\n",
            "0x59d\n",
            "0x2b1\n",
            "0x85\n",
            "0x331\n",
            "0x755\n",
            "0x2c7\n",
            "0x3e5\n",
            "0x673\n",
            "0x50f\n",
            "0xfd\n",
            "0xf1\n",
            "0x13d\n",
            "0xe7\n",
            "0x267\n",
            "0x6f9\n",
            "0x3a9\n",
            "0x415\n",
            "0x679\n",
            "0x4cb\n",
            "0x171\n",
            "0xf1\n",
            "0x137\n",
            "0xa1\n",
            "0x1cb\n",
            "0x5e1\n",
            "0x27f\n",
            "0x3a1\n",
            "0x8df\n",
            "0x5b9\n",
            "0x151\n",
            "0x97\n",
            "0xf1\n",
            "0x9f\n",
            "0x1ef\n",
            "0x2cf\n",
            "0x171\n",
            "0x2c7\n",
            "0xbff\n",
            "0x67f\n",
            "0x22f\n",
            "0xb1\n",
            "0x131\n",
            "0x81\n",
            "0x1c7\n",
            "0x2f7\n",
            "0x117\n",
            "0x2d1\n",
            "0xc01\n",
            "0x89f\n",
            "0x207\n",
            "0x47\n",
            "0x97\n",
            "0x3f\n",
            "0xff\n",
            "0x25f\n",
            "0x157\n",
            "0x329\n",
            "0xd91\n",
            "0x487\n",
            "0x22f\n",
            "0x57\n",
            "0x199\n",
            "0x77\n",
            "0x24f\n",
            "0x401\n",
            "0x451\n",
            "0x571\n",
            "0x5c1\n",
            "0x297\n",
            "0x191\n",
            "0x21b\n",
            "0x2bb\n",
            "0x109\n",
            "0x253\n",
            "0x3e5\n",
            "0x2bb\n",
            "0x55f\n",
            "0x5e1\n",
            "0x37d\n",
            "0x18f\n",
            "0x1af\n",
            "0x379\n",
            "0x191\n",
            "0x25b\n",
            "0x4ff\n",
            "0x17f\n",
            "0x389\n",
            "0x6f9\n",
            "0x67d\n",
            "0x183\n",
            "0x10f\n",
            "0x257\n",
            "0x1c7\n",
            "0x1c1\n",
            "0x2c7\n",
            "0x101\n",
            "0x217\n",
            "0x859\n",
            "0x67f\n",
            "0x2a7\n",
            "0x13f\n",
            "0x21f\n",
            "0x13f\n",
            "0x3ef\n",
            "0x11e1\n",
            "0x44f\n",
            "0x221\n",
            "0x151\n",
            "0xef\n",
            "0x71\n",
            "0x13f\n",
            "0x2bf\n",
            "0x3f\n",
            "0x6f\n",
            "0x11bf\n",
            "0x3e1\n",
            "0x141\n",
            "0x161\n",
            "0xbf\n",
            "0xbf\n",
            "0x151\n",
            "0x401\n",
            "0x41\n",
            "0x61\n",
            "0x11a1\n",
            "0x2f1\n",
            "0x1ff\n",
            "0x281\n",
            "0x11f\n",
            "0xdf\n",
            "0x101\n",
            "0x311\n",
            "0x1f\n",
            "0x4f\n",
            "0xb69\n",
            "0x227\n",
            "0x2f1\n",
            "0x3bf\n",
            "0x161\n",
            "0x179\n",
            "0x261\n",
            "0x57f\n",
            "0x49\n",
            "0xa1\n",
            "0xc7f\n",
            "0x2ff\n",
            "0x209\n",
            "0x32f\n",
            "0x2c1\n",
            "0xa9\n",
            "0x1a9\n",
            "0x491\n",
            "0x71\n",
            "0xff\n",
            "0xa7f\n",
            "0x1e1\n",
            "0x227\n",
            "0x2af\n",
            "0x177\n",
            "0x181\n",
            "0x281\n",
            "0x7ff\n",
            "0x7f\n",
            "0xbf\n",
            "0x4a9\n",
            "0xef\n",
            "0x161\n",
            "0x2e1\n",
            "0x181\n",
            "0x3d1\n",
            "0x3d1\n",
            "0xaf7\n",
            "0xa1\n",
            "0x149\n",
            "0x5a9\n",
            "0x107\n",
            "0x1b7\n",
            "0x301\n",
            "0x17f\n",
            "0x2df\n",
            "0x3c1\n",
            "0xa39\n",
            "0x101\n",
            "0x111\n",
            "0x441\n",
            "0xe9\n",
            "0xf1\n",
            "0x2bf\n",
            "0x1b1\n",
            "0x201\n",
            "0x259\n",
            "0xd87\n",
            "0x157\n",
            "0x217\n",
            "0x3ff\n",
            "0xb1\n",
            "0x101\n",
            "0x27f\n",
            "0x151\n",
            "0x201\n",
            "0x287\n",
            "0xcc1\n",
            "0x1cf\n",
            "0x331\n",
            "0x471\n",
            "0x627\n",
            "0xff\n",
            "0x10f\n",
            "0x181\n",
            "0xd9\n",
            "0x1f1\n",
            "0x9af\n",
            "0x2a7\n",
            "0x29f\n",
            "0x381\n",
            "0xdc7\n",
            "0x87\n",
            "0x79\n",
            "0x91\n",
            "0x12f\n",
            "0x20f\n",
            "0x7c1\n",
            "0xf7\n",
            "0xff\n",
            "0x361\n",
            "0x27f\n",
            "0x101\n",
            "0x13f\n",
            "0x11f\n",
            "0x1bf\n",
            "0x15f\n",
            "0x1021\n",
            "0x1a1\n",
            "0x181\n",
            "0x251\n",
            "0x16f\n",
            "0x6f\n",
            "0x241\n",
            "0x101\n",
            "0x121\n",
            "0x1bf\n",
            "0x12bf\n",
            "0x13f\n",
            "0x141\n",
            "0x19f\n",
            "0x23f\n",
            "0x6f\n",
            "0x11f\n",
            "0x1e1\n",
            "0x281\n",
            "0x1d1\n",
            "0x10a1\n",
            "0x16f\n",
            "0x201\n",
            "0x7f\n",
            "0x20f\n",
            "0x2f\n",
            "0x41\n",
            "0x61\n",
            "0x45f\n",
            "0x1bf\n",
            "0x1501\n",
            "0x4f\n",
            "0x7f\n",
            "0x171\n",
            "0x76f\n",
            "0x41\n",
            "0x7f\n",
            "0xa9\n",
            "0x187\n",
            "0x4ff\n",
            "0xb7f\n",
            "0x11f\n",
            "0x261\n",
            "0x257\n",
            "0x501\n",
            "0x61\n",
            "0xf7\n",
            "0xdf\n",
            "0xff\n",
            "0x37f\n",
            "0xae1\n",
            "0x221\n",
            "0x4b9\n",
            "0xff\n",
            "0x101\n",
            "0x3f\n",
            "0x51\n",
            "0xc1\n",
            "0xc1\n",
            "0x167\n",
            "0x987\n",
            "0x67f\n",
            "0xa57\n",
            "0x157\n",
            "0x2bf\n",
            "0x3f\n",
            "0x69\n",
            "0x1d1\n",
            "0x1ef\n",
            "0x159\n",
            "0xc37\n",
            "0x3f7\n",
            "0x5d1\n",
            "0x4b1\n",
            "0x3ff\n",
            "0x1f5\n",
            "0x261\n",
            "0x1f1\n",
            "0x38d\n",
            "0x41b\n",
            "0x745\n",
            "0x153\n",
            "0xbf\n",
            "0x5c9\n",
            "0x3f1\n",
            "0x211\n",
            "0x23f\n",
            "0x1a1\n",
            "0x2f9\n",
            "0x3b1\n",
            "0x801\n",
            "0x101\n",
            "0x8f\n",
            "0x327\n",
            "0x2c7\n",
            "0x1e1\n",
            "0x2d9\n",
            "0x15f\n",
            "0x41f\n",
            "0x3f1\n",
            "0x9d7\n",
            "0x161\n",
            "0xa1\n",
            "0x1ff\n",
            "0x247\n",
            "0x191\n",
            "0x32f\n",
            "0x229\n",
            "0x42f\n",
            "0x481\n",
            "0x9af\n",
            "0x15f\n",
            "0xf1\n",
            "0x32d\n",
            "0x3eb\n",
            "0x20d\n",
            "0x2cd\n",
            "0x23d\n",
            "0x387\n",
            "0x5d9\n",
            "0x681\n",
            "0x12b\n",
            "0xbf\n",
            "0x1c1\n",
            "0x219\n",
            "0x1bf\n",
            "0x2c7\n",
            "0x19f\n",
            "0x449\n",
            "0x4c9\n",
            "0xaaf\n",
            "0x13f\n",
            "0xc7\n",
            "0x31f\n",
            "0x2d1\n",
            "0x23f\n",
            "0x31f\n",
            "0x11f\n",
            "0x47f\n",
            "0x3e1\n",
            "0x9af\n",
            "0xc9\n",
            "0x9f\n",
            "0x22f\n",
            "0x2c9\n",
            "0x1c1\n",
            "0x241\n",
            "0x1e1\n",
            "0x3ef\n",
            "0x4af\n",
            "0xa21\n",
            "0x161\n",
            "0xff\n",
            "0x379\n",
            "0x38f\n",
            "0x29f\n",
            "0x37f\n",
            "0x1ff\n",
            "0x349\n",
            "0x2bf\n",
            "0x8f1\n",
            "0x149\n",
            "0x89\n",
            "0x22f\n",
            "0x3af\n",
            "0x1f1\n",
            "0x2b7\n",
            "0x1bf\n",
            "0x421\n",
            "0x59f\n",
            "0x81f\n",
            "0x121\n",
            "0xa7\n",
            "0x4af\n",
            "0x221\n",
            "0x277\n",
            "0x441\n",
            "0x38d\n",
            "0x171\n",
            "0x18d\n",
            "0x6b3\n",
            "0x333\n",
            "0x1f5\n",
            "0x46f\n",
            "0x391\n",
            "0x201\n",
            "0x35d\n",
            "0x4db\n",
            "0x1bf\n",
            "0x26b\n",
            "0x577\n",
            "0x25f\n",
            "0x1af\n",
            "0x37f\n",
            "0xfd\n",
            "0x277\n",
            "0x6b3\n",
            "0x5a1\n",
            "0x16f\n",
            "0xf5\n",
            "0x55f\n",
            "0x32f\n",
            "0x1cd\n",
            "0x2b9\n",
            "0x89\n",
            "0x1a9\n",
            "0x86f\n",
            "0x51f\n",
            "0x1a7\n",
            "0xc7\n",
            "0x5a1\n",
            "0x27f\n",
            "0x2e1\n",
            "0x327\n",
            "0xdf\n",
            "0x221\n",
            "0x52b\n",
            "0x713\n",
            "0x13f\n",
            "0xb9\n",
            "0x51f\n",
            "0x333\n",
            "0x33f\n",
            "0x3eb\n",
            "0x1b7\n",
            "0x275\n",
            "0x781\n",
            "0x687\n",
            "0x225\n",
            "0x16d\n",
            "0x425\n",
            "0xff\n",
            "0x107\n",
            "0x2e1\n",
            "0x221\n",
            "0x1e9\n",
            "0x54d\n",
            "0x6af\n",
            "0x279\n",
            "0x1e7\n",
            "0x5a1\n",
            "0x15f\n",
            "0x19d\n",
            "0x331\n",
            "0x163\n",
            "0x21f\n",
            "0x6b3\n",
            "0x621\n",
            "0x1c1\n",
            "0xff\n",
            "0x539\n",
            "0x1ff\n",
            "0x277\n",
            "0x2a7\n",
            "0x153\n",
            "0x259\n",
            "0x44b\n",
            "0x795\n",
            "0x13d\n",
            "0x95\n",
            "0x529\n",
            "0x403\n",
            "0x2c9\n",
            "0x2bf\n",
            "0x181\n",
            "0x1df\n",
            "0x49f\n",
            "0x871\n",
            "0x10f\n",
            "0xcf\n",
            "0x51f\n",
            "0x321\n",
            "0x291\n",
            "0x9ff\n",
            "0x2a7\n",
            "0x139\n",
            "0x71f\n",
            "0x259\n",
            "0x11f\n",
            "0x19f\n",
            "0x359\n",
            "0xf1\n",
            "0x197\n",
            "0x577\n",
            "0xaa1\n",
            "0x107\n",
            "0x2c9\n",
            "0x121\n",
            "0x169\n",
            "0x229\n",
            "0x481\n",
            "0xdf\n",
            "0x1b7\n",
            "0x54f\n",
            "0x23f\n",
            "0x3a9\n",
            "0x5c9\n",
            "0x1b1\n",
            "0x2eb\n",
            "0x19f\n",
            "0x55f\n",
            "0x11f\n",
            "0x231\n",
            "0x29f\n",
            "0xd1\n",
            "0x7f\n",
            "0x11a1\n",
            "0x161\n",
            "0x1b1\n",
            "0x161\n",
            "0x3bf\n",
            "0x81\n",
            "0x181\n",
            "0x4fd\n",
            "0x24f\n",
            "0xbf\n",
            "0x353\n",
            "0x69f\n",
            "0x363\n",
            "0x1c7\n",
            "0x451\n",
            "0x169\n",
            "0x301\n",
            "0x101\n",
            "0xf1\n",
            "0x7f\n",
            "0x141\n",
            "0xc1\n",
            "0x163f\n",
            "0xaf\n",
            "0x2d1\n",
            "0x1f\n",
            "0x17f\n",
            "0x24f\n",
            "0x421\n",
            "0x101\n",
            "0x2ff\n",
            "0x13f\n",
            "0x2ef\n",
            "0x59f\n",
            "0x7bf\n",
            "0x13f\n",
            "0x2a1\n",
            "0xc1\n",
            "0xdf\n",
            "0x3f\n",
            "0x201\n",
            "0x5f\n",
            "0x18f\n",
            "0x101\n",
            "0x1781\n",
            "0x3f\n",
            "0xd1\n",
            "0x171\n",
            "0x17f\n",
            "0x81\n",
            "0x131\n",
            "0x161\n",
            "0xa1\n",
            "0x17f\n",
            "0x2af\n",
            "0x12d1\n",
            "0x25f\n",
            "0x1a9\n",
            "0x22f\n",
            "0x77\n",
            "0x219\n",
            "0x271\n",
            "0x401\n",
            "0x1c7\n",
            "0x389\n",
            "0x137\n",
            "0xc69\n",
            "0x57f\n",
            "0x721\n",
            "0x82f\n",
            "0x1f7\n",
            "0x25f\n",
            "0x261\n",
            "0x177\n",
            "0x1a7\n",
            "0xc1\n",
            "0x97\n",
            "0x407\n",
            "0xb1f\n",
            "0x6f9\n",
            "0x17f\n",
            "0x1e7\n",
            "0x251\n",
            "0x17f\n",
            "0x107\n",
            "0xaf\n",
            "0xa7\n",
            "0x357\n",
            "0x541\n",
            "0xb77\n",
            "0x201\n",
            "0x201\n",
            "0x37f\n",
            "0x19f\n",
            "0x1af\n",
            "0x9f\n",
            "0x7f\n",
            "0x37f\n",
            "0x43d\n",
            "0x69d\n",
            "0x5d7\n",
            "0x3bd\n",
            "0x361\n",
            "0x18b\n",
            "0x1c5\n",
            "0xb5\n",
            "0x8d\n",
            "0x371\n",
            "0x581\n",
            "0x6b1\n",
            "0x20d\n",
            "0x5a1\n",
            "0x441\n",
            "0x1a3\n",
            "0x1d3\n",
            "0x77\n",
            "0x67\n",
            "0x31f\n",
            "0x31f\n",
            "0x4ef\n",
            "0x279\n",
            "0x1d1\n",
            "0x879\n",
            "0x211\n",
            "0x421\n",
            "0xdf\n",
            "0xe1\n",
            "0x333\n",
            "0x3e1\n",
            "0x537\n",
            "0x211\n",
            "0x1ff\n",
            "0x4d5\n",
            "0x45f\n",
            "0x32b\n",
            "0xf7\n",
            "0x249\n",
            "0x39d\n",
            "0x3a1\n",
            "0x4af\n",
            "0x213\n",
            "0x219\n",
            "0x34b\n",
            "0x2ab\n",
            "0x5b7\n",
            "0x23d\n",
            "0x1e7\n",
            "0x3df\n",
            "0x701\n",
            "0x615\n",
            "0x1d9\n",
            "0x201\n",
            "0x1a9\n",
            "0x19b\n",
            "0x291\n",
            "0x379\n",
            "0x1bf\n",
            "0x37b\n",
            "0x591\n",
            "0x421\n",
            "0x317\n",
            "0x1c1\n",
            "0x2cf\n",
            "0x2bf\n",
            "0x42b\n",
            "0x17f\n",
            "0x2b5\n",
            "0x5df\n",
            "0x5ff\n",
            "0x387\n",
            "0x7ff\n",
            "0x431\n",
            "0xdf\n",
            "0xb7\n",
            "0xe1\n",
            "0xff\n",
            "0x101\n",
            "0x481\n",
            "0x48f\n",
            "0x2b1\n",
            "0xb1f\n",
            "0x45f\n",
            "0x127\n",
            "0xd9\n",
            "0x101\n",
            "0xa1\n",
            "0x111\n",
            "0x419\n",
            "0x23f\n",
            "0x3b9\n",
            "0xd61\n",
            "0x381\n",
            "0x15f\n",
            "0xa1\n",
            "0x127\n",
            "0xb7\n",
            "0x101\n",
            "0x291\n",
            "0x189\n",
            "0x211\n",
            "0xea9\n",
            "0x4a9\n",
            "0x139\n",
            "0xa9\n",
            "0x181\n",
            "0x139\n",
            "0x1a1\n",
            "0x2f1\n",
            "0x301\n",
            "0x267\n",
            "0xc87\n",
            "0x4bf\n",
            "0x181\n",
            "0xd1\n",
            "0x191\n",
            "0x99\n",
            "0x189\n",
            "0x2bf\n",
            "0x1ff\n",
            "0x291\n",
            "0xb0f\n",
            "0x337\n",
            "0x2f7\n",
            "0x197\n",
            "0x339\n",
            "0xdf\n",
            "0x1a9\n",
            "0x2af\n",
            "0x2a7\n",
            "0x241\n",
            "0xac1\n",
            "0x407\n",
            "0x199\n",
            "0x1d9\n",
            "0x261\n",
            "0x13f\n",
            "0x24f\n",
            "0x221\n",
            "0x101\n",
            "0x25f\n",
            "0xb87\n",
            "0x28f\n",
            "0x1b9\n",
            "0x101\n",
            "0x4f9\n",
            "0x1d9\n",
            "0x2b1\n",
            "0x191\n",
            "0x12f\n",
            "0x147\n",
            "0xa49\n",
            "0x6bf\n",
            "0x23f\n",
            "0xff\n",
            "0x39f\n",
            "0x11f\n",
            "0x2bf\n",
            "0x1d1\n",
            "0x1df\n",
            "0x187\n",
            "0xb7f\n",
            "0x609\n",
            "0x15f\n",
            "0x17f\n",
            "0x27f\n",
            "0x11f\n",
            "0x2a7\n",
            "0x6c7\n",
            "0x593\n",
            "0x1e3\n",
            "0x793\n",
            "0x34b\n",
            "0xc5\n",
            "0x15d\n",
            "0x24f\n",
            "0x181\n",
            "0xe7\n",
            "0x5af\n",
            "0xccf\n",
            "0x187\n",
            "0x3d9\n",
            "0x207\n",
            "0xc7\n",
            "0x1cf\n",
            "0x1ef\n",
            "0xe1\n",
            "0x71\n",
            "0x48f\n",
            "0x301\n",
            "0x1ff\n",
            "0xa79\n",
            "0x349\n",
            "0x19f\n",
            "0x141\n",
            "0x317\n",
            "0x161\n",
            "0x101\n",
            "0x399\n",
            "0x259\n",
            "0x17f\n",
            "0xaf1\n",
            "0x33f\n",
            "0x17f\n",
            "0x17f\n",
            "0x2a1\n",
            "0x1e1\n",
            "0x2f9\n",
            "0x3c9\n",
            "0x271\n",
            "0x219\n",
            "0x8c7\n",
            "0x421\n",
            "0x189\n",
            "0x141\n",
            "0x2d9\n",
            "0x261\n",
            "0x291\n",
            "0x2b7\n",
            "0x32f\n",
            "0x16d\n",
            "0x63f\n",
            "0x381\n",
            "0x5db\n",
            "0x2c3\n",
            "0x405\n",
            "0xc1\n",
            "0x173\n",
            "0x3e1\n",
            "0x5af\n",
            "0x161\n",
            "0x645\n",
            "0x255\n",
            "0x13f\n",
            "0x313\n",
            "0x4b3\n",
            "0x1bf\n",
            "0x18b\n",
            "0x3e1\n",
            "0x3e7\n",
            "0x111\n",
            "0x4bb\n",
            "0x339\n",
            "0x101\n",
            "0x2d9\n",
            "0x5a3\n",
            "0x301\n",
            "0x2a1\n",
            "0x34b\n",
            "0x19d\n",
            "0x13f\n",
            "0x64b\n",
            "0x42b\n",
            "0xe7\n",
            "0x1bd\n",
            "0x477\n",
            "0x4c3\n",
            "0x37b\n",
            "0x35f\n",
            "0x2a1\n",
            "0x101\n",
            "0x68f\n",
            "0x411\n",
            "0x14f\n",
            "0x1ff\n",
            "0x327\n",
            "0x3b9\n",
            "0x42d\n",
            "0x91f\n",
            "0x2bf\n",
            "0x2d9\n",
            "0x171\n",
            "0x391\n",
            "0x409\n",
            "0x11f\n",
            "0x22f\n",
            "0x1a1\n",
            "0x311\n",
            "0x6ed\n",
            "0x1f3\n",
            "0x2df\n",
            "0x1c7\n",
            "0x4cf\n",
            "0x3ff\n",
            "0xd5\n",
            "0x383\n",
            "0xc7\n",
            "0x46b\n",
            "0x61d\n",
            "0x163\n",
            "0x35b\n",
            "0x23f\n",
            "0x5bf\n",
            "0x509\n",
            "0x165\n",
            "0x33b\n",
            "0x101\n",
            "0x275\n",
            "0x501\n",
            "0x1bf\n",
            "0x377\n",
            "0x22f\n",
            "0x7df\n",
            "0x521\n",
            "0xd1\n",
            "0x2b1\n",
            "0x107\n",
            "0x201\n",
            "0x5d9\n",
            "0x1b5\n",
            "0x367\n",
            "0x1b3\n",
            "0x73b\n",
            "0x4bd\n",
            "0x129\n",
            "0x2fb\n",
            "0x10f\n",
            "0x21b\n",
            "0x581\n",
            "0x177\n",
            "0x361\n",
            "0x25f\n",
            "0x4d9\n",
            "0x46d\n",
            "0x141\n",
            "0x3f1\n",
            "0x155\n",
            "0x36b\n",
            "0x60f\n",
            "0xe9\n",
            "0x24b\n",
            "0x241\n",
            "0x3c1\n",
            "0x5c9\n",
            "0x14f\n",
            "0x4ff\n",
            "0x1b1\n",
            "0x2d3\n",
            "0x551\n",
            "0x1b1\n",
            "0x1ff\n",
            "0x169\n",
            "0x4af\n",
            "0x36f\n",
            "0x16d\n",
            "0x603\n",
            "0x317\n",
            "0x2db\n",
            "0x7bf\n",
            "0x167\n",
            "0x1b9\n",
            "0x161\n",
            "0x47f\n",
            "0x37f\n",
            "0x131\n",
            "0x4c9\n",
            "0x21f\n",
            "0x3a1\n",
            "0x781\n",
            "0x209\n",
            "0x329\n",
            "0x125\n",
            "0x301\n",
            "0x38b\n",
            "0x13d\n",
            "0x4cf\n",
            "0x22b\n",
            "0x33f\n",
            "0x3c1\n",
            "0x577\n",
            "0x4a5\n",
            "0x29f\n",
            "0x4b7\n",
            "0x3ab\n",
            "0x161\n",
            "0x1b5\n",
            "0xc1\n",
            "0x32f\n",
            "0x315\n",
            "0x367\n",
            "0x449\n",
            "0x339\n",
            "0x73d\n",
            "0x3cb\n",
            "0x105\n",
            "0x1b3\n",
            "0x9d\n",
            "0x39f\n",
            "0x2f7\n",
            "0x391\n",
            "0x441\n",
            "0x3c9\n",
            "0x7d1\n",
            "0x507\n",
            "0xf9\n",
            "0x151\n",
            "0x67\n",
            "0x1cf\n",
            "0x20f\n",
            "0x329\n",
            "0x4bf\n",
            "0x2c9\n",
            "0x899\n",
            "0x4d1\n",
            "0x147\n",
            "0x251\n",
            "0x89\n",
            "0x189\n",
            "0x16f\n",
            "0x351\n",
            "0x5af\n",
            "0x47f\n",
            "0x9ef\n",
            "0x471\n",
            "0xaf\n",
            "0xc1\n",
            "0x41\n",
            "0xe1\n",
            "0x1ef\n",
            "0x257\n",
            "0x63f\n",
            "0x457\n",
            "0x951\n",
            "0x44f\n",
            "0x171\n",
            "0xdf\n",
            "0x59\n",
            "0xc9\n",
            "0x291\n",
            "0x1ff\n",
            "0x45f\n",
            "0x421\n",
            "0x837\n",
            "0x561\n",
            "0xe1\n",
            "0x141\n",
            "0x7f\n",
            "0x287\n",
            "0x2f1\n",
            "0x2ef\n",
            "0x39f\n",
            "0x31d\n",
            "0x71f\n",
            "0x4e3\n",
            "0x143\n",
            "0x18b\n",
            "0xd7\n",
            "0x3a3\n",
            "0x323\n",
            "0x1d3\n",
            "0x2f7\n",
            "0x1ab\n",
            "0x48f\n",
            "0x367\n",
            "0x259\n",
            "0x3bb\n",
            "0x17b\n",
            "0x6e5\n",
            "0x477\n",
            "0x229\n",
            "0x353\n",
            "0x1c1\n",
            "0x3df\n",
            "0x3af\n",
            "0x2f9\n",
            "0x327\n",
            "0x11d\n",
            "0x57b\n",
            "0x29f\n",
            "0x423\n",
            "0x4d1\n",
            "0x2af\n",
            "0x38d\n",
            "0x3c1\n",
            "0x17f\n",
            "0x369\n",
            "0x27f\n",
            "0x2f7\n",
            "0x207\n",
            "0x349\n",
            "0x42b\n",
            "0x36f\n",
            "0x71f\n",
            "0x49d\n",
            "0x10f\n",
            "0x259\n",
            "0x18b\n",
            "0x23f\n",
            "0x1f3\n",
            "0x2d1\n",
            "0x573\n",
            "0x363\n",
            "0x605\n",
            "0x531\n",
            "0x12d\n",
            "0x1f7\n",
            "0x1b1\n",
            "0x241\n",
            "0x23f\n",
            "0x201\n",
            "0x3b1\n",
            "0x37d\n",
            "0x5df\n",
            "0x45f\n",
            "0x17f\n",
            "0x269\n",
            "0x2ab\n",
            "0x3af\n",
            "0x219\n",
            "0x2bd\n",
            "0x40f\n",
            "0x27d\n",
            "0x683\n",
            "0x3b1\n",
            "0x18f\n",
            "0x283\n",
            "0x2ff\n",
            "0x339\n",
            "0x2b9\n",
            "0x1c7\n",
            "0x307\n",
            "0x23d\n",
            "0x3f9\n",
            "0x49f\n",
            "0x273\n",
            "0x2f9\n",
            "0x3e1\n",
            "0x451\n",
            "0x32d\n",
            "0x29f\n",
            "0x3a1\n",
            "0x27f\n",
            "0x491\n",
            "0x44f\n",
            "0x347\n",
            "0x3e1\n",
            "0x1f9\n",
            "0x205\n",
            "0x185\n",
            "0x207\n",
            "0x20b\n",
            "0x159\n",
            "0x27f\n",
            "0x2e9\n",
            "0x3cd\n",
            "0x6d3\n",
            "0x63b\n",
            "0x2a1\n",
            "0x24d\n",
            "0x249\n",
            "0x283\n",
            "0x20d\n",
            "0x3f7\n",
            "0x40f\n",
            "0x261\n",
            "0x59f\n",
            "0x3e1\n",
            "0x2cb\n",
            "0x217\n",
            "0x287\n",
            "0x217\n",
            "0x1c3\n",
            "0x3c5\n",
            "0x38d\n",
            "0x2bf\n",
            "0x46f\n",
            "0x525\n",
            "0x3c3\n",
            "0x3ab\n",
            "0x249\n",
            "0x1b1\n",
            "0x315\n",
            "0x4a9\n",
            "0x475\n",
            "0x301\n",
            "0x517\n",
            "0x1ab\n",
            "0x249\n",
            "0x399\n",
            "0x1e3\n",
            "0x177\n",
            "0x2ff\n",
            "0x4dd\n",
            "0x3bf\n",
            "0x201\n",
            "0x625\n",
            "0x1bd\n",
            "0x381\n",
            "0x401\n",
            "0x231\n",
            "0x1eb\n",
            "0x2b5\n",
            "0x429\n",
            "0x3ff\n",
            "0x1c3\n",
            "0x679\n",
            "0x181\n",
            "0x339\n",
            "0x47b\n",
            "0x20f\n",
            "0x26b\n",
            "0x32f\n",
            "0x413\n",
            "0x433\n",
            "0x201\n",
            "0x551\n",
            "0x12d\n",
            "0x2f5\n",
            "0x3d9\n",
            "0x235\n",
            "0x253\n",
            "0x2cf\n",
            "0x4c5\n",
            "0x4db\n",
            "0x271\n",
            "0x56d\n",
            "0x101\n",
            "0x23b\n",
            "0x3c1\n",
            "0x21f\n",
            "0x261\n",
            "0x2a5\n",
            "0x497\n",
            "0x545\n",
            "0x22b\n",
            "0x4ef\n",
            "0x11f\n",
            "0x2e1\n",
            "0x2f9\n",
            "0x191\n",
            "0x179\n",
            "0x237\n",
            "0x469\n",
            "0x625\n",
            "0x291\n",
            "0x701\n",
            "0x183\n",
            "0x20b\n",
            "0x2ff\n",
            "0x147\n",
            "0x141\n",
            "0x1f7\n",
            "0x361\n",
            "0x28f\n",
            "0x207\n",
            "0xa3f\n",
            "0x3c7\n",
            "0x267\n",
            "0x259\n",
            "0xef\n",
            "0x139\n",
            "0x27f\n",
            "0x439\n",
            "0x447\n",
            "0x209\n",
            "0x909\n",
            "0x351\n",
            "0x1e1\n",
            "0x3b1\n",
            "0x1a3\n",
            "0x241\n",
            "0x2ef\n",
            "0x3ad\n",
            "0x4e1\n",
            "0x2bf\n",
            "0x5b9\n",
            "0x1b1\n",
            "0x2bf\n",
            "0x581\n",
            "0x561\n",
            "0x44f\n",
            "0x411\n",
            "0x7ef\n",
            "0x239\n",
            "0x87\n",
            "0x5f\n",
            "0x39\n",
            "0x141\n",
            "0x5ff\n",
            "0x321\n",
            "0x39f\n",
            "0x5a7\n",
            "0x881\n",
            "0x321\n",
            "0x81\n",
            "0x41\n",
            "0x11\n",
            "0xff\n",
            "0x449\n",
            "0x2a7\n",
            "0x3b1\n",
            "0x5b1\n",
            "0xa01\n",
            "0x379\n",
            "0x71\n",
            "0x39\n",
            "0x11\n",
            "0x127\n",
            "0x2d1\n",
            "0x241\n",
            "0x2d1\n",
            "0x59f\n",
            "0xbef\n",
            "0x3b9\n",
            "0x97\n",
            "0x51\n",
            "0x1f\n",
            "0x1bf\n",
            "0x3c9\n",
            "0x23f\n",
            "0x339\n",
            "0x541\n",
            "0xa21\n",
            "0x427\n",
            "0xd1\n",
            "0x61\n",
            "0x2f\n",
            "0x1bf\n",
            "0x421\n",
            "0x1e1\n",
            "0x301\n",
            "0x4a1\n",
            "0x7e1\n",
            "0x581\n",
            "0xff\n",
            "0x7f\n",
            "0x3f\n",
            "0x317\n",
            "0x39f\n",
            "0x107\n",
            "0x249\n",
            "0x641\n",
            "0x781\n",
            "0x7e9\n",
            "0xc1\n",
            "0x8f\n",
            "0x39\n",
            "0x191\n",
            "0x36b\n",
            "0x1e7\n",
            "0x2c7\n",
            "0x4fb\n",
            "0x68f\n",
            "0x6ff\n",
            "0x20f\n",
            "0x169\n",
            "0xc1\n",
            "0x129\n",
            "0x401\n",
            "0x1dd\n",
            "0x205\n",
            "0x451\n",
            "0x66d\n",
            "0x701\n",
            "0x1af\n",
            "0x1c7\n",
            "0x101\n",
            "0x1c9\n",
            "0xb8f\n",
            "0x2e1\n",
            "0x199\n",
            "0x1bf\n",
            "0x3ff\n",
            "0x187\n",
            "0xff\n",
            "0xdf\n",
            "0x9f\n",
            "0x609\n",
            "0x3cf\n",
            "0x9e1\n",
            "0x2c1\n",
            "0x237\n",
            "0x421\n",
            "0x3e1\n",
            "0x1b9\n",
            "0xe1\n",
            "0x1a9\n",
            "0xf9\n",
            "0x47f\n",
            "0xe5f\n",
            "0x291\n",
            "0x237\n",
            "0x361\n",
            "0x281\n",
            "0x121\n",
            "0x47\n",
            "0x51\n",
            "0x81\n",
            "0x2cd\n",
            "0x71f\n",
            "0x775\n",
            "0x561\n",
            "0x37f\n",
            "0x359\n",
            "0x19d\n",
            "0x29\n",
            "0x4d\n",
            "0x3b\n",
            "0x1d1\n",
            "0x7e1\n",
            "0x701\n",
            "0x77f\n",
            "0x3ff\n",
            "0x251\n",
            "0xff\n",
            "0xf\n",
            "0x31\n",
            "0x21\n",
            "0x2f7\n",
            "0xb21\n",
            "0x249\n",
            "0x21f\n",
            "0x57f\n",
            "0x49f\n",
            "0x1d7\n",
            "0x59\n",
            "0xaf\n",
            "0x61\n",
            "0x51f\n",
            "0x673\n",
            "0x379\n",
            "0x2b9\n",
            "0x3d3\n",
            "0x3cd\n",
            "0x381\n",
            "0xe1\n",
            "0x15d\n",
            "0xd3\n",
            "0x581\n",
            "0x329\n",
            "0x2b7\n",
            "0x1bf\n",
            "0x1ff\n",
            "0x1df\n",
            "0x7ff\n",
            "0x249\n",
            "0x3d7\n",
            "0xd1\n",
            "0x4df\n",
            "0x25f\n",
            "0x131\n",
            "0x10f\n",
            "0x251\n",
            "0x2af\n",
            "0x7cf\n",
            "0x381\n",
            "0x509\n",
            "0xef\n",
            "0x501\n",
            "0x23f\n",
            "0x12f\n",
            "0x111\n",
            "0x24f\n",
            "0x39f\n",
            "0x477\n",
            "0x36f\n",
            "0x7c1\n",
            "0xef\n",
            "0x645\n",
            "0x581\n",
            "0x261\n",
            "0x1ff\n",
            "0x357\n",
            "0x2cd\n",
            "0x421\n",
            "0xfd\n",
            "0x175\n",
            "0x30f\n",
            "0x731\n",
            "0x259\n",
            "0x2f9\n",
            "0x2a5\n",
            "0x3eb\n",
            "0x559\n",
            "0x121\n",
            "0x1bd\n",
            "0x85\n",
            "0x427\n",
            "0x7d1\n",
            "0x289\n",
            "0x2ff\n",
            "0x2df\n",
            "0x419\n",
            "0x509\n",
            "0x79\n",
            "0x21f\n",
            "0x31\n",
            "0x3bf\n",
            "0x547\n",
            "0x1e7\n",
            "0x36b\n",
            "0x30b\n",
            "0x589\n",
            "0x6eb\n",
            "0xc7\n",
            "0x295\n",
            "0x35\n",
            "0x245\n",
            "0x47f\n",
            "0x2cf\n",
            "0x397\n",
            "0x311\n",
            "0x76b\n",
            "0x4c3\n",
            "0xcb\n",
            "0x26d\n",
            "0x43\n",
            "0x269\n",
            "0x615\n",
            "0x29f\n",
            "0x43f\n",
            "0x441\n",
            "0x515\n",
            "0x459\n",
            "0xcf\n",
            "0x1c5\n",
            "0x31\n",
            "0x285\n",
            "0x44d\n",
            "0x283\n",
            "0x327\n",
            "0x487\n",
            "0x619\n",
            "0x4e5\n",
            "0x13d\n",
            "0x269\n",
            "0x71\n",
            "0x267\n",
            "0x41d\n",
            "0x17d\n",
            "0x2c7\n",
            "0x4d5\n",
            "0x58b\n",
            "0x713\n",
            "0x7f\n",
            "0x193\n",
            "0x33\n",
            "0x3d9\n",
            "0x3bf\n",
            "0x25d\n",
            "0x35f\n",
            "0x51d\n",
            "0x669\n",
            "0x521\n",
            "0x13b\n",
            "0x1b1\n",
            "0x65\n",
            "0x281\n",
            "0x5ad\n",
            "0x231\n",
            "0x267\n",
            "0x299\n",
            "0x509\n",
            "0x53b\n",
            "0xc1\n",
            "0x32f\n",
            "0x61\n",
            "0x46d\n",
            "0x42f\n",
            "0x299\n",
            "0x1eb\n",
            "0x2c7\n",
            "0x531\n",
            "0x47f\n",
            "0xc7\n",
            "0x1e5\n",
            "0xb1\n",
            "0x769\n",
            "0x493\n",
            "0x621\n",
            "0x2c5\n",
            "0x2cb\n",
            "0x391\n",
            "0x41f\n",
            "0x135\n",
            "0xdb\n",
            "0x10f\n",
            "0x4d3\n",
            "0x5a7\n",
            "0x841\n",
            "0x27f\n",
            "0x27f\n",
            "0x401\n",
            "0x327\n",
            "0x12f\n",
            "0x15f\n",
            "0x13f\n",
            "0x1f9\n",
            "0x4ff\n",
            "0x701\n",
            "0x381\n",
            "0x345\n",
            "0x407\n",
            "0x291\n",
            "0x1ff\n",
            "0x171\n",
            "0x1db\n",
            "0x139\n",
            "0x521\n",
            "0xa97\n",
            "0x33f\n",
            "0x241\n",
            "0x3e7\n",
            "0x19f\n",
            "0x199\n",
            "0xaf\n",
            "0x239\n",
            "0xa7\n",
            "0x441\n",
            "0xa8f\n",
            "0x2b1\n",
            "0x267\n",
            "0x577\n",
            "0x251\n",
            "0x13f\n",
            "0xe7\n",
            "0x16f\n",
            "0xaf\n",
            "0x51f\n",
            "0x58d\n",
            "0x2b9\n",
            "0x2e1\n",
            "0x481\n",
            "0x3fb\n",
            "0x209\n",
            "0x197\n",
            "0x1b3\n",
            "0x1d9\n",
            "0x5a7\n",
            "0x41b\n",
            "0x27f\n",
            "0x1bf\n",
            "0x355\n",
            "0x341\n",
            "0x381\n",
            "0x2c5\n",
            "0x2f9\n",
            "0x211\n",
            "0x45f\n",
            "0x24f\n",
            "0x1b5\n",
            "0x123\n",
            "0x2df\n",
            "0x3c1\n",
            "0x493\n",
            "0x5a3\n",
            "0x3cd\n",
            "0x1d9\n",
            "0x493\n",
            "0x26b\n",
            "0x1ab\n",
            "0xf3\n",
            "0x2af\n",
            "0x2fd\n",
            "0x47b\n",
            "0x575\n",
            "0x4e1\n",
            "0x1c7\n",
            "0x605\n",
            "0x383\n",
            "0x25b\n",
            "0x203\n",
            "0x2af\n",
            "0x3cb\n",
            "0x23f\n",
            "0x251\n",
            "0x257\n",
            "0x49f\n",
            "0x683\n",
            "0x2af\n",
            "0x273\n",
            "0x3cd\n",
            "0x50d\n",
            "0x3d9\n",
            "0x26b\n",
            "0x195\n",
            "0x13f\n",
            "0x267\n",
            "0x2d1\n",
            "0x8d1\n",
            "0x281\n",
            "0x601\n",
            "0x557\n",
            "0x33f\n",
            "0xdf\n",
            "0xa1\n",
            "0x5f\n",
            "0x13f\n",
            "0x121\n",
            "0x2c1\n",
            "0x69f\n",
            "0xb5f\n",
            "0x5b9\n",
            "0x221\n",
            "0x121\n",
            "0x41\n",
            "0x51\n",
            "0x6f\n",
            "0xb1\n",
            "0x1ff\n",
            "0x3e1\n",
            "0x1301\n",
            "0x3ff\n",
            "0x16f\n",
            "0x61\n",
            "0x21\n",
            "0x41\n",
            "0x21\n",
            "0x191\n",
            "0x761\n",
            "0x277\n",
            "0x5bf\n",
            "0xa27\n",
            "0x1ff\n",
            "0x11f\n",
            "0x59\n",
            "0xa7\n",
            "0x71\n",
            "0xc1\n",
            "0xff\n",
            "0x1a1\n",
            "0x621\n",
            "0x321\n",
            "0x10bf\n",
            "0xff\n",
            "0x7f\n",
            "0x3f\n",
            "0xdf\n",
            "0x14f\n",
            "0x111\n",
            "0x1a1\n",
            "0x28f\n",
            "0x201\n",
            "0x1ef\n",
            "0x1301\n",
            "0xcf\n",
            "0xb1\n",
            "0xbf\n",
            "0x207\n",
            "0x129\n",
            "0x12f\n",
            "0x4df\n",
            "0x309\n",
            "0x491\n",
            "0x3cf\n",
            "0x83f\n",
            "0x101\n",
            "0x1e1\n",
            "0x38b\n",
            "0x3df\n",
            "0x2b7\n",
            "0x4c5\n",
            "0x3cd\n",
            "0x16d\n",
            "0x295\n",
            "0x195\n",
            "0x5dd\n",
            "0x1cf\n",
            "0x36f\n",
            "0x13f\n",
            "0x181\n",
            "0x51f\n",
            "0x289\n",
            "0x2c7\n",
            "0xd7\n",
            "0xb1\n",
            "0xbf\n",
            "0xccf\n",
            "0x571\n",
            "0x3a1\n",
            "0x28b\n",
            "0x251\n",
            "0x489\n",
            "0x28d\n",
            "0x2ad\n",
            "0x2b1\n",
            "0x263\n",
            "0x337\n",
            "0x591\n",
            "0x3b3\n",
            "0x25f\n",
            "0x2cb\n",
            "0x601\n",
            "0x247\n",
            "0x2cb\n",
            "0x217\n",
            "0x131\n",
            "0x335\n",
            "0x4bf\n",
            "0x29f\n",
            "0x3c5\n",
            "0x40b\n",
            "0x53f\n",
            "0x359\n",
            "0x241\n",
            "0x2a1\n",
            "0xf7\n",
            "0x247\n",
            "0x3c1\n",
            "0x1e9\n",
            "0x309\n",
            "0x50f\n",
            "0x487\n",
            "0x481\n",
            "0x2b5\n",
            "0x387\n",
            "0x107\n",
            "0x1e3\n",
            "0x3ff\n",
            "0x297\n",
            "0x333\n",
            "0x455\n",
            "0x5ff\n",
            "0x373\n",
            "0x2fd\n",
            "0x231\n",
            "0x139\n",
            "0x201\n",
            "0x35b\n",
            "0x161\n",
            "0x2b9\n",
            "0x3b1\n",
            "0x493\n",
            "0x76d\n",
            "0x2bb\n",
            "0x385\n",
            "0xd5\n",
            "0x1a5\n",
            "0x3b9\n",
            "0x193\n",
            "0x195\n",
            "0x1cf\n",
            "0x451\n",
            "0x399\n",
            "0x5f1\n",
            "0x37f\n",
            "0x1b5\n",
            "0x445\n",
            "0x30f\n",
            "0x1e3\n",
            "0x23b\n",
            "0x241\n",
            "0x34f\n",
            "0x401\n",
            "0x4c7\n",
            "0x5bb\n",
            "0x2bf\n",
            "0x1dd\n",
            "0x595\n",
            "0x209\n",
            "0x1bb\n",
            "0x18f\n",
            "0x4d3\n",
            "0x1b1\n",
            "0x395\n",
            "0x1c3\n",
            "0x48f\n",
            "0x487\n",
            "0x411\n",
            "0x1e7\n",
            "0x183\n",
            "0x1dd\n",
            "0x485\n",
            "0x38d\n",
            "0x301\n",
            "0x2f9\n",
            "0x2b1\n",
            "0x5e3\n",
            "0x259\n",
            "0x3a1\n",
            "0x271\n",
            "0x337\n",
            "0x447\n",
            "0x41f\n",
            "0x2a7\n",
            "0x495\n",
            "0x20f\n",
            "0x29f\n",
            "0x30f\n",
            "0x231\n",
            "0x261\n",
            "0x3c9\n",
            "0x549\n",
            "0x47f\n",
            "0x195\n",
            "0x5ff\n",
            "0x181\n",
            "0x1af\n",
            "0x34f\n",
            "0x251\n",
            "0x37d\n",
            "0x45f\n",
            "0x5bf\n",
            "0x52f\n",
            "0x12d\n",
            "0x44b\n",
            "0xbd\n",
            "0x147\n",
            "0x397\n",
            "0x25f\n",
            "0x249\n",
            "0x49d\n",
            "0x5fb\n",
            "0x501\n",
            "0x125\n",
            "0x4f1\n",
            "0xcd\n",
            "0x12d\n",
            "0x297\n",
            "0x217\n",
            "0x231\n",
            "0x401\n",
            "0x7cf\n",
            "0x4c1\n",
            "0x189\n",
            "0x4af\n",
            "0x141\n",
            "0xff\n",
            "0x223\n",
            "0x151\n",
            "0x249\n",
            "0x2b5\n",
            "0x4d1\n",
            "0x5fd\n",
            "0x31b\n",
            "0x6f9\n",
            "0x179\n",
            "0x129\n",
            "0x191\n",
            "0x109\n",
            "0x17d\n",
            "0x39b\n",
            "0x49b\n",
            "0x56d\n",
            "0x397\n",
            "0x7a3\n",
            "0x20b\n",
            "0xf1\n",
            "0x199\n",
            "0x139\n",
            "0x1e5\n",
            "0x341\n",
            "0x4ff\n",
            "0x45d\n",
            "0x3f3\n",
            "0x66d\n",
            "0x2e7\n",
            "0x153\n",
            "0x1c5\n",
            "0x153\n",
            "0x113\n",
            "0x23b\n",
            "0x505\n",
            "0x447\n",
            "0x3c7\n",
            "0x757\n",
            "0x367\n",
            "0x1a3\n",
            "0x20f\n",
            "0x20f\n",
            "0x1ad\n",
            "0x301\n",
            "0x5a5\n",
            "0x4c5\n",
            "0x247\n",
            "0x539\n",
            "0x26f\n",
            "0x2bd\n",
            "0x26f\n",
            "0x583\n",
            "0x1bd\n",
            "0x1e9\n",
            "0x687\n",
            "0x4fd\n",
            "0x2b1\n",
            "0x39f\n",
            "0x1ab\n",
            "0xe1\n",
            "0x351\n",
            "0x7b1\n",
            "0x201\n",
            "0x1ef\n",
            "0x5f1\n",
            "0x4ef\n",
            "0x1e1\n",
            "0x281\n",
            "0x101\n",
            "0xa1\n",
            "0x357\n",
            "0x41b\n",
            "0x37f\n",
            "0x27f\n",
            "0x51f\n",
            "0x5e5\n",
            "0x36f\n",
            "0x279\n",
            "0x133\n",
            "0x69\n",
            "0x325\n",
            "0x70b\n",
            "0x3dd\n",
            "0x2f7\n",
            "0x5af\n",
            "0x43b\n",
            "0x259\n",
            "0x15b\n",
            "0xf3\n",
            "0x5f\n",
            "0x345\n",
            "0x53b\n",
            "0x22f\n",
            "0x22b\n",
            "0x79f\n",
            "0x599\n",
            "0x1eb\n",
            "0x261\n",
            "0xf3\n",
            "0xad\n",
            "0x3b5\n",
            "0x439\n",
            "0x1ff\n",
            "0x1ff\n",
            "0x745\n",
            "0x54f\n",
            "0x2ef\n",
            "0x2af\n",
            "0x145\n",
            "0x9d\n",
            "0x441\n",
            "0x285\n",
            "0x225\n",
            "0x151\n",
            "0x4b1\n",
            "0x4c3\n",
            "0x441\n",
            "0x54d\n",
            "0x21b\n",
            "0x99\n",
            "0x47f\n",
            "0x321\n",
            "0x15b\n",
            "0x157\n",
            "0x639\n",
            "0x2d5\n",
            "0x21d\n",
            "0x76f\n",
            "0x251\n",
            "0x9f\n",
            "0x3d7\n",
            "0x1a7\n",
            "0x101\n",
            "0xe1\n",
            "0x301\n",
            "0x359\n",
            "0x337\n",
            "0xb51\n",
            "0x301\n",
            "0x79\n",
            "0x49f\n",
            "0x4f1\n",
            "0x1c1\n",
            "0x181\n",
            "0x52f\n",
            "0x40d\n",
            "0x2df\n",
            "0x47f\n",
            "0x181\n",
            "0x119\n",
            "0x267\n",
            "0x39f\n",
            "0x387\n",
            "0x1ff\n",
            "0x2ff\n",
            "0x821\n",
            "0x421\n",
            "0x13f\n",
            "0x3cf\n",
            "0x1\n",
            "0x403\n",
            "0x2a1\n",
            "0x247\n",
            "0x109\n",
            "0x353\n",
            "0x465\n",
            "0x51d\n",
            "0x29f\n",
            "0x689\n",
            "0x1\n",
            "0x3a7\n",
            "0x16d\n",
            "0x2e5\n",
            "0x1bb\n",
            "0x2c7\n",
            "0x793\n",
            "0x5b9\n",
            "0x1ab\n",
            "0x47f\n",
            "0x1\n",
            "0x3bf\n",
            "0x181\n",
            "0x201\n",
            "0x17f\n",
            "0x241\n",
            "0x5c1\n",
            "0x7d1\n",
            "0x1af\n",
            "0x5a1\n",
            "0x1\n",
            "0x4af\n",
            "0x281\n",
            "0x181\n",
            "0x131\n",
            "0x2ef\n",
            "0x4c1\n",
            "0x381\n",
            "0x2ef\n",
            "0x7f7\n",
            "0x1\n",
            "0x3d1\n",
            "0x161\n",
            "0x1b7\n",
            "0xc9\n",
            "0x14f\n",
            "0x579\n",
            "0x9b9\n",
            "0x179\n",
            "0x649\n",
            "0x1\n",
            "0x33f\n",
            "0x281\n",
            "0x27f\n",
            "0x107\n",
            "0x227\n",
            "0x3c1\n",
            "0xa3f\n",
            "0x239\n",
            "0x421\n",
            "0x1\n",
            "0x461\n",
            "0x267\n",
            "0x27f\n",
            "0x18d\n",
            "0x333\n",
            "0x327\n",
            "0x567\n",
            "0x2e3\n",
            "0x683\n",
            "0x1\n",
            "0x335\n",
            "0x44f\n",
            "0x2b7\n",
            "0x229\n",
            "0x41f\n",
            "0x487\n",
            "0x3b9\n",
            "0x24d\n",
            "0x4ef\n",
            "0x1\n",
            "0x411\n",
            "0x1c1\n",
            "0x1ff\n",
            "0x147\n",
            "0x241\n",
            "0x511\n",
            "0x831\n",
            "0x237\n",
            "0x4f1\n",
            "0x1\n",
            "0x44f\n",
            "0x44d\n",
            "0x347\n",
            "0x255\n",
            "0x217\n",
            "0x2bd\n",
            "0x2b5\n",
            "0x31f\n",
            "0x703\n",
            "0x1\n",
            "0x671\n",
            "0x161\n",
            "0x41f\n",
            "0x1e1\n",
            "0x1e9\n",
            "0x237\n",
            "0x1af\n",
            "0x327\n",
            "0x93f\n",
            "0x1\n",
            "0x831\n",
            "0x177\n",
            "0x38f\n",
            "0x171\n",
            "0xcf\n",
            "0x1ff\n",
            "0x149\n",
            "0x3f7\n",
            "0x93f\n",
            "0x1\n",
            "0x583\n",
            "0x141\n",
            "0x747\n",
            "0x1fd\n",
            "0xdb\n",
            "0x2ed\n",
            "0x1b1\n",
            "0x4cd\n",
            "0x599\n",
            "0x1\n",
            "0x49b\n",
            "0x185\n",
            "0x641\n",
            "0x337\n",
            "0x171\n",
            "0x43f\n",
            "0x281\n",
            "0x227\n",
            "0x5f7\n",
            "0x1\n",
            "0x4b1\n",
            "0x1ab\n",
            "0x67b\n",
            "0x241\n",
            "0xf7\n",
            "0x2f3\n",
            "0x37b\n",
            "0x415\n",
            "0x567\n",
            "0x1\n",
            "0x3af\n",
            "0x1e3\n",
            "0x441\n",
            "0x1a3\n",
            "0xef\n",
            "0x573\n",
            "0x4c5\n",
            "0x317\n",
            "0x64d\n",
            "0x1\n",
            "0x971\n",
            "0x17f\n",
            "0x429\n",
            "0x1a7\n",
            "0xaf\n",
            "0x1df\n",
            "0x161\n",
            "0x5a1\n",
            "0x591\n",
            "0x1\n",
            "0x4db\n",
            "0x36d\n",
            "0x331\n",
            "0x1df\n",
            "0x1e1\n",
            "0x2cd\n",
            "0x2f3\n",
            "0x3b1\n",
            "0x747\n",
            "0x1\n",
            "0x751\n",
            "0x191\n",
            "0x4f7\n",
            "0x167\n",
            "0xc9\n",
            "0x1df\n",
            "0x181\n",
            "0x49f\n",
            "0x7df\n",
            "0x1\n",
            "0x3f1\n",
            "0x3bd\n",
            "0x2f3\n",
            "0x253\n",
            "0x4eb\n",
            "0x2c7\n",
            "0x333\n",
            "0x3d7\n",
            "0x451\n",
            "0x1\n",
            "0x29d\n",
            "0x3d7\n",
            "0x1a1\n",
            "0x187\n",
            "0x4a5\n",
            "0x433\n",
            "0x49b\n",
            "0x2af\n",
            "0x619\n",
            "0x1\n",
            "0x3cb\n",
            "0x1cb\n",
            "0x22b\n",
            "0x16f\n",
            "0x301\n",
            "0x6d1\n",
            "0x363\n",
            "0x351\n",
            "0x623\n",
            "0x1\n",
            "0x3c5\n",
            "0x1c7\n",
            "0x28d\n",
            "0x201\n",
            "0x279\n",
            "0x6b9\n",
            "0x2dd\n",
            "0x403\n",
            "0x5cf\n",
            "0x1\n",
            "0x33f\n",
            "0x251\n",
            "0x16f\n",
            "0x10f\n",
            "0x371\n",
            "0x88f\n",
            "0x3ff\n",
            "0x26f\n",
            "0x587\n",
            "0x1\n",
            "0x4e9\n",
            "0x1dd\n",
            "0x1e1\n",
            "0x18d\n",
            "0x251\n",
            "0x501\n",
            "0x351\n",
            "0x435\n",
            "0x6d3\n",
            "0x1\n",
            "0x429\n",
            "0x2c7\n",
            "0x3b3\n",
            "0x24d\n",
            "0x255\n",
            "0x201\n",
            "0x4b1\n",
            "0x61f\n",
            "0x3ed\n",
            "0x1\n",
            "0x591\n",
            "0x281\n",
            "0x31f\n",
            "0x17f\n",
            "0x201\n",
            "0x251\n",
            "0x281\n",
            "0x4b9\n",
            "0x7b1\n",
            "0x1\n",
            "0x243\n",
            "0x4c3\n",
            "0x2f1\n",
            "0x23f\n",
            "0x55f\n",
            "0x281\n",
            "0x45f\n",
            "0x2d1\n",
            "0x4b9\n",
            "0x1\n",
            "0x665\n",
            "0x1f9\n",
            "0x28f\n",
            "0x23f\n",
            "0x211\n",
            "0x4a3\n",
            "0x233\n",
            "0x505\n",
            "0x4c1\n",
            "0x1\n",
            "0x34f\n",
            "0x35f\n",
            "0x377\n",
            "0x25f\n",
            "0x897\n",
            "0x2ff\n",
            "0x23f\n",
            "0x231\n",
            "0x381\n",
            "0x1\n",
            "0x2ff\n",
            "0x3c1\n",
            "0xef\n",
            "0x7f\n",
            "0xa29\n",
            "0x2af\n",
            "0x1cf\n",
            "0x189\n",
            "0x77f\n",
            "0x1\n",
            "0x51f\n",
            "0x29f\n",
            "0x289\n",
            "0x111\n",
            "0x7f1\n",
            "0x341\n",
            "0x121\n",
            "0x2bf\n",
            "0x581\n",
            "0x1\n",
            "0x397\n",
            "0x351\n",
            "0x2a9\n",
            "0x101\n",
            "0x849\n",
            "0x4a1\n",
            "0x14f\n",
            "0x299\n",
            "0x491\n",
            "0x1\n",
            "0x309\n",
            "0x3d9\n",
            "0x1af\n",
            "0x111\n",
            "0x95f\n",
            "0x2cf\n",
            "0x201\n",
            "0x211\n",
            "0x5f7\n",
            "0x1\n",
            "0x237\n",
            "0x149\n",
            "0xe1\n",
            "0x141\n",
            "0xa67\n",
            "0x767\n",
            "0x1c1\n",
            "0x261\n",
            "0x459\n",
            "0x1\n",
            "0x1f1\n",
            "0x44f\n",
            "0x261\n",
            "0xc1\n",
            "0x7df\n",
            "0x581\n",
            "0x2e1\n",
            "0x269\n",
            "0x3e1\n",
            "0x1\n",
            "0x4db\n",
            "0x1a5\n",
            "0x421\n",
            "0x335\n",
            "0x5b9\n",
            "0x315\n",
            "0x201\n",
            "0x493\n",
            "0x2bd\n",
            "0x1\n",
            "0x317\n",
            "0x381\n",
            "0x20f\n",
            "0x15f\n",
            "0x987\n",
            "0x2c7\n",
            "0x1e1\n",
            "0x27f\n",
            "0x53f\n",
            "0x1\n",
            "0x49b\n",
            "0x355\n",
            "0x27b\n",
            "0x1c3\n",
            "0x6bb\n",
            "0x3db\n",
            "0x3ff\n",
            "0x27b\n",
            "0x2d3\n",
            "0x1\n",
            "0x801\n",
            "0x1e7\n",
            "0x1af\n",
            "0x277\n",
            "0x571\n",
            "0x2c9\n",
            "0x411\n",
            "0x177\n",
            "0x407\n",
            "0x1\n",
            "0x851\n",
            "0x29f\n",
            "0x121\n",
            "0x27f\n",
            "0x417\n",
            "0x36f\n",
            "0x509\n",
            "0x161\n",
            "0x367\n",
            "0x1\n",
            "0x909\n",
            "0xf7\n",
            "0x1e1\n",
            "0x627\n",
            "0x351\n",
            "0x379\n",
            "0x259\n",
            "0x1a1\n",
            "0x301\n",
            "0x1\n",
            "0x889\n",
            "0xc1\n",
            "0x1d7\n",
            "0x6ff\n",
            "0x1a1\n",
            "0x46f\n",
            "0x289\n",
            "0x241\n",
            "0x2df\n",
            "0x1\n",
            "0x6c5\n",
            "0x157\n",
            "0x1b5\n",
            "0x3dd\n",
            "0x35b\n",
            "0x45d\n",
            "0x5ab\n",
            "0x1e3\n",
            "0x2f3\n",
            "0x1\n",
            "0x451\n",
            "0xc1\n",
            "0x299\n",
            "0x72f\n",
            "0x23f\n",
            "0x7c1\n",
            "0x3e1\n",
            "0x131\n",
            "0x201\n",
            "0x1\n",
            "0x261\n",
            "0x12f\n",
            "0x15f\n",
            "0x2bf\n",
            "0x32f\n",
            "0x9e7\n",
            "0x88f\n",
            "0xbf\n",
            "0x1df\n",
            "0x1\n",
            "0x1381\n",
            "0x8f\n",
            "0x17f\n",
            "0x2df\n",
            "0x101\n",
            "0x151\n",
            "0xff\n",
            "0x17f\n",
            "0x2bf\n",
            "0x1\n",
            "0x5ff\n",
            "0x2b3\n",
            "0x103\n",
            "0x18d\n",
            "0x551\n",
            "0x3b7\n",
            "0x4f5\n",
            "0x185\n",
            "0x52d\n",
            "0x1\n",
            "0xbe9\n",
            "0x17f\n",
            "0xe1\n",
            "0x159\n",
            "0x3af\n",
            "0x2e9\n",
            "0x2d1\n",
            "0x19f\n",
            "0x52f\n",
            "0x1\n",
            "0x279\n",
            "0x52f\n",
            "0x1b7\n",
            "0x157\n",
            "0x9f1\n",
            "0x501\n",
            "0x299\n",
            "0x7f\n",
            "0x321\n",
            "0x1\n",
            "0x47f\n",
            "0x9c1\n",
            "0x1f7\n",
            "0x139\n",
            "0x779\n",
            "0x1a7\n",
            "0xd1\n",
            "0x18f\n",
            "0x2f1\n",
            "0x1\n",
            "0x2af\n",
            "0xf9\n",
            "0x881\n",
            "0x3a1\n",
            "0x1ff\n",
            "0xb67\n",
            "0x9f\n",
            "0x117\n",
            "0xe9\n",
            "0x1\n",
            "0x317\n",
            "0x1a9\n",
            "0x65f\n",
            "0x321\n",
            "0x44f\n",
            "0xa2f\n",
            "0xd7\n",
            "0x11f\n",
            "0x111\n",
            "0x1\n",
            "0x381\n",
            "0x91f\n",
            "0x1cf\n",
            "0x101\n",
            "0x5ff\n",
            "0x359\n",
            "0x1ff\n",
            "0x1c1\n",
            "0x359\n",
            "0x1\n",
            "0x8f\n",
            "0x1c1\n",
            "0x1b1\n",
            "0x51\n",
            "0x321\n",
            "0x15a1\n",
            "0x15f\n",
            "0x7f\n",
            "0x81\n",
            "0x1\n",
            "0x17f\n",
            "0x419\n",
            "0x12f\n",
            "0x87\n",
            "0x67f\n",
            "0xacf\n",
            "0x499\n",
            "0xef\n",
            "0x1cf\n",
            "0x1\n",
            "0x43d\n",
            "0x359\n",
            "0x23d\n",
            "0x149\n",
            "0x701\n",
            "0x547\n",
            "0x1dd\n",
            "0x223\n",
            "0x487\n",
            "0x1\n",
            "0x3ed\n",
            "0x465\n",
            "0x1b1\n",
            "0x113\n",
            "0x69b\n",
            "0x49f\n",
            "0x2b7\n",
            "0x16b\n",
            "0x58b\n",
            "0x1\n",
            "0x2cf\n",
            "0x899\n",
            "0x137\n",
            "0xa9\n",
            "0xe01\n",
            "0xff\n",
            "0xf1\n",
            "0x81\n",
            "0x217\n",
            "0x1\n",
            "0x325\n",
            "0x58d\n",
            "0x1dd\n",
            "0x209\n",
            "0x491\n",
            "0x51f\n",
            "0x3c1\n",
            "0x13b\n",
            "0x4b1\n",
            "0x1\n",
            "0x501\n",
            "0x229\n",
            "0x1a1\n",
            "0x169\n",
            "0x29f\n",
            "0x2b1\n",
            "0x2f9\n",
            "0x141\n",
            "0xc21\n",
            "0x1\n",
            "0x587\n",
            "0x2ef\n",
            "0x1cf\n",
            "0x181\n",
            "0x21f\n",
            "0x459\n",
            "0x37f\n",
            "0xe1\n",
            "0x947\n",
            "0x1\n",
            "0x46f\n",
            "0x221\n",
            "0x1df\n",
            "0x20f\n",
            "0x21f\n",
            "0x601\n",
            "0x4cf\n",
            "0xbf\n",
            "0x7c1\n",
            "0x1\n",
            "0x5c1\n",
            "0x1b7\n",
            "0x27f\n",
            "0x1a9\n",
            "0x1c1\n",
            "0x349\n",
            "0x30f\n",
            "0x131\n",
            "0xae9\n",
            "0x1\n",
            "0x471\n",
            "0x2e1\n",
            "0x23f\n",
            "0x181\n",
            "0x281\n",
            "0x247\n",
            "0x341\n",
            "0x11f\n",
            "0xb77\n",
            "0x1\n",
            "0x4e1\n",
            "0x2a1\n",
            "0x42f\n",
            "0x23d\n",
            "0x251\n",
            "0x463\n",
            "0x2f1\n",
            "0x135\n",
            "0x729\n",
            "0x1\n",
            "0x669\n",
            "0x3ff\n",
            "0x149\n",
            "0x14f\n",
            "0x34f\n",
            "0x24f\n",
            "0x249\n",
            "0x1ff\n",
            "0x8ff\n",
            "0x1\n",
            "0x3f5\n",
            "0x401\n",
            "0x301\n",
            "0x23b\n",
            "0x359\n",
            "0x453\n",
            "0x3c1\n",
            "0x1cf\n",
            "0x585\n",
            "0x1\n",
            "0x5f1\n",
            "0x1df\n",
            "0x119\n",
            "0x19f\n",
            "0x2ef\n",
            "0x33f\n",
            "0x25f\n",
            "0x177\n",
            "0xb5f\n",
            "0x1\n",
            "0x23b\n",
            "0x737\n",
            "0x195\n",
            "0xf7\n",
            "0x69d\n",
            "0x24b\n",
            "0x24d\n",
            "0x1db\n",
            "0x6cb\n",
            "0x1\n",
            "0x2ef\n",
            "0x73f\n",
            "0x161\n",
            "0xe1\n",
            "0x89f\n",
            "0x2d1\n",
            "0x161\n",
            "0x181\n",
            "0x531\n",
            "0x1\n",
            "0x24f\n",
            "0x4cf\n",
            "0x2df\n",
            "0x189\n",
            "0x85f\n",
            "0x5ff\n",
            "0x1a1\n",
            "0x1c7\n",
            "0x2af\n",
            "0x1\n",
            "0x2df\n",
            "0x441\n",
            "0x1df\n",
            "0x14f\n",
            "0x9cf\n",
            "0x437\n",
            "0x287\n",
            "0x167\n",
            "0x39f\n",
            "0x1\n",
            "0x3f5\n",
            "0x525\n",
            "0x229\n",
            "0x17f\n",
            "0x601\n",
            "0x33f\n",
            "0x1d7\n",
            "0x19d\n",
            "0x67f\n",
            "0x1\n",
            "0x129\n",
            "0x899\n",
            "0x1f7\n",
            "0xf9\n",
            "0xbe1\n",
            "0x201\n",
            "0x231\n",
            "0xb9\n",
            "0x241\n",
            "0x1\n",
            "0x219\n",
            "0x681\n",
            "0x2f1\n",
            "0x19f\n",
            "0x8a7\n",
            "0x331\n",
            "0x271\n",
            "0xe7\n",
            "0x39f\n",
            "0x1\n",
            "0x433\n",
            "0x4f7\n",
            "0x409\n",
            "0x1e5\n",
            "0x475\n",
            "0x3ab\n",
            "0x1d3\n",
            "0x25f\n",
            "0x48f\n",
            "0x1\n",
            "0x3ff\n",
            "0x41f\n",
            "0x1ff\n",
            "0x16d\n",
            "0x4d9\n",
            "0x2bb\n",
            "0x293\n",
            "0x2bb\n",
            "0x773\n",
            "0x1\n",
            "0x1ef\n",
            "0xb21\n",
            "0xef\n",
            "0x81\n",
            "0xa57\n",
            "0x17f\n",
            "0x101\n",
            "0x9f\n",
            "0x3d1\n",
            "0x1\n",
            "0xa39\n",
            "0xd7\n",
            "0x107\n",
            "0x29f\n",
            "0x6f\n",
            "0x3c1\n",
            "0x409\n",
            "0x68f\n",
            "0x23f\n",
            "0x1\n",
            "0x47b\n",
            "0x3e9\n",
            "0x12d\n",
            "0x193\n",
            "0x1e9\n",
            "0x437\n",
            "0x303\n",
            "0x409\n",
            "0x7a3\n",
            "0x1\n",
            "0xf1\n",
            "0x3f\n",
            "0x1011\n",
            "0x381\n",
            "0x41\n",
            "0x781\n",
            "0x121\n",
            "0x13f\n",
            "0xbf\n",
            "0x1\n",
            "0x181\n",
            "0x41\n",
            "0x1c7\n",
            "0xe59\n",
            "0x2f\n",
            "0xa47\n",
            "0xd9\n",
            "0x157\n",
            "0x12f\n",
            "0x1\n",
            "0x4e9\n",
            "0x20f\n",
            "0xbf\n",
            "0x181\n",
            "0x15f\n",
            "0x60f\n",
            "0x2ff\n",
            "0x49f\n",
            "0x7bf\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x2f\n",
            "0x91\n",
            "0x1\n",
            "0x1e8f\n",
            "0x51\n",
            "0xf\n",
            "0xf\n",
            "0x1\n",
            "0x179\n",
            "0xc9\n",
            "0xb1\n",
            "0xff\n",
            "0x81\n",
            "0xb0f\n",
            "0xc97\n",
            "0x1f1\n",
            "0x1d1\n",
            "0x1\n",
            "0x4e1\n",
            "0xbf\n",
            "0xbf\n",
            "0x13f\n",
            "0x2f\n",
            "0x35f\n",
            "0x1a1\n",
            "0x107f\n",
            "0x261\n",
            "0x1\n",
            "0x455\n",
            "0x1a1\n",
            "0x22f\n",
            "0x437\n",
            "0x147\n",
            "0x5ef\n",
            "0x3dd\n",
            "0x3a1\n",
            "0x4d1\n",
            "0x1\n",
            "0x35f\n",
            "0xa17\n",
            "0x41\n",
            "0x97\n",
            "0x491\n",
            "0x1e7\n",
            "0x181\n",
            "0x261\n",
            "0x72f\n",
            "0x1\n",
            "0x4bf\n",
            "0x367\n",
            "0x32d\n",
            "0x2cd\n",
            "0x4af\n",
            "0x5f5\n",
            "0x333\n",
            "0x191\n",
            "0x271\n",
            "0x1\n",
            "0x3b1\n",
            "0x6d1\n",
            "0x199\n",
            "0x14d\n",
            "0x321\n",
            "0x4c7\n",
            "0x37d\n",
            "0x2d5\n",
            "0x43b\n",
            "0x1\n",
            "0x601\n",
            "0x2e3\n",
            "0x4e1\n",
            "0x1af\n",
            "0x1bf\n",
            "0x2ff\n",
            "0x323\n",
            "0x4ad\n",
            "0x3f1\n",
            "0x1\n",
            "0x62f\n",
            "0x337\n",
            "0x3a7\n",
            "0x303\n",
            "0x1eb\n",
            "0x387\n",
            "0x201\n",
            "0x441\n",
            "0x427\n",
            "0x1\n",
            "0x51d\n",
            "0x385\n",
            "0x16b\n",
            "0x159\n",
            "0x379\n",
            "0x2e9\n",
            "0x37b\n",
            "0x5a5\n",
            "0x51b\n",
            "0x1\n",
            "0x47f\n",
            "0x121\n",
            "0xff\n",
            "0xef\n",
            "0x13f\n",
            "0xb5f\n",
            "0x47f\n",
            "0x381\n",
            "0x3c1\n",
            "0x1\n",
            "0x339\n",
            "0x319\n",
            "0x17f\n",
            "0x287\n",
            "0x1e7\n",
            "0x5a9\n",
            "0x6db\n",
            "0x371\n",
            "0x3bf\n",
            "0x1\n",
            "0x871\n",
            "0x1f1\n",
            "0x201\n",
            "0x12f\n",
            "0x1c7\n",
            "0x4c1\n",
            "0x31f\n",
            "0x54f\n",
            "0x361\n",
            "0x1\n",
            "0x37f\n",
            "0x389\n",
            "0x377\n",
            "0x40b\n",
            "0x28d\n",
            "0x635\n",
            "0x2ff\n",
            "0x33b\n",
            "0x25b\n",
            "0x1\n",
            "0x4cb\n",
            "0x5c9\n",
            "0x2d9\n",
            "0x199\n",
            "0x38f\n",
            "0x42b\n",
            "0x29b\n",
            "0x22f\n",
            "0x471\n",
            "0x1\n",
            "0x32f\n",
            "0x599\n",
            "0x34d\n",
            "0x18b\n",
            "0x6bd\n",
            "0x2dd\n",
            "0x1d5\n",
            "0x279\n",
            "0x46f\n",
            "0x1\n",
            "0x42b\n",
            "0x46f\n",
            "0x2a9\n",
            "0xff\n",
            "0x3db\n",
            "0x5df\n",
            "0x2ff\n",
            "0x16d\n",
            "0x595\n",
            "0x1\n",
            "0x7c9\n",
            "0x23f\n",
            "0x5af\n",
            "0x21f\n",
            "0x1c9\n",
            "0x3e9\n",
            "0x24f\n",
            "0x13f\n",
            "0x4df\n",
            "0x1\n",
            "0x625\n",
            "0x24d\n",
            "0x699\n",
            "0x281\n",
            "0x1f1\n",
            "0x40b\n",
            "0x3b3\n",
            "0x179\n",
            "0x341\n",
            "0x1\n",
            "0x47f\n",
            "0x419\n",
            "0x381\n",
            "0x1a5\n",
            "0x381\n",
            "0x591\n",
            "0x391\n",
            "0x131\n",
            "0x44d\n",
            "0x1\n",
            "0x491\n",
            "0x37f\n",
            "0x253\n",
            "0x115\n",
            "0x2c5\n",
            "0x78d\n",
            "0x42b\n",
            "0x141\n",
            "0x4b1\n",
            "0x1\n",
            "0x439\n",
            "0x407\n",
            "0x1ab\n",
            "0xe5\n",
            "0x341\n",
            "0x4f1\n",
            "0x687\n",
            "0xdf\n",
            "0x571\n",
            "0x1\n",
            "0x33f\n",
            "0x331\n",
            "0x2ef\n",
            "0x20f\n",
            "0x35f\n",
            "0x8a1\n",
            "0x3af\n",
            "0x109\n",
            "0x3b7\n",
            "0x1\n",
            "0x395\n",
            "0x4ff\n",
            "0x2fb\n",
            "0x161\n",
            "0x48f\n",
            "0x2b7\n",
            "0x2d3\n",
            "0x1bb\n",
            "0x737\n",
            "0x1\n",
            "0x62b\n",
            "0x43f\n",
            "0x489\n",
            "0x2b3\n",
            "0x2ff\n",
            "0x3d3\n",
            "0x285\n",
            "0x1c5\n",
            "0x31b\n",
            "0x1\n",
            "0x201\n",
            "0x4e5\n",
            "0x19f\n",
            "0xff\n",
            "0x607\n",
            "0x4a9\n",
            "0x36b\n",
            "0x171\n",
            "0x6cb\n",
            "0x1\n",
            "0x341\n",
            "0x701\n",
            "0x137\n",
            "0x97\n",
            "0x8df\n",
            "0x1df\n",
            "0x111\n",
            "0xf7\n",
            "0x709\n",
            "0x1\n",
            "0x37f\n",
            "0x237\n",
            "0x3ff\n",
            "0x2b1\n",
            "0x3e1\n",
            "0x97f\n",
            "0x141\n",
            "0xdf\n",
            "0x3ff\n",
            "0x1\n",
            "0x2df\n",
            "0x1ff\n",
            "0x2b7\n",
            "0x21f\n",
            "0x4a9\n",
            "0xb99\n",
            "0x117\n",
            "0xc9\n",
            "0x409\n",
            "0x1\n",
            "0x2bf\n",
            "0x477\n",
            "0x12f\n",
            "0xdf\n",
            "0x861\n",
            "0x331\n",
            "0x1ff\n",
            "0x11f\n",
            "0x7ff\n",
            "0x1\n",
            "0x2d9\n",
            "0x321\n",
            "0x1d9\n",
            "0x181\n",
            "0x39f\n",
            "0x87f\n",
            "0x2df\n",
            "0x61\n",
            "0x721\n",
            "0x1\n",
            "0x35f\n",
            "0x219\n",
            "0x107\n",
            "0xe7\n",
            "0x487\n",
            "0x579\n",
            "0x37f\n",
            "0x7f\n",
            "0xa7f\n",
            "0x1\n",
            "0x32f\n",
            "0x4f1\n",
            "0x1d9\n",
            "0x181\n",
            "0x7cf\n",
            "0x371\n",
            "0x2df\n",
            "0x10f\n",
            "0x551\n",
            "0x1\n",
            "0x3c1\n",
            "0x501\n",
            "0x11f\n",
            "0xdf\n",
            "0x4f1\n",
            "0x361\n",
            "0x301\n",
            "0x1df\n",
            "0x7ff\n",
            "0x1\n",
            "0x1f7\n",
            "0x64f\n",
            "0x13f\n",
            "0xdf\n",
            "0xac1\n",
            "0x3ff\n",
            "0x1c1\n",
            "0x87\n",
            "0x4a9\n",
            "0x1\n",
            "어텐션 확률 합: tensor([[[0.9988, 0.9973, 0.9983, 0.9988, 1.0012, 1.0007, 0.9939, 0.9995,\n",
            "          0.9993, 0.9958],\n",
            "         [0.9980, 0.9917, 0.9990, 0.9966, 1.0010, 1.0010, 0.9966, 0.9978,\n",
            "          1.0007, 0.9976],\n",
            "         [0.9985, 0.9961, 0.9993, 0.9973, 1.0000, 0.9976, 0.9961, 0.9985,\n",
            "          0.9978, 0.9995],\n",
            "         [0.9971, 0.9956, 1.0000, 0.9980, 0.9966, 0.9961, 0.9966, 1.0002,\n",
            "          0.9954, 0.9946],\n",
            "         [0.9922, 0.9902, 0.9905, 0.9995, 0.9888, 0.9919, 0.9846, 0.9846,\n",
            "          0.9912, 0.9888],\n",
            "         [0.9946, 0.9980, 0.9958, 0.9895, 0.9978, 0.9958, 0.9988, 1.0007,\n",
            "          1.0002, 0.9978],\n",
            "         [0.9951, 0.9902, 0.9990, 0.9912, 0.9993, 0.9912, 0.9973, 0.9946,\n",
            "          0.9978, 0.9922],\n",
            "         [0.9983, 1.0007, 0.9995, 0.9995, 0.9961, 0.9988, 0.9961, 0.9985,\n",
            "          0.9968, 0.9973],\n",
            "         [0.9973, 0.9963, 0.9897, 0.9976, 0.9944, 0.9915, 0.9954, 0.9958,\n",
            "          0.9961, 0.9944],\n",
            "         [0.9998, 0.9990, 0.9998, 0.9990, 1.0017, 0.9993, 0.9978, 0.9939,\n",
            "          0.9988, 1.0005],\n",
            "         [0.9990, 0.9968, 0.9958, 0.9958, 0.9958, 0.9954, 0.9983, 0.9973,\n",
            "          0.9971, 0.9988],\n",
            "         [1.0012, 0.9956, 0.9944, 0.9966, 0.9949, 0.9941, 0.9963, 1.0007,\n",
            "          0.9973, 0.9922]],\n",
            "\n",
            "        [[0.9941, 0.9983, 0.9919, 0.9966, 0.9951, 0.9988, 0.9961, 0.9995,\n",
            "          0.9968, 0.9968],\n",
            "         [0.9949, 0.9932, 0.9971, 0.9978, 0.9951, 0.9995, 0.9924, 0.9917,\n",
            "          0.9980, 0.9980],\n",
            "         [0.9929, 0.9944, 0.9905, 0.9792, 0.9963, 0.9944, 0.9988, 0.9976,\n",
            "          0.9995, 0.9963],\n",
            "         [0.9995, 1.0000, 1.0000, 0.9958, 0.9885, 0.9956, 0.9958, 1.0000,\n",
            "          0.9978, 0.9973],\n",
            "         [0.9900, 0.9907, 0.9863, 0.9966, 0.9934, 0.9976, 0.9958, 0.9944,\n",
            "          0.9954, 0.9934],\n",
            "         [0.9968, 0.9939, 0.9883, 0.9863, 0.9902, 0.9780, 0.9939, 0.9932,\n",
            "          0.9949, 0.9949],\n",
            "         [0.9988, 0.9968, 0.9980, 0.9958, 0.9993, 0.9929, 0.9966, 0.9993,\n",
            "          0.9980, 0.9976],\n",
            "         [0.9978, 0.9971, 1.0007, 0.9971, 0.9978, 0.9956, 0.9966, 0.9988,\n",
            "          0.9993, 0.9958],\n",
            "         [0.9988, 0.9905, 0.9973, 0.9927, 0.9963, 0.9941, 0.9956, 0.9805,\n",
            "          1.0002, 0.9934],\n",
            "         [0.9995, 0.9905, 0.9995, 0.9961, 0.9968, 0.9963, 0.9993, 0.9973,\n",
            "          0.9956, 0.9983],\n",
            "         [1.0015, 0.9983, 0.9944, 0.9915, 0.9888, 0.9966, 0.9922, 0.9946,\n",
            "          0.9934, 0.9963],\n",
            "         [0.9985, 0.9917, 0.9895, 1.0032, 0.9941, 0.9973, 0.9954, 0.9976,\n",
            "          0.9988, 0.9995]],\n",
            "\n",
            "        [[0.9924, 0.9958, 0.9990, 0.9980, 0.9978, 0.9980, 0.9961, 0.9973,\n",
            "          0.9990, 0.9954],\n",
            "         [0.9966, 0.9993, 0.9973, 0.9946, 0.9961, 0.9978, 0.9941, 0.9968,\n",
            "          1.0002, 0.9993],\n",
            "         [0.9978, 0.9951, 0.9968, 0.9978, 0.9961, 0.9993, 0.9983, 0.9946,\n",
            "          0.9951, 0.9961],\n",
            "         [0.9966, 0.9983, 0.9980, 0.9958, 0.9973, 0.9956, 0.9971, 0.9963,\n",
            "          0.9927, 0.9993],\n",
            "         [0.9934, 0.9954, 0.9900, 0.9980, 0.9973, 0.9954, 0.9907, 1.0005,\n",
            "          0.9963, 0.9946],\n",
            "         [0.9968, 0.9924, 0.9971, 0.9963, 0.9958, 0.9988, 0.9978, 0.9929,\n",
            "          1.0005, 0.9976],\n",
            "         [0.9988, 0.9963, 0.9978, 1.0010, 0.9976, 0.9993, 0.9983, 0.9985,\n",
            "          0.9961, 0.9980],\n",
            "         [0.9968, 0.9946, 0.9963, 0.9968, 0.9985, 0.9978, 0.9968, 1.0002,\n",
            "          0.9961, 0.9968],\n",
            "         [0.9998, 0.9951, 0.9956, 0.9966, 0.9958, 0.9998, 0.9922, 0.9932,\n",
            "          0.9988, 0.9907],\n",
            "         [0.9995, 0.9998, 0.9968, 0.9980, 0.9990, 0.9961, 1.0005, 0.9956,\n",
            "          0.9954, 0.9990],\n",
            "         [0.9983, 0.9988, 0.9968, 0.9971, 0.9971, 0.9988, 0.9980, 0.9978,\n",
            "          0.9954, 0.9963],\n",
            "         [0.9990, 0.9949, 0.9990, 0.9985, 0.9998, 1.0000, 0.9983, 0.9956,\n",
            "          0.9917, 1.0007]],\n",
            "\n",
            "        [[0.9956, 0.9983, 0.9983, 0.9966, 0.9993, 0.9988, 0.9932, 0.9995,\n",
            "          1.0000, 0.9927],\n",
            "         [0.9966, 1.0010, 0.9988, 0.9971, 0.9971, 0.9993, 1.0002, 0.9963,\n",
            "          0.9983, 0.9971],\n",
            "         [1.0002, 0.9951, 0.9954, 0.9995, 1.0005, 0.9961, 1.0005, 0.9978,\n",
            "          1.0000, 0.9954],\n",
            "         [1.0015, 0.9958, 0.9976, 0.9988, 0.9954, 0.9976, 0.9978, 0.9988,\n",
            "          0.9985, 1.0022],\n",
            "         [0.9951, 0.9971, 0.9939, 0.9954, 0.9971, 0.9978, 0.9983, 0.9998,\n",
            "          0.9983, 0.9954],\n",
            "         [0.9963, 0.9963, 0.9941, 0.9929, 0.9963, 0.9829, 0.9985, 0.9976,\n",
            "          0.9998, 0.9944],\n",
            "         [0.9988, 0.9961, 0.9968, 0.9978, 0.9946, 0.9905, 0.9983, 0.9968,\n",
            "          0.9985, 0.9976],\n",
            "         [0.9954, 0.9985, 0.9995, 0.9963, 0.9988, 0.9922, 0.9993, 0.9993,\n",
            "          0.9961, 0.9932],\n",
            "         [0.9919, 0.9985, 0.9888, 0.9912, 1.0005, 0.9924, 0.9956, 0.9900,\n",
            "          0.9963, 0.9951],\n",
            "         [0.9993, 0.9958, 0.9985, 0.9976, 1.0005, 0.9978, 0.9985, 0.9973,\n",
            "          0.9963, 0.9995],\n",
            "         [0.9990, 0.9998, 0.9988, 0.9988, 0.9961, 0.9971, 0.9954, 0.9958,\n",
            "          0.9995, 0.9958],\n",
            "         [0.9956, 0.9961, 0.9968, 0.9961, 0.9985, 0.9946, 0.9966, 0.9993,\n",
            "          0.9983, 1.0027]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def run_validation(model, validation_dataloader, device):\n",
        "    \"\"\"\n",
        "    CORDIC‐Softmax가 적용된 BERT 모델로 validation을 수행하고\n",
        "    평균 정확도와 소요 시간을 출력합니다.\n",
        "    \"\"\"\n",
        "    model.eval()  # 평가 모드\n",
        "    eval_accuracy = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "            # 배치를 GPU에 올리고\n",
        "            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n",
        "\n",
        "            # 순전파: logits만 얻기\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # CPU로 내리고 numpy 변환\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "            # 배치별 정확도 계산\n",
        "            batch_acc = flat_accuracy(logits, label_ids)\n",
        "            eval_accuracy += batch_acc\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "    avg_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Accuracy: {avg_accuracy:.2f}\")\n",
        "    print(f\"Validation Time   : {format_time(time.time() - t0)}\")\n",
        "\n",
        "    return avg_accuracy\n",
        "\n",
        "# 사용 예시:\n",
        "# avg_acc = run_validation(model, validation_dataloader, device)\n",
        "avg_acc = run_validation(model, validation_dataloader, device)"
      ],
      "metadata": {
        "id": "r3GHXPFAyqKm",
        "outputId": "67bdedbb-3da2-4ebf-b398-d54c72c96e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "0x351\n",
            "0x1df\n",
            "0x411\n",
            "0x511\n",
            "0x6e1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x7a7\n",
            "0x331\n",
            "0x33f\n",
            "0x287\n",
            "0x2bf\n",
            "0x2cf\n",
            "0x9c1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x76f\n",
            "0x391\n",
            "0x391\n",
            "0x261\n",
            "0x2e1\n",
            "0x257\n",
            "0x9bf\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x6eb\n",
            "0x205\n",
            "0x1d7\n",
            "0x78b\n",
            "0x633\n",
            "0x2c7\n",
            "0x4a1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x7bf\n",
            "0x339\n",
            "0x401\n",
            "0x409\n",
            "0x4d1\n",
            "0x251\n",
            "0x5b7\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x6c3\n",
            "0x3bb\n",
            "0x441\n",
            "0x575\n",
            "0x4bf\n",
            "0x1bf\n",
            "0x543\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x709\n",
            "0x33f\n",
            "0x4df\n",
            "0x241\n",
            "0x311\n",
            "0x35f\n",
            "0x821\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x7ff\n",
            "0x409\n",
            "0x371\n",
            "0x54f\n",
            "0x39f\n",
            "0x197\n",
            "0x5f7\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x7bf\n",
            "0x3cf\n",
            "0x37f\n",
            "0x5d9\n",
            "0x3bf\n",
            "0x181\n",
            "0x5cf\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x801\n",
            "0x3d1\n",
            "0x32f\n",
            "0x619\n",
            "0x407\n",
            "0x181\n",
            "0x547\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x737\n",
            "0x337\n",
            "0x2ff\n",
            "0x1a7\n",
            "0x161\n",
            "0x3b7\n",
            "0xbc7\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xc01\n",
            "0x237\n",
            "0x33f\n",
            "0x11f\n",
            "0xd1\n",
            "0x307\n",
            "0x97f\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xb99\n",
            "0x1ff\n",
            "0x30f\n",
            "0x189\n",
            "0x141\n",
            "0x2f1\n",
            "0x981\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xbcf\n",
            "0x2d9\n",
            "0x2df\n",
            "0x219\n",
            "0x171\n",
            "0x2b7\n",
            "0x83f\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xb27\n",
            "0x2bf\n",
            "0x1f7\n",
            "0x201\n",
            "0x259\n",
            "0x2df\n",
            "0x8e1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xa99\n",
            "0x381\n",
            "0x26f\n",
            "0x2ff\n",
            "0x1e1\n",
            "0x341\n",
            "0x731\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x7bf\n",
            "0x291\n",
            "0x381\n",
            "0x159\n",
            "0x12f\n",
            "0x2df\n",
            "0xcc7\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x9a9\n",
            "0x29f\n",
            "0x379\n",
            "0x199\n",
            "0x9f\n",
            "0x241\n",
            "0xbc1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x9ff\n",
            "0x231\n",
            "0x267\n",
            "0x141\n",
            "0x99\n",
            "0x271\n",
            "0xcf1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x9a7\n",
            "0x2a1\n",
            "0x31f\n",
            "0x1d7\n",
            "0xc1\n",
            "0x261\n",
            "0xb91\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x8c9\n",
            "0x357\n",
            "0x1d9\n",
            "0x2c1\n",
            "0x7f7\n",
            "0x201\n",
            "0x1e7\n",
            "0x129\n",
            "0xcf\n",
            "0x147\n",
            "0x5df\n",
            "0x301\n",
            "0x2b9\n",
            "0x2e1\n",
            "0xbd7\n",
            "0x2ff\n",
            "0x13f\n",
            "0x71\n",
            "0x47\n",
            "0xa1\n",
            "0x4f7\n",
            "0x2b1\n",
            "0x321\n",
            "0x221\n",
            "0xf19\n",
            "0x247\n",
            "0xd9\n",
            "0x29\n",
            "0xf\n",
            "0x51\n",
            "0x59f\n",
            "0x2a7\n",
            "0x287\n",
            "0x279\n",
            "0xe39\n",
            "0x26f\n",
            "0x117\n",
            "0x41\n",
            "0x27\n",
            "0x5f\n",
            "0x68f\n",
            "0x177\n",
            "0x187\n",
            "0x30f\n",
            "0xde7\n",
            "0x249\n",
            "0x159\n",
            "0xb9\n",
            "0x77\n",
            "0xa7\n",
            "0x5cf\n",
            "0x23f\n",
            "0x22f\n",
            "0x349\n",
            "0xcbf\n",
            "0x341\n",
            "0x101\n",
            "0x6f\n",
            "0x3f\n",
            "0x7f\n",
            "0x667\n",
            "0x1b1\n",
            "0x241\n",
            "0x43f\n",
            "0xc37\n",
            "0x1b1\n",
            "0x1b7\n",
            "0x99\n",
            "0x5f\n",
            "0xaf\n",
            "0x9c1\n",
            "0x191\n",
            "0x26f\n",
            "0x2d9\n",
            "0x7a7\n",
            "0x251\n",
            "0x1d9\n",
            "0x151\n",
            "0xcf\n",
            "0x16f\n",
            "0x92f\n",
            "0x201\n",
            "0x361\n",
            "0x267\n",
            "0x6f7\n",
            "0x2f9\n",
            "0x1e1\n",
            "0x141\n",
            "0xcf\n",
            "0x11f\n",
            "0x6e7\n",
            "0x237\n",
            "0x319\n",
            "0x279\n",
            "0xbb9\n",
            "0x217\n",
            "0x111\n",
            "0xcf\n",
            "0x89\n",
            "0xdf\n",
            "0x507\n",
            "0x14f\n",
            "0x201\n",
            "0x341\n",
            "0x7cf\n",
            "0x4a1\n",
            "0x1d7\n",
            "0x2df\n",
            "0x149\n",
            "0x1e7\n",
            "0x3c9\n",
            "0x149\n",
            "0x27b\n",
            "0x48b\n",
            "0x6c1\n",
            "0x46d\n",
            "0x535\n",
            "0x191\n",
            "0x91\n",
            "0x13f\n",
            "0x46f\n",
            "0x26f\n",
            "0x385\n",
            "0x5c1\n",
            "0x449\n",
            "0x56d\n",
            "0x41f\n",
            "0xb7\n",
            "0x5d\n",
            "0xc5\n",
            "0x55b\n",
            "0x211\n",
            "0x377\n",
            "0x777\n",
            "0x435\n",
            "0x3a9\n",
            "0x261\n",
            "0x14f\n",
            "0xa3\n",
            "0x15f\n",
            "0x63f\n",
            "0xff\n",
            "0x305\n",
            "0x4bf\n",
            "0x3bd\n",
            "0x5e1\n",
            "0x349\n",
            "0x1ff\n",
            "0xa3\n",
            "0x157\n",
            "0x4bf\n",
            "0x121\n",
            "0x381\n",
            "0x68d\n",
            "0x60b\n",
            "0x4f7\n",
            "0x2c9\n",
            "0xff\n",
            "0x61\n",
            "0xd7\n",
            "0x309\n",
            "0x287\n",
            "0x221\n",
            "0xd61\n",
            "0x317\n",
            "0x1ff\n",
            "0x2df\n",
            "0xaf\n",
            "0xaf\n",
            "0x169\n",
            "0x4af\n",
            "0x161\n",
            "0xd7\n",
            "0x9a7\n",
            "0x357\n",
            "0x251\n",
            "0x4a1\n",
            "0x10f\n",
            "0xf1\n",
            "0x30f\n",
            "0x501\n",
            "0x7b\n",
            "0xaf\n",
            "0x64f\n",
            "0x473\n",
            "0x223\n",
            "0x761\n",
            "0x1e3\n",
            "0xdb\n",
            "0x2b5\n",
            "0x4f7\n",
            "0xc3\n",
            "0xe1\n",
            "0x72d\n",
            "0x39d\n",
            "0x207\n",
            "0x5fd\n",
            "0x26d\n",
            "0x14d\n",
            "0x2bf\n",
            "0x30b\n",
            "0x3ed\n",
            "0x3e7\n",
            "0x3d7\n",
            "0x727\n",
            "0x447\n",
            "0x253\n",
            "0x15b\n",
            "0x101\n",
            "0x101\n",
            "0x259\n",
            "0x4b1\n",
            "0x601\n",
            "0x437\n",
            "0x3b7\n",
            "0x415\n",
            "0x411\n",
            "0x15f\n",
            "0xef\n",
            "0x7f\n",
            "0x2df\n",
            "0x3c7\n",
            "0x641\n",
            "0x26f\n",
            "0x7c1\n",
            "0x569\n",
            "0x19f\n",
            "0xf1\n",
            "0x99\n",
            "0x4f\n",
            "0x3ff\n",
            "0x2c1\n",
            "0x3b7\n",
            "0x3e1\n",
            "0x80f\n",
            "0x409\n",
            "0x23f\n",
            "0x167\n",
            "0x117\n",
            "0xd1\n",
            "0x30f\n",
            "0x141\n",
            "0x2bf\n",
            "0x1df\n",
            "0x107f\n",
            "0x3a1\n",
            "0xe1\n",
            "0xe1\n",
            "0x81\n",
            "0x41\n",
            "0x441\n",
            "0x311\n",
            "0x471\n",
            "0x271\n",
            "0x989\n",
            "0x48f\n",
            "0x179\n",
            "0xe1\n",
            "0xa7\n",
            "0x7f\n",
            "0x1f1\n",
            "0x1bd\n",
            "0x521\n",
            "0x353\n",
            "0x38b\n",
            "0x44f\n",
            "0x675\n",
            "0x2ab\n",
            "0x1bf\n",
            "0x101\n",
            "0x1e1\n",
            "0x15f\n",
            "0x1d1\n",
            "0x17f\n",
            "0x31f\n",
            "0x381\n",
            "0x4d7\n",
            "0x831\n",
            "0x421\n",
            "0x18f\n",
            "0x205\n",
            "0x16b\n",
            "0x227\n",
            "0x1ad\n",
            "0x47f\n",
            "0x3cf\n",
            "0x4b3\n",
            "0x697\n",
            "0x3a7\n",
            "0x159\n",
            "0x1cf\n",
            "0x197\n",
            "0x241\n",
            "0x219\n",
            "0x2ef\n",
            "0x2ff\n",
            "0x7c9\n",
            "0x569\n",
            "0x2e9\n",
            "0x231\n",
            "0x77b\n",
            "0x2bf\n",
            "0x5d9\n",
            "0x20b\n",
            "0x68b\n",
            "0x609\n",
            "0x75\n",
            "0x1f\n",
            "0x4f\n",
            "0x41\n",
            "0x779\n",
            "0x611\n",
            "0x81f\n",
            "0x2a1\n",
            "0x26f\n",
            "0x41f\n",
            "0x8f\n",
            "0x11\n",
            "0x37\n",
            "0x37\n",
            "0x501\n",
            "0x4c1\n",
            "0xcdf\n",
            "0x227\n",
            "0x26f\n",
            "0x409\n",
            "0x7f\n",
            "0x1\n",
            "0x11\n",
            "0xf\n",
            "0x739\n",
            "0x331\n",
            "0x6db\n",
            "0x3d7\n",
            "0x35f\n",
            "0x511\n",
            "0x17d\n",
            "0x2f\n",
            "0x55\n",
            "0x51\n",
            "0x539\n",
            "0x237\n",
            "0x5b1\n",
            "0x23f\n",
            "0xabf\n",
            "0x57f\n",
            "0x3f\n",
            "0x1\n",
            "0x9\n",
            "0xf\n",
            "0x66f\n",
            "0x301\n",
            "0x73f\n",
            "0x2e1\n",
            "0x337\n",
            "0x7bf\n",
            "0xdf\n",
            "0x11\n",
            "0x31\n",
            "0x37\n",
            "0x777\n",
            "0x411\n",
            "0x44f\n",
            "0x261\n",
            "0x13d\n",
            "0x32f\n",
            "0x62d\n",
            "0xc5\n",
            "0x153\n",
            "0x101\n",
            "0x891\n",
            "0x227\n",
            "0x247\n",
            "0x139\n",
            "0xef\n",
            "0x261\n",
            "0x6c1\n",
            "0x1b1\n",
            "0x399\n",
            "0x231\n",
            "0x9af\n",
            "0x271\n",
            "0x327\n",
            "0x117\n",
            "0xd7\n",
            "0x301\n",
            "0x4ef\n",
            "0x101\n",
            "0x3b1\n",
            "0x1ff\n",
            "0xa07\n",
            "0x309\n",
            "0x36f\n",
            "0x129\n",
            "0x17f\n",
            "0x3b1\n",
            "0x3f7\n",
            "0xc1\n",
            "0x28f\n",
            "0x1b1\n",
            "0x949\n",
            "0x2c1\n",
            "0x1b9\n",
            "0x381\n",
            "0x601\n",
            "0x16f\n",
            "0x261\n",
            "0x24f\n",
            "0x12f\n",
            "0x161\n",
            "0x441\n",
            "0x351\n",
            "0xb9\n",
            "0x431\n",
            "0x201\n",
            "0x57\n",
            "0xd11\n",
            "0x189\n",
            "0xff\n",
            "0x119\n",
            "0x4c1\n",
            "0x3e1\n",
            "0x127\n",
            "0x461\n",
            "0x2af\n",
            "0xaf\n",
            "0xb11\n",
            "0x151\n",
            "0x101\n",
            "0xe7\n",
            "0x7d1\n",
            "0x3d1\n",
            "0x109\n",
            "0x4ef\n",
            "0x38f\n",
            "0xd1\n",
            "0x701\n",
            "0xff\n",
            "0xaf\n",
            "0x157\n",
            "0x601\n",
            "0x2ff\n",
            "0x121\n",
            "0x681\n",
            "0x7bf\n",
            "0x1f9\n",
            "0x2df\n",
            "0x11f\n",
            "0xdf\n",
            "0xc1\n",
            "0x61b\n",
            "0x337\n",
            "0x1ed\n",
            "0x579\n",
            "0x59f\n",
            "0x1b1\n",
            "0x465\n",
            "0x17d\n",
            "0x10d\n",
            "0xff\n",
            "0x3e7\n",
            "0x271\n",
            "0x87\n",
            "0x4b7\n",
            "0x101\n",
            "0x91\n",
            "0xddf\n",
            "0x169\n",
            "0x149\n",
            "0x207\n",
            "0x6bf\n",
            "0x139\n",
            "0x39\n",
            "0x26f\n",
            "0x8f\n",
            "0x8f\n",
            "0x9f1\n",
            "0x331\n",
            "0x287\n",
            "0x481\n",
            "0x73f\n",
            "0x18f\n",
            "0x49\n",
            "0x239\n",
            "0xaf\n",
            "0xb7\n",
            "0x821\n",
            "0x32f\n",
            "0x337\n",
            "0x489\n",
            "0x8bf\n",
            "0x181\n",
            "0x41\n",
            "0x231\n",
            "0x7f\n",
            "0x9f\n",
            "0x77f\n",
            "0x28f\n",
            "0x26f\n",
            "0x581\n",
            "0xc6f\n",
            "0x23f\n",
            "0x127\n",
            "0x1ff\n",
            "0x387\n",
            "0x179\n",
            "0x2d7\n",
            "0x239\n",
            "0x1af\n",
            "0x27f\n",
            "0xb1f\n",
            "0x361\n",
            "0x1c7\n",
            "0x419\n",
            "0x2f7\n",
            "0x71\n",
            "0x581\n",
            "0x71\n",
            "0x7f\n",
            "0x181\n",
            "0x90f\n",
            "0x37f\n",
            "0x641\n",
            "0x271\n",
            "0x2d1\n",
            "0x1a7\n",
            "0x3d1\n",
            "0x87\n",
            "0xb1\n",
            "0x13f\n",
            "0x8ef\n",
            "0x3e1\n",
            "0x4f1\n",
            "0x3e1\n",
            "0x349\n",
            "0x267\n",
            "0x2f7\n",
            "0x51\n",
            "0x7f\n",
            "0xd7\n",
            "0xb37\n",
            "0x2bf\n",
            "0x301\n",
            "0x23f\n",
            "0x759\n",
            "0x2e1\n",
            "0x12f\n",
            "0x5f\n",
            "0x47\n",
            "0xa7\n",
            "0x8e9\n",
            "0x279\n",
            "0x487\n",
            "0x2c1\n",
            "0x471\n",
            "0x541\n",
            "0x201\n",
            "0x61\n",
            "0x61\n",
            "0x97\n",
            "0x689\n",
            "0x1d9\n",
            "0x111\n",
            "0x1b9\n",
            "0xb9\n",
            "0xe1\n",
            "0x87f\n",
            "0x31f\n",
            "0x357\n",
            "0x417\n",
            "0x66b\n",
            "0x6b\n",
            "0x55\n",
            "0xcf\n",
            "0xdb\n",
            "0x109\n",
            "0x35b\n",
            "0x70f\n",
            "0x771\n",
            "0x42b\n",
            "0x5e7\n",
            "0x99\n",
            "0x7f\n",
            "0xef\n",
            "0xef\n",
            "0xdf\n",
            "0x39f\n",
            "0x5a1\n",
            "0x86f\n",
            "0x491\n",
            "0x5d9\n",
            "0xcb\n",
            "0x69\n",
            "0x101\n",
            "0xb7\n",
            "0xc1\n",
            "0x3ff\n",
            "0x509\n",
            "0x783\n",
            "0x5e5\n",
            "0x7f9\n",
            "0x171\n",
            "0xb9\n",
            "0x391\n",
            "0x4a7\n",
            "0x191\n",
            "0x2bf\n",
            "0x381\n",
            "0x30f\n",
            "0x2bf\n",
            "0x801\n",
            "0x19f\n",
            "0xf7\n",
            "0x339\n",
            "0x457\n",
            "0x201\n",
            "0x46f\n",
            "0x2c1\n",
            "0x22f\n",
            "0x237\n",
            "0x8df\n",
            "0x211\n",
            "0x161\n",
            "0x381\n",
            "0x47f\n",
            "0x251\n",
            "0x4b1\n",
            "0x1e1\n",
            "0x17f\n",
            "0x137\n",
            "0x7ff\n",
            "0x21f\n",
            "0x151\n",
            "0x37f\n",
            "0x5a9\n",
            "0x26f\n",
            "0x3a1\n",
            "0x1f1\n",
            "0x1a1\n",
            "0x1b1\n",
            "0x785\n",
            "0x191\n",
            "0xdf\n",
            "0x28d\n",
            "0x633\n",
            "0x11d\n",
            "0x349\n",
            "0x393\n",
            "0x21b\n",
            "0x32b\n",
            "0x94f\n",
            "0x13f\n",
            "0xb7\n",
            "0x36f\n",
            "0x47f\n",
            "0x1f1\n",
            "0x3d7\n",
            "0x2bf\n",
            "0x1a9\n",
            "0x289\n",
            "0xa4f\n",
            "0x147\n",
            "0x97\n",
            "0x381\n",
            "0x4bf\n",
            "0x1b7\n",
            "0x307\n",
            "0x289\n",
            "0x1e1\n",
            "0x241\n",
            "0xeef\n",
            "0x159\n",
            "0x29\n",
            "0x2ef\n",
            "0x44f\n",
            "0xa1\n",
            "0x1af\n",
            "0x237\n",
            "0x107\n",
            "0x297\n",
            "0xf17\n",
            "0x16f\n",
            "0x21\n",
            "0x271\n",
            "0x429\n",
            "0xa1\n",
            "0x237\n",
            "0x217\n",
            "0x139\n",
            "0x227\n",
            "0xe29\n",
            "0x13f\n",
            "0x31\n",
            "0x231\n",
            "0x237\n",
            "0x71\n",
            "0x29f\n",
            "0x297\n",
            "0x1a7\n",
            "0x499\n",
            "0x709\n",
            "0x1a7\n",
            "0xb1\n",
            "0x251\n",
            "0x5ef\n",
            "0x33f\n",
            "0x661\n",
            "0x129\n",
            "0x175\n",
            "0x201\n",
            "0x5b7\n",
            "0x189\n",
            "0x17f\n",
            "0x237\n",
            "0x5a7\n",
            "0x3e1\n",
            "0x7f1\n",
            "0xe7\n",
            "0x179\n",
            "0x111\n",
            "0x6cd\n",
            "0x1c7\n",
            "0x20d\n",
            "0x205\n",
            "0x671\n",
            "0x4c9\n",
            "0x5e5\n",
            "0xa1\n",
            "0xdb\n",
            "0x93\n",
            "0x589\n",
            "0x219\n",
            "0x18f\n",
            "0x23f\n",
            "0x571\n",
            "0x481\n",
            "0x723\n",
            "0x121\n",
            "0x11d\n",
            "0x135\n",
            "0x4ff\n",
            "0x225\n",
            "0x12f\n",
            "0x3d9\n",
            "0x4fd\n",
            "0x529\n",
            "0x633\n",
            "0xe9\n",
            "0x13d\n",
            "0x141\n",
            "0x45f\n",
            "0x179\n",
            "0x15f\n",
            "0x26f\n",
            "0x81f\n",
            "0x749\n",
            "0x481\n",
            "0xc1\n",
            "0xe1\n",
            "0xc1\n",
            "0x63f\n",
            "0x14f\n",
            "0x141\n",
            "0x147\n",
            "0x5e1\n",
            "0x381\n",
            "0x86f\n",
            "0x141\n",
            "0x169\n",
            "0x159\n",
            "0x83f\n",
            "0x1c1\n",
            "0xff\n",
            "0xef\n",
            "0x69f\n",
            "0x3a1\n",
            "0x539\n",
            "0x1c1\n",
            "0x17f\n",
            "0x13f\n",
            "0x76d\n",
            "0x1ed\n",
            "0xfd\n",
            "0xfd\n",
            "0x5e7\n",
            "0x489\n",
            "0x6f3\n",
            "0xe9\n",
            "0x14d\n",
            "0xfd\n",
            "0x8bf\n",
            "0x1c1\n",
            "0x101\n",
            "0x141\n",
            "0x4c7\n",
            "0x341\n",
            "0x5b9\n",
            "0x17f\n",
            "0x1df\n",
            "0x207\n",
            "0x1101\n",
            "0x341\n",
            "0x10f\n",
            "0x13f\n",
            "0x2f1\n",
            "0x91\n",
            "0x281\n",
            "0xcf\n",
            "0x51\n",
            "0x1e1\n",
            "0x6cf\n",
            "0xb09\n",
            "0x201\n",
            "0x181\n",
            "0xa7\n",
            "0x41\n",
            "0x4ff\n",
            "0xd9\n",
            "0x5f\n",
            "0x369\n",
            "0x5f9\n",
            "0x5f1\n",
            "0x941\n",
            "0x269\n",
            "0xd9\n",
            "0x7f\n",
            "0x3bf\n",
            "0x121\n",
            "0x87\n",
            "0x189\n",
            "0x595\n",
            "0x353\n",
            "0x139\n",
            "0x73f\n",
            "0x2b7\n",
            "0xc7\n",
            "0x5d1\n",
            "0x117\n",
            "0x91\n",
            "0x3a5\n",
            "0x51f\n",
            "0x17f\n",
            "0x6f\n",
            "0x281\n",
            "0x1241\n",
            "0xcf\n",
            "0x1a1\n",
            "0x7f\n",
            "0x11\n",
            "0xff\n",
            "0x761\n",
            "0x23f\n",
            "0xc1\n",
            "0x2cf\n",
            "0x53f\n",
            "0x901\n",
            "0x181\n",
            "0xc1\n",
            "0xb7\n",
            "0x157\n",
            "0x201\n",
            "0x18f\n",
            "0x7f\n",
            "0xdf\n",
            "0x51\n",
            "0x1f\n",
            "0x12cf\n",
            "0x20f\n",
            "0xef\n",
            "0x4b1\n",
            "0x429\n",
            "0x169\n",
            "0x6f\n",
            "0xc1\n",
            "0x6f\n",
            "0x27\n",
            "0x4e1\n",
            "0xb4f\n",
            "0x217\n",
            "0x63f\n",
            "0x63d\n",
            "0x335\n",
            "0xdb\n",
            "0x1cb\n",
            "0xbd\n",
            "0x59\n",
            "0x523\n",
            "0x407\n",
            "0x299\n",
            "0x6ef\n",
            "0x5b7\n",
            "0x37f\n",
            "0xb1\n",
            "0x1b1\n",
            "0x79\n",
            "0x19\n",
            "0x7c9\n",
            "0x26f\n",
            "0xc9\n",
            "0x8a1\n",
            "0x8c1\n",
            "0x15f\n",
            "0xbf\n",
            "0x1d1\n",
            "0x4ff\n",
            "0x14f\n",
            "0x351\n",
            "0x279\n",
            "0x3cf\n",
            "0x361\n",
            "0x9f1\n",
            "0x51f\n",
            "0x1af\n",
            "0x1ff\n",
            "0x43f\n",
            "0x101\n",
            "0x417\n",
            "0xa9\n",
            "0x159\n",
            "0x1c9\n",
            "0xac1\n",
            "0x281\n",
            "0x281\n",
            "0x19f\n",
            "0x527\n",
            "0x149\n",
            "0x341\n",
            "0x141\n",
            "0x1a1\n",
            "0x1c1\n",
            "0x75b\n",
            "0x20b\n",
            "0x1ff\n",
            "0x36d\n",
            "0x4cf\n",
            "0x2c3\n",
            "0x413\n",
            "0xff\n",
            "0x1ff\n",
            "0x27f\n",
            "0x5bf\n",
            "0x20f\n",
            "0x12f\n",
            "0x2df\n",
            "0x7bf\n",
            "0x27f\n",
            "0x489\n",
            "0x14f\n",
            "0x1c1\n",
            "0x22f\n",
            "0x5a1\n",
            "0x115\n",
            "0xcf\n",
            "0x2bd\n",
            "0x6f5\n",
            "0x4b9\n",
            "0x303\n",
            "0x19f\n",
            "0x2ed\n",
            "0x267\n",
            "0x487\n",
            "0x13f\n",
            "0xab\n",
            "0x1ab\n",
            "0x39b\n",
            "0x27d\n",
            "0x689\n",
            "0x297\n",
            "0x4e9\n",
            "0x39f\n",
            "0x481\n",
            "0xcf\n",
            "0x41\n",
            "0x121\n",
            "0x297\n",
            "0x209\n",
            "0x28f\n",
            "0x561\n",
            "0x7f9\n",
            "0x4a1\n",
            "0x471\n",
            "0xd9\n",
            "0x5f\n",
            "0x121\n",
            "0x26f\n",
            "0x241\n",
            "0x301\n",
            "0x3bf\n",
            "0x94f\n",
            "0x441\n",
            "0x583\n",
            "0x12f\n",
            "0x9b\n",
            "0x1c7\n",
            "0x25f\n",
            "0x1c1\n",
            "0x407\n",
            "0x33d\n",
            "0x711\n",
            "0x45d\n",
            "0x619\n",
            "0x4d5\n",
            "0x23f\n",
            "0x209\n",
            "0x637\n",
            "0x743\n",
            "0x11b\n",
            "0xc1\n",
            "0x97\n",
            "0xc1\n",
            "0x595\n",
            "0x5c1\n",
            "0x32b\n",
            "0x24f\n",
            "0x775\n",
            "0x51b\n",
            "0x1b1\n",
            "0x49\n",
            "0x3b\n",
            "0x49\n",
            "0x499\n",
            "0x409\n",
            "0x423\n",
            "0x2a1\n",
            "0x67f\n",
            "0x77f\n",
            "0x1fd\n",
            "0x2f\n",
            "0x31\n",
            "0x23\n",
            "0x50f\n",
            "0x251\n",
            "0x17f\n",
            "0x371\n",
            "0xa11\n",
            "0x5f7\n",
            "0x1ff\n",
            "0x8f\n",
            "0x7f\n",
            "0x4f\n",
            "0x537\n",
            "0x281\n",
            "0x279\n",
            "0x201\n",
            "0x759\n",
            "0x987\n",
            "0x171\n",
            "0x81\n",
            "0x91\n",
            "0x4f\n",
            "0x4ef\n",
            "0x2bf\n",
            "0x22f\n",
            "0x301\n",
            "0x6c7\n",
            "0xa31\n",
            "0x17f\n",
            "0x31\n",
            "0x31\n",
            "0x1f\n",
            "0x519\n",
            "0x1ff\n",
            "0x127\n",
            "0x26f\n",
            "0x7bf\n",
            "0x5b7\n",
            "0x53f\n",
            "0xf1\n",
            "0xef\n",
            "0xc1\n",
            "0x7a1\n",
            "0x1dd\n",
            "0xf5\n",
            "0x13b\n",
            "0x601\n",
            "0x5d9\n",
            "0x2f3\n",
            "0x1c3\n",
            "0x1f3\n",
            "0x1b7\n",
            "0x7b1\n",
            "0x1a7\n",
            "0xaf\n",
            "0x16f\n",
            "0x61f\n",
            "0x3ef\n",
            "0x301\n",
            "0x24f\n",
            "0x2d1\n",
            "0x241\n",
            "0x90f\n",
            "0x277\n",
            "0x117\n",
            "0x11f\n",
            "0x67f\n",
            "0x357\n",
            "0x29f\n",
            "0x1bf\n",
            "0x1c1\n",
            "0x231\n",
            "0x531\n",
            "0x271\n",
            "0x181\n",
            "0x167\n",
            "0xa81\n",
            "0x2b7\n",
            "0x16f\n",
            "0x347\n",
            "0x179\n",
            "0x1e7\n",
            "0x68f\n",
            "0x281\n",
            "0x201\n",
            "0xff\n",
            "0xae1\n",
            "0x1a7\n",
            "0x271\n",
            "0x201\n",
            "0x149\n",
            "0x161\n",
            "0x7cf\n",
            "0x1bf\n",
            "0x2af\n",
            "0xff\n",
            "0xa21\n",
            "0x227\n",
            "0x251\n",
            "0x1a9\n",
            "0x141\n",
            "0x129\n",
            "0xa61\n",
            "0x131\n",
            "0x159\n",
            "0xa1\n",
            "0xc0f\n",
            "0x247\n",
            "0x10f\n",
            "0x127\n",
            "0xe1\n",
            "0xdf\n",
            "0x69f\n",
            "0xef\n",
            "0xef\n",
            "0x7f\n",
            "0x11ef\n",
            "0x26f\n",
            "0x81\n",
            "0xe1\n",
            "0x7f\n",
            "0xbf\n",
            "0x9ff\n",
            "0x17f\n",
            "0x1d1\n",
            "0xe1\n",
            "0x8af\n",
            "0x33f\n",
            "0x157\n",
            "0x221\n",
            "0x119\n",
            "0x129\n",
            "0x6c1\n",
            "0x1b1\n",
            "0x19f\n",
            "0xa1\n",
            "0x417\n",
            "0x101\n",
            "0xaff\n",
            "0x209\n",
            "0x191\n",
            "0x19f\n",
            "0x5b7\n",
            "0xb7\n",
            "0xef\n",
            "0x79\n",
            "0x269\n",
            "0x9f\n",
            "0xeb7\n",
            "0x2c1\n",
            "0x171\n",
            "0x221\n",
            "0x41f\n",
            "0x101\n",
            "0x141\n",
            "0x81\n",
            "0x1ef\n",
            "0x7f\n",
            "0x1221\n",
            "0x141\n",
            "0x13f\n",
            "0x18f\n",
            "0x5e1\n",
            "0x111\n",
            "0x117\n",
            "0x97\n",
            "0x377\n",
            "0xe9\n",
            "0xc87\n",
            "0x20f\n",
            "0x1c1\n",
            "0x257\n",
            "0x241\n",
            "0x25f\n",
            "0x25f\n",
            "0x17f\n",
            "0xaf\n",
            "0x81\n",
            "0x1f\n",
            "0x7f\n",
            "0x13ff\n",
            "0x121\n",
            "0x11f\n",
            "0x11f\n",
            "0x1e1\n",
            "0xaf\n",
            "0x51\n",
            "0x3f\n",
            "0x11\n",
            "0x7f\n",
            "0x19c1\n",
            "0x51\n",
            "0xaf\n",
            "0xaf\n",
            "0x131\n",
            "0x51\n",
            "0x11\n",
            "0x1\n",
            "0x1\n",
            "0x11\n",
            "0x1c3f\n",
            "0x21\n",
            "0x161\n",
            "0x151\n",
            "0x1bf\n",
            "0x18f\n",
            "0xe1\n",
            "0xbf\n",
            "0x51\n",
            "0xaf\n",
            "0x15c1\n",
            "0xd1\n",
            "0x181\n",
            "0x33f\n",
            "0x3c1\n",
            "0x1ff\n",
            "0x15f\n",
            "0x15f\n",
            "0x8f\n",
            "0xff\n",
            "0x1001\n",
            "0xe1\n",
            "0x111\n",
            "0x141\n",
            "0x111\n",
            "0xff\n",
            "0xff\n",
            "0xff\n",
            "0x9f\n",
            "0xcf\n",
            "0x173f\n",
            "0x81\n",
            "0xf1\n",
            "0xcf\n",
            "0x91\n",
            "0x91\n",
            "0x9f\n",
            "0x81\n",
            "0x51\n",
            "0x9f\n",
            "0x1a71\n",
            "0x7f\n",
            "0xb1\n",
            "0xaf\n",
            "0xf1\n",
            "0x5f\n",
            "0x3f\n",
            "0x1f\n",
            "0x1\n",
            "0x4f\n",
            "0x1b3f\n",
            "0x1f\n",
            "0x7f\n",
            "0x51\n",
            "0x6f\n",
            "0x81\n",
            "0x2f\n",
            "0x11\n",
            "0x1\n",
            "0x21\n",
            "0x1c81\n",
            "0x81\n",
            "0x26f\n",
            "0x191\n",
            "0x1af\n",
            "0x2ff\n",
            "0x18f\n",
            "0x161\n",
            "0xcf\n",
            "0xcf\n",
            "0x108f\n",
            "0x201\n",
            "0x2a7\n",
            "0x6c3\n",
            "0x5cb\n",
            "0x2a7\n",
            "0x93\n",
            "0xbd\n",
            "0x8f\n",
            "0x29d\n",
            "0x7a1\n",
            "0x1e7\n",
            "0x1f1\n",
            "0x709\n",
            "0x49f\n",
            "0x20f\n",
            "0x37\n",
            "0x67\n",
            "0x5f\n",
            "0x367\n",
            "0xb2f\n",
            "0x81\n",
            "0x1ff\n",
            "0x6ff\n",
            "0x3f7\n",
            "0x1c7\n",
            "0x47\n",
            "0x59\n",
            "0x41\n",
            "0x2ef\n",
            "0xd07\n",
            "0x71\n",
            "0x1e1\n",
            "0x3ef\n",
            "0x261\n",
            "0x201\n",
            "0x41\n",
            "0x3f\n",
            "0x21\n",
            "0x1c1\n",
            "0x1211\n",
            "0xaf\n",
            "0x18f\n",
            "0x2c1\n",
            "0x27f\n",
            "0x1f1\n",
            "0x61\n",
            "0x41\n",
            "0x1f\n",
            "0x151\n",
            "0x1401\n",
            "0x9f\n",
            "0xff\n",
            "0x1af\n",
            "0xaf\n",
            "0xc1\n",
            "0x21\n",
            "0x2f\n",
            "0x1f\n",
            "0x14f\n",
            "0x1931\n",
            "0x81\n",
            "0xbf\n",
            "0x18f\n",
            "0x81\n",
            "0x101\n",
            "0x51\n",
            "0x71\n",
            "0x41\n",
            "0x151\n",
            "0x183f\n",
            "0xef\n",
            "0x21f\n",
            "0x33f\n",
            "0x181\n",
            "0x281\n",
            "0xd1\n",
            "0x151\n",
            "0xf1\n",
            "0x1c1\n",
            "0xfc1\n",
            "0x1e1\n",
            "0x4f\n",
            "0x4f\n",
            "0xf\n",
            "0x11\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0xf\n",
            "0x1dd1\n",
            "0x61\n",
            "0x1d1\n",
            "0x261\n",
            "0xb9\n",
            "0x307\n",
            "0xb9\n",
            "0x101\n",
            "0x107\n",
            "0x217\n",
            "0xf41\n",
            "0x3b9\n",
            "0xbf\n",
            "0x8df\n",
            "0x199\n",
            "0x217\n",
            "0x23f\n",
            "0x4ff\n",
            "0x27f\n",
            "0x47f\n",
            "0xc1\n",
            "0x3b9\n",
            "0x81\n",
            "0x9ff\n",
            "0x161\n",
            "0x121\n",
            "0x157\n",
            "0x411\n",
            "0x29f\n",
            "0x479\n",
            "0x4af\n",
            "0x1a1\n",
            "0xd1\n",
            "0x6cf\n",
            "0x1c1\n",
            "0x131\n",
            "0x10f\n",
            "0x229\n",
            "0x189\n",
            "0x341\n",
            "0xad7\n",
            "0x267\n",
            "0x81\n",
            "0x8cf\n",
            "0x19f\n",
            "0x157\n",
            "0x12f\n",
            "0x41f\n",
            "0x30f\n",
            "0x3e7\n",
            "0x3a7\n",
            "0x3a1\n",
            "0xe1\n",
            "0x5f9\n",
            "0x17d\n",
            "0x143\n",
            "0x173\n",
            "0x661\n",
            "0x52b\n",
            "0x569\n",
            "0x1c1\n",
            "0x233\n",
            "0xc5\n",
            "0x44d\n",
            "0xef\n",
            "0xed\n",
            "0xff\n",
            "0x62b\n",
            "0x6e9\n",
            "0x5b5\n",
            "0x345\n",
            "0x1fb\n",
            "0x127\n",
            "0x2c5\n",
            "0x9f\n",
            "0x10f\n",
            "0x147\n",
            "0x693\n",
            "0x791\n",
            "0x4a3\n",
            "0x2e7\n",
            "0x351\n",
            "0xef\n",
            "0x535\n",
            "0x11f\n",
            "0x111\n",
            "0x143\n",
            "0x46b\n",
            "0x381\n",
            "0x661\n",
            "0x48f\n",
            "0x375\n",
            "0xa1\n",
            "0x81\n",
            "0x7f\n",
            "0x51\n",
            "0x3f\n",
            "0x5f\n",
            "0x3f\n",
            "0x8f\n",
            "0x1b1f\n",
            "0x101\n",
            "0xb7\n",
            "0x56f\n",
            "0x10f\n",
            "0xa1\n",
            "0x91\n",
            "0x2c1\n",
            "0x33f\n",
            "0x537\n",
            "0x371\n",
            "0x8d1\n",
            "0x199\n",
            "0x17f\n",
            "0xff\n",
            "0x36f\n",
            "0x21f\n",
            "0x35f\n",
            "0x117\n",
            "0x1ff\n",
            "0xe31\n",
            "0x159\n",
            "0xa1\n",
            "0x81\n",
            "0x6f\n",
            "0x7f\n",
            "0x8f\n",
            "0x8f\n",
            "0x61\n",
            "0xaf\n",
            "0x1ae1\n",
            "0x1f\n",
            "0x71\n",
            "0x5f\n",
            "0x71\n",
            "0x4f\n",
            "0x51\n",
            "0x3f\n",
            "0x31\n",
            "0x81\n",
            "0x1cc1\n",
            "0x11\n",
            "0xb1\n",
            "0x4f\n",
            "0x3f\n",
            "0xcf\n",
            "0x91\n",
            "0xb1\n",
            "0x6f\n",
            "0xb1\n",
            "0x1acf\n",
            "0x5f\n",
            "0x11f\n",
            "0xc1\n",
            "0x7f\n",
            "0x14f\n",
            "0x14f\n",
            "0x1c1\n",
            "0x101\n",
            "0x101\n",
            "0x167f\n",
            "0x91\n",
            "0xaf\n",
            "0x5f\n",
            "0x41\n",
            "0xbf\n",
            "0x11f\n",
            "0x201\n",
            "0xcf\n",
            "0xd1\n",
            "0x182f\n",
            "0x31\n",
            "0xc1\n",
            "0xe1\n",
            "0x7f\n",
            "0xdf\n",
            "0x13f\n",
            "0x20f\n",
            "0x121\n",
            "0xff\n",
            "0x1741\n",
            "0x4f\n",
            "0x6f\n",
            "0x31\n",
            "0x3f\n",
            "0x81\n",
            "0x91\n",
            "0xbf\n",
            "0x6f\n",
            "0xa1\n",
            "0x1b91\n",
            "0x1f\n",
            "0x7f\n",
            "0x1f\n",
            "0xf\n",
            "0x2f\n",
            "0x2f\n",
            "0x1f\n",
            "0xf\n",
            "0x2f\n",
            "0x1d9f\n",
            "0x61\n",
            "0x16f\n",
            "0xc1\n",
            "0xcf\n",
            "0x1df\n",
            "0x19f\n",
            "0x211\n",
            "0x17f\n",
            "0x15f\n",
            "0x13cf\n",
            "0xdf\n",
            "0x3a1\n",
            "0x2c1\n",
            "0xff\n",
            "0x1ff\n",
            "0xff\n",
            "0x81\n",
            "0x51\n",
            "0x61\n",
            "0x11a1\n",
            "0x25f\n",
            "0x25f\n",
            "0x601\n",
            "0x28f\n",
            "0x141\n",
            "0x7f\n",
            "0x71\n",
            "0xc1\n",
            "0xd1\n",
            "0x1061\n",
            "0x91\n",
            "0x201\n",
            "0x42f\n",
            "0x32f\n",
            "0xc1\n",
            "0x7f\n",
            "0x41\n",
            "0x5f\n",
            "0xef\n",
            "0x130f\n",
            "0x5f\n",
            "0x1f1\n",
            "0xf1\n",
            "0x3f\n",
            "0xff\n",
            "0x3f\n",
            "0x31\n",
            "0x7f\n",
            "0x3f\n",
            "0x17bf\n",
            "0x2a1\n",
            "0x18f\n",
            "0xc1\n",
            "0x1\n",
            "0xbf\n",
            "0x21\n",
            "0xf\n",
            "0x6f\n",
            "0x11\n",
            "0x187f\n",
            "0x2cf\n",
            "0x61\n",
            "0x21\n",
            "0x1\n",
            "0x4f\n",
            "0x1\n",
            "0x11\n",
            "0xbf\n",
            "0x1\n",
            "0x1c01\n",
            "0x14f\n",
            "0x8f\n",
            "0x3f\n",
            "0x1\n",
            "0x7f\n",
            "0x1f\n",
            "0x41\n",
            "0x1ef\n",
            "0x1f\n",
            "0x19d1\n",
            "0x1a1\n",
            "0xff\n",
            "0x91\n",
            "0x3f\n",
            "0xc1\n",
            "0x5f\n",
            "0x7f\n",
            "0x121\n",
            "0x131\n",
            "0x17c1\n",
            "0x1ef\n",
            "0x3f\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1f01\n",
            "0x21\n",
            "0x91\n",
            "0x1\n",
            "0x1\n",
            "0x2f\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1c6f\n",
            "0x20f\n",
            "0x2c1\n",
            "0x5ef\n",
            "0x81f\n",
            "0x28f\n",
            "0xef\n",
            "0x159\n",
            "0xc1\n",
            "0x151\n",
            "0x639\n",
            "0x1f1\n",
            "0x111\n",
            "0x301\n",
            "0x23f\n",
            "0x81\n",
            "0x41\n",
            "0x41\n",
            "0x41\n",
            "0xc1\n",
            "0x16bf\n",
            "0x81\n",
            "0x149\n",
            "0x437\n",
            "0x64f\n",
            "0xf9\n",
            "0x71\n",
            "0x8f\n",
            "0x71\n",
            "0x201\n",
            "0xf09\n",
            "0x8f\n",
            "0x149\n",
            "0x141\n",
            "0x12f\n",
            "0x437\n",
            "0x221\n",
            "0x297\n",
            "0x1ff\n",
            "0x121\n",
            "0xe29\n",
            "0x201\n",
            "0x181\n",
            "0x101\n",
            "0xc1\n",
            "0x201\n",
            "0x181\n",
            "0x171\n",
            "0xff\n",
            "0x91\n",
            "0x14c1\n",
            "0x13f\n",
            "0x141\n",
            "0x7f\n",
            "0x61\n",
            "0x271\n",
            "0x17f\n",
            "0x28f\n",
            "0x2f1\n",
            "0x16f\n",
            "0x1001\n",
            "0x2bf\n",
            "0x117\n",
            "0xc1\n",
            "0xd9\n",
            "0x29f\n",
            "0x107\n",
            "0x1e7\n",
            "0x647\n",
            "0x239\n",
            "0xb41\n",
            "0x3e1\n",
            "0x101\n",
            "0x169\n",
            "0x111\n",
            "0x1ff\n",
            "0xc1\n",
            "0x21f\n",
            "0x31f\n",
            "0x6ff\n",
            "0xbdf\n",
            "0x17f\n",
            "0xcf\n",
            "0x1f\n",
            "0x31\n",
            "0x51\n",
            "0x51\n",
            "0x1f\n",
            "0x11\n",
            "0x61\n",
            "0x1c01\n",
            "0xd1\n",
            "0x121\n",
            "0xa1\n",
            "0x81\n",
            "0x201\n",
            "0x101\n",
            "0x19f\n",
            "0x181\n",
            "0x15f\n",
            "0x10a1\n",
            "0x511\n",
            "0x2c1\n",
            "0xff\n",
            "0x7f\n",
            "0x151\n",
            "0x13f\n",
            "0x9f\n",
            "0xff\n",
            "0x61\n",
            "0x1581\n",
            "0x1b1\n",
            "0xdf\n",
            "0x3f\n",
            "0x1\n",
            "0x4f\n",
            "0x51\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1d11\n",
            "0x4f\n",
            "0x111\n",
            "0x71\n",
            "0x1\n",
            "0xb1\n",
            "0x8f\n",
            "0xf\n",
            "0x1\n",
            "0x1\n",
            "0x1bc1\n",
            "0xaf\n",
            "0x2f1\n",
            "0x7f\n",
            "0x1f\n",
            "0x13f\n",
            "0xd1\n",
            "0x31\n",
            "0x3f\n",
            "0x1\n",
            "0x17ff\n",
            "0x17f\n",
            "0x46f\n",
            "0x101\n",
            "0x41\n",
            "0x22f\n",
            "0x1e1\n",
            "0xbf\n",
            "0xff\n",
            "0x21\n",
            "0x111f\n",
            "0x2ff\n",
            "0x19f\n",
            "0x3f\n",
            "0x1\n",
            "0x71\n",
            "0xd1\n",
            "0x2f\n",
            "0x6f\n",
            "0x1\n",
            "0x1a7f\n",
            "0xe1\n",
            "0x121\n",
            "0x21\n",
            "0x1\n",
            "0x51\n",
            "0xbf\n",
            "0x1f\n",
            "0x4f\n",
            "0x1\n",
            "0x1ba1\n",
            "0x9f\n",
            "0x161\n",
            "0x4f\n",
            "0x1\n",
            "0x6f\n",
            "0x81\n",
            "0x31\n",
            "0x41\n",
            "0xf\n",
            "0x1bff\n",
            "0xa1\n",
            "0x91\n",
            "0x51\n",
            "0x1f\n",
            "0xcf\n",
            "0x4f\n",
            "0x21\n",
            "0xf\n",
            "0x11\n",
            "0x1c4f\n",
            "0xcf\n",
            "0x3f1\n",
            "0x81\n",
            "0x31\n",
            "0x1c1\n",
            "0x16f\n",
            "0x41\n",
            "0x7f\n",
            "0xf\n",
            "0x1501\n",
            "0x23f\n",
            "0x51\n",
            "0x71\n",
            "0x8f\n",
            "0x9f\n",
            "0x2f\n",
            "0xf\n",
            "0xf\n",
            "0x21\n",
            "0x1c81\n",
            "0x51\n",
            "0xa1\n",
            "0xa1\n",
            "0x151\n",
            "0xc1\n",
            "0x71\n",
            "0x1f\n",
            "0x1f\n",
            "0x51\n",
            "0x1aaf\n",
            "0x11\n",
            "0x1af\n",
            "0x11f\n",
            "0x2ff\n",
            "0x1cf\n",
            "0x8f\n",
            "0x41\n",
            "0x3f\n",
            "0x81\n",
            "0x1601\n",
            "0x61\n",
            "0x71\n",
            "0x51\n",
            "0x9f\n",
            "0x10f\n",
            "0x4f\n",
            "0x31\n",
            "0x3f\n",
            "0x71\n",
            "0x1b4f\n",
            "0x2f\n",
            "0x61\n",
            "0x7f\n",
            "0x81\n",
            "0xef\n",
            "0x51\n",
            "0x31\n",
            "0x41\n",
            "0x61\n",
            "0x1bb1\n",
            "0x2f\n",
            "0x1f\n",
            "0x11\n",
            "0x1f\n",
            "0x5f\n",
            "0x1f\n",
            "0x1f\n",
            "0x2f\n",
            "0x41\n",
            "0x1e11\n",
            "0xf\n",
            "0x11\n",
            "0x11\n",
            "0x1f\n",
            "0x6f\n",
            "0x21\n",
            "0x21\n",
            "0x51\n",
            "0x81\n",
            "0x1d0f\n",
            "0x1\n",
            "0x2f\n",
            "0x21\n",
            "0x5f\n",
            "0x41\n",
            "0x1f\n",
            "0x11\n",
            "0x2f\n",
            "0x9f\n",
            "0x1d91\n",
            "0x11\n",
            "0x6f\n",
            "0x31\n",
            "0x81\n",
            "0x3f\n",
            "0xf\n",
            "0x1\n",
            "0x1\n",
            "0x31\n",
            "0x1d51\n",
            "0x21\n",
            "0x61\n",
            "0x31\n",
            "0x61\n",
            "0xdf\n",
            "0x2f\n",
            "0x3f\n",
            "0x3f\n",
            "0x6f\n",
            "0x1ba1\n",
            "0x51\n",
            "0x3ef\n",
            "0x391\n",
            "0x2e1\n",
            "0x1d1\n",
            "0xc1\n",
            "0x29f\n",
            "0x3af\n",
            "0x1ef\n",
            "0x1ef\n",
            "0x8b9\n",
            "0x1af\n",
            "0xd41\n",
            "0x2f1\n",
            "0x251\n",
            "0x6f\n",
            "0xdf\n",
            "0x181\n",
            "0xaf\n",
            "0x441\n",
            "0x3d9\n",
            "0x159\n",
            "0x561\n",
            "0xd81\n",
            "0x271\n",
            "0x31\n",
            "0x107\n",
            "0xe1\n",
            "0x137\n",
            "0x351\n",
            "0x269\n",
            "0xbf\n",
            "0x20f\n",
            "0xa1\n",
            "0xb0f\n",
            "0xe1\n",
            "0x2e1\n",
            "0x29f\n",
            "0xd9\n",
            "0xb7\n",
            "0x95f\n",
            "0xf1\n",
            "0x1f7\n",
            "0x39\n",
            "0x40f\n",
            "0x441\n",
            "0x289\n",
            "0x2f9\n",
            "0xb9\n",
            "0x25f\n",
            "0xbb9\n",
            "0x9f\n",
            "0x117\n",
            "0x41\n",
            "0x53f\n",
            "0x111\n",
            "0x817\n",
            "0x591\n",
            "0xc1\n",
            "0x159\n",
            "0x7b9\n",
            "0x7f\n",
            "0x13f\n",
            "0x3f\n",
            "0x291\n",
            "0xdf\n",
            "0x42f\n",
            "0x108f\n",
            "0xaf\n",
            "0x81\n",
            "0x421\n",
            "0xa7\n",
            "0x189\n",
            "0x19f\n",
            "0x1e9\n",
            "0xaf\n",
            "0x21f\n",
            "0x3ff\n",
            "0x997\n",
            "0x48f\n",
            "0x52f\n",
            "0x2f\n",
            "0x21\n",
            "0x3f\n",
            "0xdf\n",
            "0x4f\n",
            "0x2f\n",
            "0xf\n",
            "0x1f\n",
            "0x1ba1\n",
            "0x11f\n",
            "0x41\n",
            "0x71\n",
            "0xf\n",
            "0x1f1\n",
            "0x7f\n",
            "0x10f\n",
            "0x13f\n",
            "0x41\n",
            "0x19f\n",
            "0x17ff\n",
            "0x501\n",
            "0x1b9\n",
            "0x187\n",
            "0x4ef\n",
            "0x65f\n",
            "0x441\n",
            "0x1c9\n",
            "0x169\n",
            "0x1ed\n",
            "0x2ff\n",
            "0x20b\n",
            "0x381\n",
            "0x17f\n",
            "0x371\n",
            "0x4d5\n",
            "0x513\n",
            "0x2d9\n",
            "0x17f\n",
            "0x4e1\n",
            "0x241\n",
            "0x21f\n",
            "0x2a9\n",
            "0x249\n",
            "0x2df\n",
            "0x2f1\n",
            "0x43f\n",
            "0x2a1\n",
            "0x181\n",
            "0x921\n",
            "0x181\n",
            "0x2af\n",
            "0x11b\n",
            "0x7f\n",
            "0x3d9\n",
            "0x753\n",
            "0x549\n",
            "0x27f\n",
            "0xed\n",
            "0x605\n",
            "0x1af\n",
            "0x309\n",
            "0x119\n",
            "0x9f\n",
            "0x3a7\n",
            "0x9e7\n",
            "0x469\n",
            "0x241\n",
            "0x109\n",
            "0x3ff\n",
            "0x1e7\n",
            "0x261\n",
            "0xef\n",
            "0x7f\n",
            "0x359\n",
            "0x4b1\n",
            "0x61f\n",
            "0x3b7\n",
            "0x139\n",
            "0x7bf\n",
            "0x167\n",
            "0x1d7\n",
            "0xbf\n",
            "0x5f\n",
            "0x221\n",
            "0x2ff\n",
            "0x661\n",
            "0x6a9\n",
            "0x13f\n",
            "0x7ff\n",
            "0x18f\n",
            "0x26f\n",
            "0xef\n",
            "0x7d\n",
            "0x39b\n",
            "0x337\n",
            "0x52d\n",
            "0x3a9\n",
            "0x33f\n",
            "0x6a3\n",
            "0x283\n",
            "0xbf\n",
            "0xc1\n",
            "0x51\n",
            "0x81\n",
            "0x4f\n",
            "0xc1\n",
            "0x61\n",
            "0x7f\n",
            "0x1b2f\n",
            "0x91\n",
            "0x22d\n",
            "0xe3\n",
            "0x87\n",
            "0x487\n",
            "0x33b\n",
            "0x667\n",
            "0x2f7\n",
            "0x221\n",
            "0x701\n",
            "0x1ff\n",
            "0x1a1\n",
            "0x34f\n",
            "0x411\n",
            "0x27f\n",
            "0xb1\n",
            "0xbf\n",
            "0xc1\n",
            "0xd1\n",
            "0xf6f\n",
            "0x1e1\n",
            "0x81\n",
            "0x101\n",
            "0x10f\n",
            "0x2f\n",
            "0x21\n",
            "0x31\n",
            "0x8f\n",
            "0x41\n",
            "0x1adf\n",
            "0x2f\n",
            "0x51\n",
            "0xf1\n",
            "0x17f\n",
            "0x1f\n",
            "0x1\n",
            "0x11\n",
            "0x4f\n",
            "0x31\n",
            "0x1b51\n",
            "0x1f\n",
            "0x23f\n",
            "0x261\n",
            "0x2a1\n",
            "0x201\n",
            "0x101\n",
            "0xf1\n",
            "0xdf\n",
            "0xff\n",
            "0x11bf\n",
            "0x121\n",
            "0x1af\n",
            "0x1cf\n",
            "0x111\n",
            "0x181\n",
            "0x1a1\n",
            "0x101\n",
            "0xff\n",
            "0xcf\n",
            "0x1421\n",
            "0x141\n",
            "0x131\n",
            "0xef\n",
            "0xc1\n",
            "0xaf\n",
            "0xdf\n",
            "0xa1\n",
            "0x131\n",
            "0xbf\n",
            "0x1821\n",
            "0xbf\n",
            "0xb1\n",
            "0x7f\n",
            "0x61\n",
            "0x51\n",
            "0x81\n",
            "0x71\n",
            "0xf1\n",
            "0x7f\n",
            "0x1a91\n",
            "0x81\n",
            "0x13f\n",
            "0x111\n",
            "0x12f\n",
            "0x71\n",
            "0x4f\n",
            "0x5f\n",
            "0x7f\n",
            "0x8f\n",
            "0x192f\n",
            "0x7f\n",
            "0x3f\n",
            "0x11\n",
            "0x11\n",
            "0x21\n",
            "0x1f\n",
            "0x11\n",
            "0x11\n",
            "0xf\n",
            "0x1e8f\n",
            "0x5f\n",
            "0x22f\n",
            "0x17f\n",
            "0x151\n",
            "0x151\n",
            "0xf1\n",
            "0xbf\n",
            "0xff\n",
            "0xaf\n",
            "0x12d1\n",
            "0x39f\n",
            "0x1a1\n",
            "0x38f\n",
            "0x4df\n",
            "0x181\n",
            "0x41\n",
            "0x9f\n",
            "0x61\n",
            "0xdf\n",
            "0x115f\n",
            "0xa1\n",
            "0x191\n",
            "0x131\n",
            "0x241\n",
            "0x5f\n",
            "0x1f\n",
            "0x11\n",
            "0x21\n",
            "0x8f\n",
            "0x1901\n",
            "0x21\n",
            "0x13f\n",
            "0x101\n",
            "0x47f\n",
            "0x7f\n",
            "0x21\n",
            "0x21\n",
            "0x21\n",
            "0xc1\n",
            "0x1741\n",
            "0x3f\n",
            "0x71\n",
            "0x51\n",
            "0x5f\n",
            "0x51\n",
            "0x21\n",
            "0x3f\n",
            "0x1f\n",
            "0x3f\n",
            "0x1ca1\n",
            "0x4f\n",
            "0x7f\n",
            "0x41\n",
            "0x41\n",
            "0x2f\n",
            "0x1f\n",
            "0x21\n",
            "0x11\n",
            "0x3f\n",
            "0x1cef\n",
            "0x1f\n",
            "0x21\n",
            "0x21\n",
            "0xf\n",
            "0x11\n",
            "0x11\n",
            "0x2f\n",
            "0x21\n",
            "0x2f\n",
            "0x1e3f\n",
            "0x1f\n",
            "0x11\n",
            "0x1f\n",
            "0x1\n",
            "0x11\n",
            "0x11\n",
            "0x21\n",
            "0x4f\n",
            "0x31\n",
            "0x1e71\n",
            "0x11\n",
            "0x81\n",
            "0x6f\n",
            "0x9f\n",
            "0x71\n",
            "0x4f\n",
            "0x6f\n",
            "0x91\n",
            "0x121\n",
            "0x1b2f\n",
            "0x61\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1\n",
            "0x1ebf\n",
            "0x1\n",
            "0x61\n",
            "0x3f\n",
            "0x31\n",
            "0xb1\n",
            "0x41\n",
            "0xa1\n",
            "0x61\n",
            "0x41\n",
            "0x1b4f\n",
            "0xcf\n",
            "0x5e9\n",
            "0x8f9\n",
            "0x1a1\n",
            "0x2c7\n",
            "0x1a1\n",
            "0x589\n",
            "0x2b1\n",
            "0x97\n",
            "0xaf\n",
            "0x171\n",
            "0x347\n",
            "0x831\n",
            "0x181\n",
            "0x2d1\n",
            "0x241\n",
            "0x771\n",
            "0x35f\n",
            "0xdf\n",
            "0x9f\n",
            "0x181\n",
            "0x37f\n",
            "0x97f\n",
            "0x1e1\n",
            "0x357\n",
            "0x1d1\n",
            "0x6b9\n",
            "0x267\n",
            "0x89\n",
            "0x6f\n",
            "0x1d7\n",
            "0x35f\n",
            "0x6a7\n",
            "0x15f\n",
            "0x289\n",
            "0x22f\n",
            "0x86f\n",
            "0x447\n",
            "0xdf\n",
            "0x81\n",
            "0x1c1\n",
            "0x269\n",
            "0x581\n",
            "0xef\n",
            "0x221\n",
            "0x1d7\n",
            "0x901\n",
            "0x681\n",
            "0x11f\n",
            "0x101\n",
            "0x161\n",
            "0x2d1\n",
            "0x88f\n",
            "0x227\n",
            "0x271\n",
            "0x101\n",
            "0x8ff\n",
            "0x34f\n",
            "0x101\n",
            "0x89\n",
            "0xf7\n",
            "0x2bf\n",
            "0x53f\n",
            "0xff\n",
            "0x14f\n",
            "0xd1\n",
            "0xa61\n",
            "0x6bf\n",
            "0x171\n",
            "0x137\n",
            "0x101\n",
            "0x281\n",
            "0x77f\n",
            "0x141\n",
            "0x131\n",
            "0xc7\n",
            "0x967\n",
            "0x69f\n",
            "0x12f\n",
            "0xb9\n",
            "0xc9\n",
            "0x289\n",
            "0x541\n",
            "0xff\n",
            "0x117\n",
            "0xc1\n",
            "0xa57\n",
            "0x791\n",
            "0x149\n",
            "0x101\n",
            "0xff\n",
            "0x33f\n",
            "0x697\n",
            "0x19f\n",
            "0x247\n",
            "0x181\n",
            "0x8b1\n",
            "0x529\n",
            "0xf1\n",
            "0xb1\n",
            "0x12f\n",
            "0x515\n",
            "0x577\n",
            "0x21f\n",
            "0x487\n",
            "0x3d7\n",
            "0x50b\n",
            "0x17d\n",
            "0x81\n",
            "0x75\n",
            "0x35b\n",
            "0x33d\n",
            "0x6b3\n",
            "0x2a3\n",
            "0x50f\n",
            "0x453\n",
            "0x4f3\n",
            "0xe7\n",
            "0x5b\n",
            "0x61\n",
            "0x369\n",
            "0x441\n",
            "0x93f\n",
            "0x2ff\n",
            "0x501\n",
            "0x2df\n",
            "0x2ef\n",
            "0xbf\n",
            "0x51\n",
            "0x7f\n",
            "0x32f\n",
            "0x431\n",
            "0x731\n",
            "0x3af\n",
            "0x4cf\n",
            "0x427\n",
            "0x399\n",
            "0x75\n",
            "0x23\n",
            "0x3f\n",
            "0x37f\n",
            "0x3b1\n",
            "0x831\n",
            "0x1ef\n",
            "0x501\n",
            "0x417\n",
            "0x3df\n",
            "0x141\n",
            "0x3f\n",
            "0x59\n",
            "0x339\n",
            "0x3b9\n",
            "0x829\n",
            "0x19f\n",
            "0x429\n",
            "0x3e1\n",
            "0x321\n",
            "0x19f\n",
            "0x67\n",
            "0xb1\n",
            "0x467\n",
            "0x239\n",
            "0x4a1\n",
            "0x1e1\n",
            "0x339\n",
            "0x8bf\n",
            "0x381\n",
            "0x1ef\n",
            "0xa1\n",
            "0x181\n",
            "0x3b9\n",
            "0x297\n",
            "0x70b\n",
            "0x2c9\n",
            "0x357\n",
            "0x5ad\n",
            "0x3a3\n",
            "0x1f5\n",
            "0x63\n",
            "0xa3\n",
            "0x3ed\n",
            "0x22f\n",
            "0x377\n",
            "0x189\n",
            "0x2e7\n",
            "0x7e1\n",
            "0x481\n",
            "0x4c7\n",
            "0xa1\n",
            "0xc1\n",
            "0x341\n",
            "0x361\n",
            "0x499\n",
            "0x2b5\n",
            "0x5e3\n",
            "0x4d3\n",
            "0x3cb\n",
            "0x2ef\n",
            "0x7f\n",
            "0x53\n",
            "0x2ff\n",
            "0x1c1\n",
            "0x597\n",
            "0x4e9\n",
            "0x467\n",
            "0x415\n",
            "0x301\n",
            "0x4e5\n",
            "0xc9\n",
            "0xa9\n",
            "0x1ef\n",
            "0x17f\n",
            "0x6e7\n",
            "0x677\n",
            "0x453\n",
            "0x469\n",
            "0x333\n",
            "0x1ff\n",
            "0x8d\n",
            "0xad\n",
            "0x201\n",
            "0x177\n",
            "0x52f\n",
            "0x75f\n",
            "0x513\n",
            "0x481\n",
            "0x207\n",
            "0x3b3\n",
            "0x79\n",
            "0x63\n",
            "0x19f\n",
            "0x167\n",
            "0x5e9\n",
            "0x521\n",
            "0x41b\n",
            "0x57f\n",
            "0x301\n",
            "0x36d\n",
            "0xc9\n",
            "0xf1\n",
            "0x1c1\n",
            "0x141\n",
            "0x3ff\n",
            "0x43f\n",
            "0x391\n",
            "0x439\n",
            "0x35f\n",
            "0x84f\n",
            "0xc1\n",
            "0xc1\n",
            "0x171\n",
            "0x197\n",
            "0x41d\n",
            "0x36f\n",
            "0x341\n",
            "0x3d9\n",
            "0x4ad\n",
            "0x51f\n",
            "0x1e3\n",
            "0x1e3\n",
            "0x227\n",
            "0xa1\n",
            "0x181\n",
            "0x141\n",
            "0x17f\n",
            "0x25f\n",
            "0x27f\n",
            "0x108f\n",
            "0x241\n",
            "0x17f\n",
            "0x17f\n",
            "0x131\n",
            "0x3ff\n",
            "0x419\n",
            "0x281\n",
            "0x2f1\n",
            "0x30f\n",
            "0x909\n",
            "0x1f9\n",
            "0x13f\n",
            "0x1cf\n",
            "0x181\n",
            "0x553\n",
            "0x4bf\n",
            "0x2ff\n",
            "0x333\n",
            "0x29b\n",
            "0x6b1\n",
            "0x163\n",
            "0x13d\n",
            "0x23f\n",
            "0xfb\n",
            "0x6c5\n",
            "0x607\n",
            "0x37f\n",
            "0x485\n",
            "0x251\n",
            "0x3df\n",
            "0xd5\n",
            "0xb5\n",
            "0x25d\n",
            "0x27b\n",
            "0x415\n",
            "0x1e1\n",
            "0x423\n",
            "0x2a9\n",
            "0x71f\n",
            "0x39b\n",
            "0xf3\n",
            "0x135\n",
            "0x3bb\n",
            "0x261\n",
            "0x4a7\n",
            "0x1d3\n",
            "0x415\n",
            "0x37f\n",
            "0x329\n",
            "0x631\n",
            "0x109\n",
            "0x141\n",
            "0x3c1\n",
            "0x22f\n",
            "0x29d\n",
            "0x2bf\n",
            "0x3bf\n",
            "0x303\n",
            "0x59f\n",
            "0x4e5\n",
            "0x20d\n",
            "0x1c9\n",
            "0x341\n",
            "0x2af\n",
            "0x32f\n",
            "0x28d\n",
            "0x4e5\n",
            "0x2e3\n",
            "0x5c7\n",
            "0x439\n",
            "0x117\n",
            "0x143\n",
            "0x34f\n",
            "0x257\n",
            "0x43d\n",
            "0x1f7\n",
            "0x3fb\n",
            "0x367\n",
            "0x6cf\n",
            "0x447\n",
            "0x111\n",
            "0x11b\n",
            "0x2bb\n",
            "0x1ff\n",
            "0x1ff\n",
            "0x15f\n",
            "0x1ff\n",
            "0x1c1\n",
            "0x1041\n",
            "0x35f\n",
            "0x101\n",
            "0x101\n",
            "0x101\n",
            "0x181\n",
            "0x217\n",
            "0xe9\n",
            "0x101\n",
            "0x1b1\n",
            "0x4c1\n",
            "0xc81\n",
            "0x271\n",
            "0x2e1\n",
            "0x1cf\n",
            "0x15f\n",
            "0x249\n",
            "0x129\n",
            "0x101\n",
            "0x1f1\n",
            "0x517\n",
            "0x967\n",
            "0x3a1\n",
            "0x377\n",
            "0x28f\n",
            "0x11f\n",
            "0x249\n",
            "0xf1\n",
            "0xff\n",
            "0x19f\n",
            "0x24f\n",
            "0xb1f\n",
            "0x417\n",
            "0x4df\n",
            "0x2a1\n",
            "0x1c1\n",
            "0x29f\n",
            "0x149\n",
            "0x19f\n",
            "0x1e9\n",
            "0x55f\n",
            "0x8a1\n",
            "0x20f\n",
            "0x2df\n",
            "0x3af\n",
            "0x775\n",
            "0x579\n",
            "0x1bb\n",
            "0x369\n",
            "0x29d\n",
            "0x2bd\n",
            "0x1f5\n",
            "0x97\n",
            "0x103\n",
            "0x4f9\n",
            "0x70d\n",
            "0x4a3\n",
            "0x245\n",
            "0x3df\n",
            "0x279\n",
            "0x27b\n",
            "0x299\n",
            "0x97\n",
            "0xd3\n",
            "0x531\n",
            "0x7d7\n",
            "0x41f\n",
            "0x6a1\n",
            "0x579\n",
            "0x191\n",
            "0x131\n",
            "0xaf\n",
            "0x6f\n",
            "0x71\n",
            "0x3a1\n",
            "0x7d1\n",
            "0x3d9\n",
            "0x37f\n",
            "0x6af\n",
            "0x1e9\n",
            "0x151\n",
            "0x121\n",
            "0x37\n",
            "0x47\n",
            "0x541\n",
            "0x6d7\n",
            "0x4ff\n",
            "0x1fd\n",
            "0x46d\n",
            "0x281\n",
            "0x17b\n",
            "0x2a5\n",
            "0x6f\n",
            "0xc7\n",
            "0x5e7\n",
            "0x573\n",
            "0x3ad\n",
            "0xf1\n",
            "0x2b1\n",
            "0x37b\n",
            "0x431\n",
            "0x457\n",
            "0xc1\n",
            "0x153\n",
            "0x509\n",
            "0x477\n",
            "0x2cf\n",
            "0x141\n",
            "0x2cd\n",
            "0x401\n",
            "0x295\n",
            "0x509\n",
            "0x11b\n",
            "0x197\n",
            "0x641\n",
            "0x4bf\n",
            "0x367\n",
            "0xf3\n",
            "0x143\n",
            "0x261\n",
            "0x231\n",
            "0x5ff\n",
            "0x231\n",
            "0x33d\n",
            "0x599\n",
            "0x381\n",
            "0x2bf\n",
            "0xd3\n",
            "0x15b\n",
            "0x277\n",
            "0x1a7\n",
            "0x70f\n",
            "0x265\n",
            "0x317\n",
            "0x6c5\n",
            "0x691\n",
            "0x3af\n",
            "0x1b9\n",
            "0x31f\n",
            "0x231\n",
            "0x1ef\n",
            "0x301\n",
            "0xbf\n",
            "0xff\n",
            "0x801\n",
            "0x3df\n",
            "0x8af\n",
            "0x14f\n",
            "0x127\n",
            "0x289\n",
            "0x571\n",
            "0x1f7\n",
            "0xd9\n",
            "0xf1\n",
            "0x521\n",
            "0x47f\n",
            "0xb81\n",
            "0x271\n",
            "0x249\n",
            "0x221\n",
            "0x3d1\n",
            "0x127\n",
            "0x37\n",
            "0x3f\n",
            "0x35f\n",
            "0x3b1\n",
            "0x901\n",
            "0x2e1\n",
            "0x2d9\n",
            "0x29f\n",
            "0x531\n",
            "0x179\n",
            "0x69\n",
            "0x57\n",
            "0x361\n",
            "0x3a1\n",
            "0x9d7\n",
            "0x211\n",
            "0x4f9\n",
            "0x351\n",
            "0x2b7\n",
            "0x20f\n",
            "0x31\n",
            "0x31\n",
            "0x2ef\n",
            "0x2b9\n",
            "0x4e7\n",
            "0x105\n",
            "0x1eb\n",
            "0x769\n",
            "0x335\n",
            "0x527\n",
            "0xa1\n",
            "0xf5\n",
            "0x40b\n",
            "0x1f1\n",
            "0x1ff\n",
            "0xaf\n",
            "0x57\n",
            "0x1ff\n",
            "0xc01\n",
            "0x4d1\n",
            "0x1e1\n",
            "0x157\n",
            "0x4e1\n",
            "0xc1\n",
            "0xc1\n",
            "0x1f\n",
            "0xf\n",
            "0x201\n",
            "0x381\n",
            "0xf61\n",
            "0x3d1\n",
            "0x2af\n",
            "0x281\n",
            "0x81\n",
            "0xa1\n",
            "0xf\n",
            "0xf\n",
            "0x12f\n",
            "0x439\n",
            "0xb7f\n",
            "0x7d7\n",
            "0x3e1\n",
            "0x207\n",
            "0x81\n",
            "0x81\n",
            "0x1\n",
            "0x1\n",
            "0x1e1\n",
            "0x2f9\n",
            "0x869\n",
            "0x751\n",
            "0x741\n",
            "0x2e1\n",
            "0x301\n",
            "0x48f\n",
            "0x97\n",
            "0xaf\n",
            "0x287\n",
            "0x54f\n",
            "0x4a9\n",
            "0x117\n",
            "0x111\n",
            "0x839\n",
            "0x4b3\n",
            "0x609\n",
            "0x201\n",
            "0x695\n",
            "0x11f\n",
            "0x755\n",
            "0x16f\n",
            "0xb3\n",
            "0xa3\n",
            "0x15d\n",
            "0x2cf\n",
            "0x5f7\n",
            "0x2f9\n",
            "0x5cf\n",
            "0x101\n",
            "0x8df\n",
            "0x1af\n",
            "0xf7\n",
            "0xa1\n",
            "0x149\n",
            "0x46f\n",
            "0x531\n",
            "0x2ff\n",
            "0x581\n",
            "0xdf\n",
            "0x901\n",
            "0x111\n",
            "0xff\n",
            "0x7f\n",
            "0x159\n",
            "0x359\n",
            "0x4e7\n",
            "0x301\n",
            "0x6c7\n",
            "0xbf\n",
            "0x9c7\n",
            "0x127\n",
            "0xa1\n",
            "0x71\n",
            "0x129\n",
            "0x2d7\n",
            "0x5cf\n",
            "0x231\n",
            "0x621\n",
            "0x101\n",
            "0x931\n",
            "0x1c1\n",
            "0xc9\n",
            "0x87\n",
            "0x199\n",
            "0x369\n",
            "0x4a7\n",
            "0x239\n",
            "0x641\n",
            "0x119\n",
            "0xaff\n",
            "0x111\n",
            "0x89\n",
            "0x77\n",
            "0x11f\n",
            "0x277\n",
            "0x5ff\n",
            "0x1ef\n",
            "0x37f\n",
            "0x1a1\n",
            "0x97f\n",
            "0x301\n",
            "0x101\n",
            "0xef\n",
            "0x1ff\n",
            "0x23f\n",
            "0x581\n",
            "0x1c1\n",
            "0x441\n",
            "0x139\n",
            "0xb9f\n",
            "0x23f\n",
            "0x101\n",
            "0xa1\n",
            "0x147\n",
            "0x27f\n",
            "0x60f\n",
            "0x189\n",
            "0x43f\n",
            "0x119\n",
            "0xa0f\n",
            "0x2bf\n",
            "0x15f\n",
            "0xcf\n",
            "0x191\n",
            "0x381\n",
            "0x537\n",
            "0x23f\n",
            "0x607\n",
            "0xbf\n",
            "0xac1\n",
            "0x151\n",
            "0x81\n",
            "0x57\n",
            "0x107\n",
            "0x2ef\n",
            "0x8bf\n",
            "0x349\n",
            "0x2cf\n",
            "0x34f\n",
            "0x54f\n",
            "0x181\n",
            "0xff\n",
            "0xaf\n",
            "0x24f\n",
            "0x313\n",
            "0x71b\n",
            "0x2e1\n",
            "0x29b\n",
            "0x3d1\n",
            "0x4eb\n",
            "0x25d\n",
            "0x153\n",
            "0x133\n",
            "0x2a9\n",
            "0x381\n",
            "0x647\n",
            "0x349\n",
            "0x2cd\n",
            "0x451\n",
            "0x38d\n",
            "0x2df\n",
            "0x16f\n",
            "0x101\n",
            "0x2cd\n",
            "0x2c9\n",
            "0x4b1\n",
            "0x271\n",
            "0x27d\n",
            "0x3e1\n",
            "0x511\n",
            "0x45f\n",
            "0x1a5\n",
            "0x1db\n",
            "0x2ad\n",
            "0x26b\n",
            "0x411\n",
            "0x14f\n",
            "0x175\n",
            "0x617\n",
            "0x41f\n",
            "0x4f1\n",
            "0x1d7\n",
            "0x249\n",
            "0x351\n",
            "0x339\n",
            "0x447\n",
            "0x24f\n",
            "0x179\n",
            "0x241\n",
            "0xa9f\n",
            "0x2e1\n",
            "0x131\n",
            "0x101\n",
            "0x289\n",
            "0x1e1\n",
            "0x2b9\n",
            "0x13f\n",
            "0x11f\n",
            "0x2b7\n",
            "0x331\n",
            "0x87f\n",
            "0x45f\n",
            "0x3f7\n",
            "0x211\n",
            "0x24b\n",
            "0x2f7\n",
            "0x12f\n",
            "0x12f\n",
            "0x353\n",
            "0x271\n",
            "0x6a9\n",
            "0x3ff\n",
            "0x457\n",
            "0x37f\n",
            "0x181\n",
            "0x28d\n",
            "0xe5\n",
            "0x11d\n",
            "0x4d3\n",
            "0x299\n",
            "0x6b1\n",
            "0x383\n",
            "0x5b3\n",
            "0x28d\n",
            "0x281\n",
            "0x3df\n",
            "0x1e1\n",
            "0x209\n",
            "0x44f\n",
            "0x595\n",
            "0x49f\n",
            "0x1d9\n",
            "0x235\n",
            "0x301\n",
            "0x68f\n",
            "0x917\n",
            "0x13f\n",
            "0x21f\n",
            "0x257\n",
            "0x137\n",
            "0x101\n",
            "0x59\n",
            "0xaf\n",
            "0x731\n",
            "0x401\n",
            "0x1041\n",
            "0x1ff\n",
            "0x1cf\n",
            "0x211\n",
            "0xb1\n",
            "0x7f\n",
            "0x61\n",
            "0x61\n",
            "0x3c1\n",
            "0x2e1\n",
            "0x767\n",
            "0x857\n",
            "0x541\n",
            "0x379\n",
            "0xb9\n",
            "0x5f\n",
            "0x67\n",
            "0x61\n",
            "0x291\n",
            "0x257\n",
            "0xa3f\n",
            "0x2c1\n",
            "0x941\n",
            "0x241\n",
            "0x81\n",
            "0xbf\n",
            "0x51\n",
            "0x41\n",
            "0x331\n",
            "0x199\n",
            "0x631\n",
            "0x131\n",
            "0x229\n",
            "0xe61\n",
            "0x11f\n",
            "0xb1\n",
            "0x79\n",
            "0x91\n",
            "0x379\n",
            "0x201\n",
            "0x37f\n",
            "0x7f\n",
            "0xc1\n",
            "0xff\n",
            "0x1241\n",
            "0x181\n",
            "0x41\n",
            "0x5f\n",
            "0x341\n",
            "0x8f\n",
            "0xb1\n",
            "0xf\n",
            "0x2f\n",
            "0x5f\n",
            "0x161\n",
            "0x1481\n",
            "0x42f\n",
            "0x181\n",
            "0x201\n",
            "0x81\n",
            "0xc1\n",
            "0x3f\n",
            "0x3f\n",
            "0xbf\n",
            "0x91\n",
            "0x5af\n",
            "0x134f\n",
            "0x1ef\n",
            "0x1b1\n",
            "0x191\n",
            "0x1f1\n",
            "0x5f\n",
            "0x71\n",
            "0x17f\n",
            "0x1bf\n",
            "0x3ff\n",
            "0x4e7\n",
            "0xa81\n",
            "0x4df\n",
            "0x317\n",
            "0x6e1\n",
            "0x109\n",
            "0x139\n",
            "0x241\n",
            "0x189\n",
            "0x137\n",
            "0xef\n",
            "0x101\n",
            "0xc91\n",
            "0x4e9\n",
            "0x91f\n",
            "0x359\n",
            "0x417\n",
            "0x289\n",
            "0x389\n",
            "0x121\n",
            "0xa9\n",
            "0xff\n",
            "0x18f\n",
            "0x3ad\n",
            "0x72b\n",
            "0x3df\n",
            "0x495\n",
            "0x317\n",
            "0x421\n",
            "0x183\n",
            "0xfd\n",
            "0x11d\n",
            "0x1c9\n",
            "0x38d\n",
            "0x723\n",
            "0x4fb\n",
            "0x4cd\n",
            "0x33f\n",
            "0x2ab\n",
            "0x1ed\n",
            "0x141\n",
            "0xf7\n",
            "0x175\n",
            "0x481\n",
            "0x711\n",
            "0x3b7\n",
            "0x5c3\n",
            "0x333\n",
            "0x35d\n",
            "0x129\n",
            "0xc1\n",
            "0xb7\n",
            "0x1b1\n",
            "0x2a1\n",
            "0x6dd\n",
            "0x357\n",
            "0x417\n",
            "0x683\n",
            "0x3e5\n",
            "0x177\n",
            "0xf7\n",
            "0xed\n",
            "0x143\n",
            "0x47f\n",
            "0x7ff\n",
            "0x41f\n",
            "0x359\n",
            "0x311\n",
            "0x3c9\n",
            "0x239\n",
            "0xbf\n",
            "0xb1\n",
            "0x15f\n",
            "0x2e7\n",
            "0x3d3\n",
            "0x54b\n",
            "0x1ff\n",
            "0x31b\n",
            "0x2bb\n",
            "0x76f\n",
            "0x273\n",
            "0x133\n",
            "0xff\n",
            "0x2b5\n",
            "0x3d5\n",
            "0x60f\n",
            "0x239\n",
            "0x271\n",
            "0x3ff\n",
            "0x211\n",
            "0x537\n",
            "0x1d3\n",
            "0x183\n",
            "0x3cb\n",
            "0x59d\n",
            "0x471\n",
            "0x29f\n",
            "0x235\n",
            "0x4c1\n",
            "0x163\n",
            "0x285\n",
            "0x2c1\n",
            "0x1db\n",
            "0x4ab\n",
            "0x651\n",
            "0x4c1\n",
            "0x40f\n",
            "0x253\n",
            "0x52f\n",
            "0x15d\n",
            "0xcd\n",
            "0xe1\n",
            "0x18d\n",
            "0x21f\n",
            "0x9ef\n",
            "0x2bf\n",
            "0x3df\n",
            "0x261\n",
            "0x419\n",
            "0x251\n",
            "0xbf\n",
            "0xe9\n",
            "0x2b9\n",
            "0x1df\n",
            "0x969\n",
            "0x23f\n",
            "0x2d1\n",
            "0x2d1\n",
            "0x37f\n",
            "0x451\n",
            "0x121\n",
            "0x151\n",
            "0x28f\n",
            "0x229\n",
            "0x98f\n",
            "0x41f\n",
            "0x39f\n",
            "0x21f\n",
            "0x3e1\n",
            "0x231\n",
            "0xff\n",
            "0xff\n",
            "0x21f\n",
            "0x2df\n",
            "0x9a9\n",
            "0x361\n",
            "0x5ff\n",
            "0x1f9\n",
            "0x2c7\n",
            "0x201\n",
            "0x61\n",
            "0x5f\n",
            "0x25f\n",
            "0x1a7\n",
            "0x7bf\n",
            "0x221\n",
            "0x1ff\n",
            "0x381\n",
            "0x381\n",
            "0x4d9\n",
            "0xf1\n",
            "0x17f\n",
            "0x411\n",
            "0x1f7\n",
            "0x7af\n",
            "0x23f\n",
            "0x1df\n",
            "0x2a7\n",
            "0x377\n",
            "0x481\n",
            "0x221\n",
            "0x227\n",
            "0x359\n",
            "0x161\n",
            "0x52f\n",
            "0x1d1\n",
            "0x127\n",
            "0x2ef\n",
            "0x3c7\n",
            "0x8bf\n",
            "0x19f\n",
            "0x247\n",
            "0x2ff\n",
            "0x17f\n",
            "0x7cf\n",
            "0x2bf\n",
            "0x18f\n",
            "0x287\n",
            "0x3b7\n",
            "0x5c1\n",
            "0x211\n",
            "0x24f\n",
            "0x201\n",
            "0x141\n",
            "0x711\n",
            "0x1ef\n",
            "0x1bd\n",
            "0x2b1\n",
            "0x455\n",
            "0x605\n",
            "0x1e3\n",
            "0x1f5\n",
            "0x301\n",
            "0x1a1\n",
            "0x8e7\n",
            "0x2e7\n",
            "0x3c1\n",
            "0x25f\n",
            "0x2d7\n",
            "0x4c7\n",
            "0xd7\n",
            "0x10f\n",
            "0x2e7\n",
            "0x247\n",
            "0x6ad\n",
            "0x135\n",
            "0x1d3\n",
            "0x341\n",
            "0x749\n",
            "0x611\n",
            "0x83\n",
            "0xef\n",
            "0x1db\n",
            "0x277\n",
            "0x909\n",
            "0x191\n",
            "0x2c1\n",
            "0x3c7\n",
            "0x491\n",
            "0x4ff\n",
            "0x57\n",
            "0x87\n",
            "0x1df\n",
            "0x29f\n",
            "0x95f\n",
            "0x269\n",
            "0x32f\n",
            "0x2ef\n",
            "0x631\n",
            "0x371\n",
            "0x3f\n",
            "0x49\n",
            "0x141\n",
            "0x21f\n",
            "0x927\n",
            "0x189\n",
            "0x317\n",
            "0x4c7\n",
            "0x419\n",
            "0x481\n",
            "0x4f\n",
            "0x79\n",
            "0x1a7\n",
            "0x19f\n",
            "0x6af\n",
            "0xd3\n",
            "0x1ef\n",
            "0x5ff\n",
            "0x63f\n",
            "0x61f\n",
            "0x5d\n",
            "0xb3\n",
            "0x171\n",
            "0x13f\n",
            "0x6c1\n",
            "0xff\n",
            "0x111\n",
            "0x1c1\n",
            "0x10a1\n",
            "0x201\n",
            "0x41\n",
            "0x3f\n",
            "0x9f\n",
            "0x109\n",
            "0x3c1\n",
            "0x67\n",
            "0x77\n",
            "0x1c7\n",
            "0x6b1\n",
            "0xe89\n",
            "0x149\n",
            "0x157\n",
            "0x8f\n",
            "0x10f\n",
            "0x2df\n",
            "0x61\n",
            "0x8f\n",
            "0x1c7\n",
            "0x53f\n",
            "0xf09\n",
            "0x25f\n",
            "0x201\n",
            "0x99\n",
            "0xf1\n",
            "0x1ff\n",
            "0x21\n",
            "0x5f\n",
            "0x2bf\n",
            "0x3bf\n",
            "0x10ff\n",
            "0x1c1\n",
            "0x2a1\n",
            "0x8f\n",
            "0x1ff\n",
            "0x551\n",
            "0xe1\n",
            "0x187\n",
            "0x36f\n",
            "0x497\n",
            "0xb1f\n",
            "0xaf\n",
            "0xff\n",
            "0x12f\n",
            "0x3c7\n",
            "0x5af\n",
            "0x879\n",
            "0x28f\n",
            "0x121\n",
            "0x4a7\n",
            "0x1e1\n",
            "0x3cf\n",
            "0x1\n",
            "0x1\n",
            "0x539\n",
            "0x699\n",
            "0x221\n",
            "0x97\n",
            "0x1a7\n",
            "0x2df\n",
            "0x181\n",
            "0xb4f\n",
            "0x1\n",
            "0x1\n",
            "0x6ff\n",
            "0x1ff\n",
            "0x861\n",
            "0x247\n",
            "0xef\n",
            "0x2af\n",
            "0x311\n",
            "0x58f\n",
            "0x1\n",
            "0x1\n",
            "0x7f7\n",
            "0x1ff\n",
            "0x65f\n",
            "0x1ff\n",
            "0x15f\n",
            "0x361\n",
            "0x29f\n",
            "0x641\n",
            "0x1\n",
            "0x1\n",
            "0x41f\n",
            "0x6b1\n",
            "0x1f9\n",
            "0xf9\n",
            "0x177\n",
            "0x471\n",
            "0x181\n",
            "0xacf\n",
            "0x1\n",
            "0x1\n",
            "0x459\n",
            "0x4f1\n",
            "0x27f\n",
            "0xf1\n",
            "0x281\n",
            "0x48f\n",
            "0x201\n",
            "0xa21\n",
            "0x1\n",
            "0x1\n",
            "0x63f\n",
            "0x3c1\n",
            "0x591\n",
            "0x1a9\n",
            "0x119\n",
            "0x435\n",
            "0x3d5\n",
            "0x59f\n",
            "0x1\n",
            "0x1\n",
            "0x4d5\n",
            "0x76d\n",
            "0x389\n",
            "0x1b5\n",
            "0x213\n",
            "0x439\n",
            "0x247\n",
            "0x5d5\n",
            "0x1\n",
            "0x1\n",
            "0x6e7\n",
            "0x5ef\n",
            "0x169\n",
            "0x77\n",
            "0x151\n",
            "0x4df\n",
            "0x1ff\n",
            "0x911\n",
            "0x1\n",
            "0x1\n",
            "0x427\n",
            "0x5a7\n",
            "0x5f\n",
            "0x1f\n",
            "0x13f\n",
            "0x4f7\n",
            "0x99\n",
            "0xea7\n",
            "0x1\n",
            "0x1\n",
            "0x349\n",
            "0x287\n",
            "0x69f\n",
            "0x4e3\n",
            "0x1dd\n",
            "0x24b\n",
            "0x30b\n",
            "0x781\n",
            "0x1\n",
            "0x1\n",
            "0x76f\n",
            "0xe1\n",
            "0x517\n",
            "0x241\n",
            "0x1c1\n",
            "0x229\n",
            "0x3ff\n",
            "0x841\n",
            "0x1\n",
            "0x1\n",
            "0x82f\n",
            "0x13f\n",
            "0x381\n",
            "0x21f\n",
            "0x1a7\n",
            "0x2b1\n",
            "0x391\n",
            "0x8ff\n",
            "0x1\n",
            "0x1\n",
            "0x4e1\n",
            "0xf7\n",
            "0xa8f\n",
            "0x381\n",
            "0x181\n",
            "0x181\n",
            "0x3e7\n",
            "0x521\n",
            "0x1\n",
            "0x1\n",
            "0x53f\n",
            "0x89\n",
            "0x60f\n",
            "0x2d1\n",
            "0x111\n",
            "0x1df\n",
            "0x63f\n",
            "0x811\n",
            "0x1\n",
            "0x1\n",
            "0x5c1\n",
            "0x1a1\n",
            "0xa01\n",
            "0x2e1\n",
            "0x1e1\n",
            "0x177\n",
            "0x347\n",
            "0x4ef\n",
            "0x1\n",
            "0x1\n",
            "0x661\n",
            "0x12f\n",
            "0x8b7\n",
            "0x401\n",
            "0x13f\n",
            "0xf7\n",
            "0x481\n",
            "0x4ff\n",
            "0x1\n",
            "0x1\n",
            "0x5b3\n",
            "0x153\n",
            "0x48b\n",
            "0x2ff\n",
            "0x281\n",
            "0x28d\n",
            "0x4eb\n",
            "0x759\n",
            "0x1\n",
            "0x1\n",
            "0x7ff\n",
            "0x5f\n",
            "0x87f\n",
            "0x2a1\n",
            "0xb1\n",
            "0x10f\n",
            "0x661\n",
            "0x441\n",
            "0x1\n",
            "0x1\n",
            "0xa5f\n",
            "0x57\n",
            "0x3f9\n",
            "0x187\n",
            "0x19f\n",
            "0x101\n",
            "0x571\n",
            "0x7a1\n",
            "0x1\n",
            "0x1\n",
            "0x181\n",
            "0x6f1\n",
            "0x211\n",
            "0x181\n",
            "0x1f9\n",
            "0x8ff\n",
            "0x2c9\n",
            "0x619\n",
            "0x1\n",
            "0x1\n",
            "0x447\n",
            "0x431\n",
            "0x169\n",
            "0x1e7\n",
            "0x2c1\n",
            "0x55f\n",
            "0x361\n",
            "0x891\n",
            "0x1\n",
            "0x1\n",
            "0x2f1\n",
            "0x277\n",
            "0x1b9\n",
            "0x15f\n",
            "0x441\n",
            "0x5af\n",
            "0x59f\n",
            "0x7d9\n",
            "0x1\n",
            "0x1\n",
            "0x291\n",
            "0x151\n",
            "0xf9\n",
            "0x1bf\n",
            "0x36f\n",
            "0x799\n",
            "0x44f\n",
            "0x9e1\n",
            "0x1\n",
            "0x1\n",
            "0x191\n",
            "0x2af\n",
            "0xe1\n",
            "0xb9\n",
            "0x701\n",
            "0x79f\n",
            "0x31f\n",
            "0x83f\n",
            "0x1\n",
            "0x1\n",
            "0x191\n",
            "0x31f\n",
            "0x121\n",
            "0x1e1\n",
            "0x4a9\n",
            "0xacf\n",
            "0x1bf\n",
            "0x701\n",
            "0x1\n",
            "0x1\n",
            "0x209\n",
            "0x421\n",
            "0x181\n",
            "0xaf\n",
            "0x2df\n",
            "0x9bf\n",
            "0x391\n",
            "0x751\n",
            "0x1\n",
            "0x1\n",
            "0x1bf\n",
            "0x4c9\n",
            "0x1b7\n",
            "0x1a9\n",
            "0x511\n",
            "0x96f\n",
            "0x2d1\n",
            "0x4b7\n",
            "0x1\n",
            "0x1\n",
            "0x2d1\n",
            "0x1ff\n",
            "0x1a1\n",
            "0x1b7\n",
            "0x547\n",
            "0x8c1\n",
            "0x4c7\n",
            "0x4f1\n",
            "0x1\n",
            "0x1\n",
            "0x3bf\n",
            "0x11f\n",
            "0x97\n",
            "0xff\n",
            "0x68f\n",
            "0x9af\n",
            "0x261\n",
            "0x6bf\n",
            "0x1\n",
            "0x1\n",
            "0x535\n",
            "0x65d\n",
            "0x47f\n",
            "0x1d7\n",
            "0x133\n",
            "0x395\n",
            "0x367\n",
            "0x5e9\n",
            "0x1\n",
            "0x1\n",
            "0x4f1\n",
            "0xa89\n",
            "0x1af\n",
            "0xbf\n",
            "0xb9\n",
            "0x341\n",
            "0x241\n",
            "0x7c1\n",
            "0x1\n",
            "0x1\n",
            "0x727\n",
            "0x629\n",
            "0x2af\n",
            "0x81\n",
            "0x14f\n",
            "0x177\n",
            "0x347\n",
            "0x967\n",
            "0x1\n",
            "0x1\n",
            "0x5f7\n",
            "0x7bf\n",
            "0x43f\n",
            "0x10f\n",
            "0x1c7\n",
            "0x16f\n",
            "0x381\n",
            "0x639\n",
            "0x1\n",
            "0x1\n",
            "0x481\n",
            "0x32f\n",
            "0x13f\n",
            "0x117\n",
            "0x73f\n",
            "0x451\n",
            "0x219\n",
            "0x82f\n",
            "0x1\n",
            "0x1\n",
            "0x461\n",
            "0x261\n",
            "0x14f\n",
            "0x36f\n",
            "0x2d1\n",
            "0x9bf\n",
            "0x201\n",
            "0x5d7\n",
            "0x1\n",
            "0x1\n",
            "0x7d1\n",
            "0x34f\n",
            "0x5ff\n",
            "0x28f\n",
            "0x129\n",
            "0x239\n",
            "0x4f1\n",
            "0x3f1\n",
            "0x1\n",
            "0x1\n",
            "0x60f\n",
            "0x467\n",
            "0x201\n",
            "0x119\n",
            "0x101\n",
            "0x301\n",
            "0x2a9\n",
            "0xba1\n",
            "0x1\n",
            "0x1\n",
            "0x9ff\n",
            "0x411\n",
            "0x26f\n",
            "0x24f\n",
            "0x131\n",
            "0x37f\n",
            "0x37f\n",
            "0x4ef\n",
            "0x1\n",
            "0x1\n",
            "0xbff\n",
            "0x24f\n",
            "0x1bf\n",
            "0x117\n",
            "0x1b7\n",
            "0x1df\n",
            "0x301\n",
            "0x837\n",
            "0x1\n",
            "0x1\n",
            "0xa71\n",
            "0x35f\n",
            "0x2f7\n",
            "0x23f\n",
            "0x481\n",
            "0x101\n",
            "0x2b1\n",
            "0x489\n",
            "0x1\n",
            "0x1\n",
            "0xa01\n",
            "0x307\n",
            "0x1cf\n",
            "0x2ff\n",
            "0x547\n",
            "0x2b9\n",
            "0x22f\n",
            "0x3f1\n",
            "0x1\n",
            "0x1\n",
            "0xde1\n",
            "0xf9\n",
            "0x211\n",
            "0x41f\n",
            "0x47f\n",
            "0x121\n",
            "0x2e7\n",
            "0x261\n",
            "0x1\n",
            "0x1\n",
            "0xf9f\n",
            "0xa1\n",
            "0x27f\n",
            "0x301\n",
            "0x301\n",
            "0xbf\n",
            "0x4ff\n",
            "0x141\n",
            "0x1\n",
            "0x1\n",
            "0x579\n",
            "0x1d1\n",
            "0x8f\n",
            "0xdf\n",
            "0xee1\n",
            "0x261\n",
            "0x271\n",
            "0x37f\n",
            "0x1\n",
            "0x1\n",
            "0xa3f\n",
            "0x241\n",
            "0x1e1\n",
            "0x30f\n",
            "0x521\n",
            "0x2cf\n",
            "0x249\n",
            "0x429\n",
            "0x1\n",
            "0x1\n",
            "0x16e1\n",
            "0x3f\n",
            "0x11f\n",
            "0x91\n",
            "0x301\n",
            "0x1f\n",
            "0x181\n",
            "0x201\n",
            "0x1\n",
            "0x1\n",
            "0xaa9\n",
            "0x2ef\n",
            "0x101\n",
            "0x179\n",
            "0x4b1\n",
            "0x257\n",
            "0x2b9\n",
            "0x601\n",
            "0x1\n",
            "0x1\n",
            "0xfff\n",
            "0x121\n",
            "0x101\n",
            "0x7f\n",
            "0x45f\n",
            "0x11f\n",
            "0x1bf\n",
            "0x5ff\n",
            "0x1\n",
            "0x1\n",
            "0x6c1\n",
            "0x29f\n",
            "0x47\n",
            "0x1f\n",
            "0xa57\n",
            "0x21f\n",
            "0x171\n",
            "0x837\n",
            "0x1\n",
            "0x1\n",
            "0x2f7\n",
            "0x6e1\n",
            "0x13f\n",
            "0x1c1\n",
            "0x30f\n",
            "0x801\n",
            "0x11f\n",
            "0x6ef\n",
            "0x1\n",
            "0x1\n",
            "0x62f\n",
            "0x7e1\n",
            "0x331\n",
            "0x251\n",
            "0x371\n",
            "0x1a1\n",
            "0x221\n",
            "0x52f\n",
            "0x1\n",
            "0x1\n",
            "0x5c1\n",
            "0x1af\n",
            "0x829\n",
            "0xa67\n",
            "0xc1\n",
            "0x39\n",
            "0x281\n",
            "0x277\n",
            "0x1\n",
            "0x1\n",
            "0x7cf\n",
            "0x29f\n",
            "0x2df\n",
            "0x639\n",
            "0x45f\n",
            "0x161\n",
            "0x461\n",
            "0x241\n",
            "0x1\n",
            "0x1\n",
            "0x341\n",
            "0x46f\n",
            "0xc1\n",
            "0x17f\n",
            "0x1041\n",
            "0x181\n",
            "0x151\n",
            "0x29f\n",
            "0x1\n",
            "0x1\n",
            "0x4f5\n",
            "0x34d\n",
            "0x331\n",
            "0x51f\n",
            "0x437\n",
            "0x58b\n",
            "0x165\n",
            "0x443\n",
            "0x1\n",
            "0x1\n",
            "0x85f\n",
            "0x1cf\n",
            "0x3d1\n",
            "0x311\n",
            "0x2df\n",
            "0x18f\n",
            "0x637\n",
            "0x43f\n",
            "0x1\n",
            "0x1\n",
            "0x5e7\n",
            "0x2ff\n",
            "0x1a1\n",
            "0x1ff\n",
            "0x41f\n",
            "0x25f\n",
            "0x229\n",
            "0xac1\n",
            "0x1\n",
            "0x1\n",
            "0x395\n",
            "0x753\n",
            "0x321\n",
            "0x2af\n",
            "0x51d\n",
            "0x3f5\n",
            "0x1ab\n",
            "0x485\n",
            "0x1\n",
            "0x1\n",
            "0x239\n",
            "0x609\n",
            "0x149\n",
            "0xf1\n",
            "0xe87\n",
            "0x227\n",
            "0x97\n",
            "0x3ff\n",
            "0x1\n",
            "0x1\n",
            "0x4cb\n",
            "0x66d\n",
            "0x18d\n",
            "0x269\n",
            "0x581\n",
            "0x349\n",
            "0x201\n",
            "0x5e9\n",
            "0x1\n",
            "0x1\n",
            "0x587\n",
            "0x209\n",
            "0x159\n",
            "0x19f\n",
            "0x7d9\n",
            "0x337\n",
            "0x1a1\n",
            "0x8bf\n",
            "0x1\n",
            "0x1\n",
            "0x591\n",
            "0x1c1\n",
            "0x139\n",
            "0x209\n",
            "0x709\n",
            "0x237\n",
            "0x207\n",
            "0xa01\n",
            "0x1\n",
            "0x1\n",
            "0x35f\n",
            "0x25f\n",
            "0x141\n",
            "0x239\n",
            "0xc01\n",
            "0x1e1\n",
            "0x12f\n",
            "0x77f\n",
            "0x1\n",
            "0x1\n",
            "0x4d7\n",
            "0x3b1\n",
            "0x171\n",
            "0x207\n",
            "0x759\n",
            "0x23f\n",
            "0x141\n",
            "0x907\n",
            "0x1\n",
            "0x1\n",
            "0x549\n",
            "0x2ff\n",
            "0x1c1\n",
            "0x1b7\n",
            "0x6d1\n",
            "0x2cf\n",
            "0x16f\n",
            "0x911\n",
            "0x1\n",
            "0x1\n",
            "0x797\n",
            "0x251\n",
            "0xa1\n",
            "0xbf\n",
            "0x421\n",
            "0x1ff\n",
            "0x331\n",
            "0xb2f\n",
            "0x1\n",
            "0x1\n",
            "0x513\n",
            "0x363\n",
            "0x1eb\n",
            "0x27f\n",
            "0x535\n",
            "0x3e7\n",
            "0x28f\n",
            "0x761\n",
            "0x1\n",
            "0x1\n",
            "0x649\n",
            "0x209\n",
            "0xff\n",
            "0x129\n",
            "0x527\n",
            "0x3d1\n",
            "0x201\n",
            "0xa61\n",
            "0x1\n",
            "0x1\n",
            "0x627\n",
            "0x211\n",
            "0xbf\n",
            "0x81\n",
            "0x449\n",
            "0x377\n",
            "0xdf\n",
            "0xdb9\n",
            "0x1\n",
            "0x1\n",
            "0x4f1\n",
            "0x241\n",
            "0x267\n",
            "0x11f\n",
            "0x417\n",
            "0x4df\n",
            "0x401\n",
            "0x81f\n",
            "0x1\n",
            "0x1\n",
            "0x69f\n",
            "0x2df\n",
            "0x15f\n",
            "0x81\n",
            "0x297\n",
            "0x4e7\n",
            "0x471\n",
            "0x891\n",
            "0x1\n",
            "0x1\n",
            "0x5d9\n",
            "0x1c1\n",
            "0x62b\n",
            "0x2a3\n",
            "0x201\n",
            "0x1e3\n",
            "0x6bf\n",
            "0x4d3\n",
            "0x1\n",
            "0x1\n",
            "0x7f9\n",
            "0x351\n",
            "0x2ff\n",
            "0x141\n",
            "0x261\n",
            "0x347\n",
            "0x41f\n",
            "0x6a1\n",
            "0x1\n",
            "0x1\n",
            "0x5b7\n",
            "0x401\n",
            "0xa1\n",
            "0x61\n",
            "0x29f\n",
            "0x609\n",
            "0x2ff\n",
            "0x971\n",
            "0x1\n",
            "0x1\n",
            "0x5e7\n",
            "0x2df\n",
            "0x23f\n",
            "0x1a1\n",
            "0x2ff\n",
            "0x597\n",
            "0x309\n",
            "0x7bf\n",
            "0x1\n",
            "0x1\n",
            "0x68f\n",
            "0x249\n",
            "0x739\n",
            "0x25b\n",
            "0x1cf\n",
            "0x1f1\n",
            "0x455\n",
            "0x581\n",
            "0x1\n",
            "0x1\n",
            "0x685\n",
            "0x191\n",
            "0x1cf\n",
            "0xe1\n",
            "0x433\n",
            "0x547\n",
            "0x431\n",
            "0x775\n",
            "0x1\n",
            "0x1\n",
            "0x531\n",
            "0x3e7\n",
            "0x1b9\n",
            "0xb7\n",
            "0x381\n",
            "0x5a9\n",
            "0x227\n",
            "0x911\n",
            "0x1\n",
            "0x1\n",
            "0x581\n",
            "0x41f\n",
            "0xa9\n",
            "0x2f\n",
            "0x329\n",
            "0x6df\n",
            "0x159\n",
            "0xa09\n",
            "0x1\n",
            "0x1\n",
            "0x849\n",
            "0x91\n",
            "0xbf\n",
            "0x3f1\n",
            "0x1a9\n",
            "0x7f\n",
            "0xe1f\n",
            "0x211\n",
            "0x1\n",
            "0x1\n",
            "0x8af\n",
            "0x1b7\n",
            "0xbf\n",
            "0x151\n",
            "0x1df\n",
            "0x31f\n",
            "0x481\n",
            "0x9f1\n",
            "0x1\n",
            "0x1\n",
            "0x277\n",
            "0x131\n",
            "0xf1f\n",
            "0x427\n",
            "0x97\n",
            "0x81\n",
            "0x5a7\n",
            "0x231\n",
            "0x1\n",
            "0x1\n",
            "0x1df\n",
            "0x2f\n",
            "0xb1\n",
            "0x192f\n",
            "0x5f\n",
            "0x11\n",
            "0x241\n",
            "0xef\n",
            "0x1\n",
            "0x1\n",
            "0x301\n",
            "0x81\n",
            "0x1\n",
            "0x5f\n",
            "0x154f\n",
            "0x101\n",
            "0x271\n",
            "0x2df\n",
            "0x1\n",
            "0x1\n",
            "0x431\n",
            "0x7f\n",
            "0x1f\n",
            "0x16f\n",
            "0x1e1\n",
            "0x1311\n",
            "0x181\n",
            "0x2ef\n",
            "0x1\n",
            "0x1\n",
            "0x3e1\n",
            "0x21\n",
            "0x31\n",
            "0xff\n",
            "0x21\n",
            "0xf\n",
            "0x194f\n",
            "0xc1\n",
            "0x1\n",
            "0x1\n",
            "0x8a7\n",
            "0xdf\n",
            "0x129\n",
            "0x419\n",
            "0x3b1\n",
            "0x181\n",
            "0x53f\n",
            "0x6b7\n",
            "0x1\n",
            "0x1\n",
            "0x659\n",
            "0x3d9\n",
            "0x4f\n",
            "0x9f\n",
            "0x239\n",
            "0x2cf\n",
            "0x587\n",
            "0xa2f\n",
            "0x1\n",
            "0x1\n",
            "0x7d9\n",
            "0x321\n",
            "0x11\n",
            "0x49\n",
            "0x4e1\n",
            "0x187\n",
            "0x4c7\n",
            "0x95f\n",
            "0x1\n",
            "0x1\n",
            "0x65f\n",
            "0x419\n",
            "0x23b\n",
            "0x35f\n",
            "0x1db\n",
            "0x46f\n",
            "0x2f9\n",
            "0x6a5\n",
            "0x1\n",
            "0x1\n",
            "0x7d1\n",
            "0x69f\n",
            "0x209\n",
            "0x1df\n",
            "0x27f\n",
            "0x30f\n",
            "0x287\n",
            "0x57f\n",
            "0x1\n",
            "0x1\n",
            "0x3d9\n",
            "0x277\n",
            "0xa2f\n",
            "0x1e9\n",
            "0x1b1\n",
            "0x1b1\n",
            "0x3df\n",
            "0x64f\n",
            "0x1\n",
            "0x1\n",
            "0x5ff\n",
            "0x13f\n",
            "0x381\n",
            "0x38f\n",
            "0x441\n",
            "0x1e1\n",
            "0x331\n",
            "0x841\n",
            "0x1\n",
            "0x1\n",
            "0x56f\n",
            "0x359\n",
            "0x1a1\n",
            "0x359\n",
            "0x889\n",
            "0x1e1\n",
            "0x321\n",
            "0x499\n",
            "0x1\n",
            "0x1\n",
            "0x7f7\n",
            "0x2c1\n",
            "0x24f\n",
            "0x289\n",
            "0x297\n",
            "0x361\n",
            "0x38f\n",
            "0x6d9\n",
            "0x1\n",
            "0x1\n",
            "0xbb9\n",
            "0x119\n",
            "0xf9\n",
            "0xf7\n",
            "0x127\n",
            "0x117\n",
            "0x809\n",
            "0x6c9\n",
            "0x1\n",
            "0x1\n",
            "0x481\n",
            "0x461\n",
            "0x3bf\n",
            "0x467\n",
            "0x24b\n",
            "0x2d1\n",
            "0x431\n",
            "0x59d\n",
            "0x1\n",
            "0x1\n",
            "0x79f\n",
            "0x3bf\n",
            "0x209\n",
            "0x19f\n",
            "0x289\n",
            "0x31f\n",
            "0x2e9\n",
            "0x861\n",
            "0x1\n",
            "0x1\n",
            "0x8ff\n",
            "0x1e1\n",
            "0xd7\n",
            "0xaf\n",
            "0x301\n",
            "0x19f\n",
            "0x2c7\n",
            "0xc01\n",
            "0x1\n",
            "0x1\n",
            "0x2b9\n",
            "0x347\n",
            "0x189\n",
            "0x177\n",
            "0x1c9\n",
            "0xacf\n",
            "0x499\n",
            "0x5bf\n",
            "0x1\n",
            "0x1\n",
            "0x419\n",
            "0x3ff\n",
            "0x359\n",
            "0x15f\n",
            "0x1f1\n",
            "0x7f7\n",
            "0x33f\n",
            "0x5ff\n",
            "0x1\n",
            "0x1\n",
            "0x8d9\n",
            "0x2b7\n",
            "0x57f\n",
            "0x101\n",
            "0x14f\n",
            "0x359\n",
            "0x27f\n",
            "0x6c1\n",
            "0x1\n",
            "0x1\n",
            "0x511\n",
            "0x335\n",
            "0x651\n",
            "0x369\n",
            "0x1a5\n",
            "0x4e9\n",
            "0x443\n",
            "0x319\n",
            "0x1\n",
            "0x1\n",
            "0x4b5\n",
            "0x581\n",
            "0x2e1\n",
            "0x107\n",
            "0x33f\n",
            "0x40d\n",
            "0x3a3\n",
            "0x6d5\n",
            "0x1\n",
            "0x1\n",
            "0x4e1\n",
            "0x341\n",
            "0x4df\n",
            "0x2d1\n",
            "0x22f\n",
            "0x475\n",
            "0x41b\n",
            "0x571\n",
            "0x1\n",
            "0x1\n",
            "0x3f7\n",
            "0x381\n",
            "0x5bf\n",
            "0x4db\n",
            "0x2fb\n",
            "0x46f\n",
            "0x249\n",
            "0x431\n",
            "0x1\n",
            "0x1\n",
            "0x451\n",
            "0x31f\n",
            "0x177\n",
            "0xef\n",
            "0x321\n",
            "0x5e9\n",
            "0x301\n",
            "0xa1f\n",
            "0x1\n",
            "0x1\n",
            "0x761\n",
            "0x32d\n",
            "0x63d\n",
            "0x1ef\n",
            "0x209\n",
            "0x391\n",
            "0x303\n",
            "0x495\n",
            "0x1\n",
            "0x1\n",
            "0x5e9\n",
            "0x27f\n",
            "0x177\n",
            "0x51\n",
            "0x4e9\n",
            "0x2c7\n",
            "0x12f\n",
            "0xcbf\n",
            "0x1\n",
            "0x1\n",
            "0x28f\n",
            "0x401\n",
            "0xd1\n",
            "0x171\n",
            "0x1a1\n",
            "0x967\n",
            "0x3a1\n",
            "0x83f\n",
            "0x1\n",
            "0x1\n",
            "0x3a1\n",
            "0x53f\n",
            "0x13f\n",
            "0xc7\n",
            "0x169\n",
            "0x841\n",
            "0x1ff\n",
            "0x957\n",
            "0x1\n",
            "0x1\n",
            "0x501\n",
            "0x357\n",
            "0x25f\n",
            "0x27f\n",
            "0x19f\n",
            "0x60f\n",
            "0x271\n",
            "0x891\n",
            "0x1\n",
            "0x1\n",
            "0x3f9\n",
            "0x4a7\n",
            "0x191\n",
            "0x33f\n",
            "0x281\n",
            "0x899\n",
            "0x1a1\n",
            "0x5c1\n",
            "0x1\n",
            "0x1\n",
            "0x2ff\n",
            "0x41f\n",
            "0xe7\n",
            "0x127\n",
            "0xa2f\n",
            "0x4e1\n",
            "0xd1\n",
            "0x6c7\n",
            "0x1\n",
            "0x1\n",
            "0x3d9\n",
            "0x2af\n",
            "0x81\n",
            "0x187\n",
            "0x249\n",
            "0xc71\n",
            "0x201\n",
            "0x681\n",
            "0x1\n",
            "0x1\n",
            "0x54f\n",
            "0x4c1\n",
            "0x251\n",
            "0x2c9\n",
            "0x119\n",
            "0x82f\n",
            "0x2b1\n",
            "0x4c1\n",
            "0x1\n",
            "0x1\n",
            "0x46f\n",
            "0x349\n",
            "0x179\n",
            "0x101\n",
            "0x281\n",
            "0x5b7\n",
            "0x2ff\n",
            "0xa71\n",
            "0x1\n",
            "0x1\n",
            "0x3c1\n",
            "0x5e7\n",
            "0x1b7\n",
            "0x1a7\n",
            "0x261\n",
            "0x681\n",
            "0x141\n",
            "0x8cf\n",
            "0x1\n",
            "0x1\n",
            "0x321\n",
            "0x6df\n",
            "0xc1\n",
            "0x67\n",
            "0x449\n",
            "0x45f\n",
            "0x91\n",
            "0xba1\n",
            "0x1\n",
            "0x1\n",
            "Validation Accuracy: 0.80\n",
            "Validation Time   : 0:08:19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ."
      ],
      "metadata": {
        "id": "cSd2dA-wQ94T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVB2VLaY9PK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ac9163-ed66-494c-c94d-63509395a687"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1yMW5ojZ43o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d91f934-02e4-4f8f-de1c-06e6df726675"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdjsOX9naA-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98982dbc-fdd4-46cb-a268-d17516645fd4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkMgQEmaPEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b496f6c9-2fe6-4a7c-b50a-75bd29398821"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LflYZlN3aYSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ddf3c3-9514-4001-b900-b6b174fc52bb"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25uSDxqadmc"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_j9f6Qjami9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "d18ce68e-d175-4357-adc0-ad436d1d1ac8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "8120            ad03      0           *   \n",
              "3416            l-93      1         NaN   \n",
              "8185            ad03      1         NaN   \n",
              "491             bc01      1         NaN   \n",
              "7701            ad03      0           *   \n",
              "2592            l-93      1         NaN   \n",
              "7420           sks13      0           *   \n",
              "3545            ks08      1         NaN   \n",
              "5140            ks08      0           *   \n",
              "532             bc01      1         NaN   \n",
              "\n",
              "                                               sentence  \n",
              "8120  Who did that Plato loved prove to be his undoing.  \n",
              "3416  The phone company billed me $10 for that phone...  \n",
              "8185    Alison and David soaked their feet after dinner  \n",
              "491                                    John is refused.  \n",
              "7701                Gilgamesh doesn't be in the dungeon  \n",
              "2592             Jessica sprayed paint under the table.  \n",
              "7420                    John studies carefully Russian.  \n",
              "3545                     They read the scientific book.  \n",
              "5140                 It is Kim on whom Sandy relies on.  \n",
              "532           Fruit at once hit the roof from the tree.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-941c81a3-d37d-4bff-bf02-c589284f0fa6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8120</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Who did that Plato loved prove to be his undoing.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3416</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The phone company billed me $10 for that phone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8185</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alison and David soaked their feet after dinner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John is refused.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7701</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Gilgamesh doesn't be in the dungeon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2592</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jessica sprayed paint under the table.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7420</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>John studies carefully Russian.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They read the scientific book.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5140</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>It is Kim on whom Sandy relies on.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fruit at once hit the roof from the tree.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-941c81a3-d37d-4bff-bf02-c589284f0fa6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-941c81a3-d37d-4bff-bf02-c589284f0fa6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-941c81a3-d37d-4bff-bf02-c589284f0fa6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bacae0c3-b1d5-4bbe-93b3-e2fed6a7a1c3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bacae0c3-b1d5-4bbe-93b3-e2fed6a7a1c3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bacae0c3-b1d5-4bbe-93b3-e2fed6a7a1c3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"l-93\",\n          \"ks08\",\n          \"bc01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"It is Kim on whom Sandy relies on.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGFvVN-aqY5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "eae51a39-7cc6-46b0-8f47-951a7c820634"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  label\n",
              "1514    I never met that man who anybody tried to kill.      0\n",
              "1243  Myron is making Suzie's claim that dead is bet...      0\n",
              "7398                          They are intense of Bill.      0\n",
              "7349                                   I handed to Reg.      0\n",
              "1489  Who is Ann going to send a picture to her teac...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26de1fcc-18e3-4c52-a14b-148d863b4d42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1514</th>\n",
              "      <td>I never met that man who anybody tried to kill.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>Myron is making Suzie's claim that dead is bet...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7398</th>\n",
              "      <td>They are intense of Bill.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7349</th>\n",
              "      <td>I handed to Reg.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>Who is Ann going to send a picture to her teac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26de1fcc-18e3-4c52-a14b-148d863b4d42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26de1fcc-18e3-4c52-a14b-148d863b4d42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26de1fcc-18e3-4c52-a14b-148d863b4d42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c10088e2-eb88-43f2-8296-b3a8a1905e44\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c10088e2-eb88-43f2-8296-b3a8a1905e44')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c10088e2-eb88-43f2-8296-b3a8a1905e44 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Myron is making Suzie's claim that dead is better than red.\",\n          \"Who is Ann going to send a picture to her teacher of, as soon as she gets home?\",\n          \"They are intense of Bill.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxfCoezXau5n"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Kj3sWgaxan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dc1dde-6e19-486a-a319-f24204b9478d"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8T8JMCma1OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0b9865-853a-42b3-e916-6e11d7dd3a08"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tm_8o_a54i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a182c72-4358-4dae-d626-3c042ae2ff35"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V80hCa9CbEOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d48b3a4-4d92-4adc-e800-e9210c263975"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU5-MHxHbJrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a26d6c9-6f4e-488f-ff29-8bbadba03a2b"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 10\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 10 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6e3Oe-JbNJM"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcSGMwoFbRAM"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49qrloHrbTmE"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TFSrS7bXI-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0jiGwk3batl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a12368-8b7b-4763-e74b-cba2d01899c0"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoerOh8qbegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1633137-95d3-4a07-b0d4-d55d129f62cb"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DfOPFdbkI3"
      },
      "source": [
        "#Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hliUyfZsboiy"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXsLGIi6bvl-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcJnBTsbyoJ"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2YjFs8gb1hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42cc6f28-a1c2-422e-d2a4-1725d7200256"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:00:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:14.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:17\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:14.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:00:17\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:09.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:14.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:00:17\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdFfKyoLb7Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "fc88072a-8f8c-48da-d74f-6cbf118532a1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-N3fVYKcApy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f469ef0-ca2b-4642-a7a7-3c1fc688e1d0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNF30U8FcITz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2361b823-10ca-434d-c0a2-eb0dbe8f179b"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTk5LY4cLfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca39ba93-306b-48d9-ec85-4e8d4b1e6007"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy_SFwR8cQ58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6575e14-5ad6-4384-e918-622442f300eb"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-MCNw0cTCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdabe90-6f62-4d64-b1c9-ee2a0f27b188"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(0.21713222235566895),\n",
              " 0.0,\n",
              " np.float64(0.4177371279974369),\n",
              " np.float64(0.4127594582445936),\n",
              " np.float64(0.3567530340063379),\n",
              " np.float64(0.6979824404521128),\n",
              " np.float64(0.28941438070815306),\n",
              " 0.0,\n",
              " np.float64(0.9165151389911681),\n",
              " np.float64(0.6146362971528592),\n",
              " np.float64(0.8459051693633014),\n",
              " np.float64(0.647150228929434),\n",
              " np.float64(0.6979824404521128),\n",
              " np.float64(0.9078412990032037),\n",
              " np.float64(0.5447047794019222),\n",
              " np.float64(0.49382916465843113),\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbT-rwHocVqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12c8079-a8b2-4aef-df76-018f277d1c03"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론 코드"
      ],
      "metadata": {
        "id": "xd5BzXwU8HM1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5avgvjcYXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37203ca5-e34b-40b3-bc3b-4d760f6e79dc"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# 1. 모델 및 토크나이저 로드 (예: 'bert-base-uncased' 사용)\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.cuda()               # GPU로 이동\n",
        "model.eval()               # 평가 모드로 전환\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "# 2. 추론할 영어 문장 예시\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "\n",
        "# 3. 문장 토큰화 및 인코딩\n",
        "inputs = tokenizer.encode_plus(\n",
        "    sentence,\n",
        "    add_special_tokens=True,      # [CLS]와 [SEP] 토큰 추가\n",
        "    max_length=10,                # 최대 길이 설정\n",
        "    padding=\"max_length\",         # 최대 길이에 맞춰 패딩\n",
        "    truncation=True,              # 길면 잘라냄\n",
        "    return_tensors=\"pt\"           # 파이토치 텐서 반환\n",
        ")\n",
        "\n",
        "# 4. 텐서를 GPU로 이동\n",
        "inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "# 5. 추론 (forward pass)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs[0]  # 모델의 출력은 튜플이며, 첫 번째 요소가 로짓입니다.\n",
        "\n",
        "# 6. Softmax 적용 (선택 사항) 및 예측 클래스 결정\n",
        "probs = torch.softmax(logits, dim=1)\n",
        "predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# 7. 결과 출력\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", predicted_class.item())\n",
        "print(\"예측 확률:\", probs)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 1\n",
            "예측 확률: tensor([[0.4742, 0.5258]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    입력된 문장에 대해 학습된 모델을 사용하여 예측을 수행합니다.\n",
        "    Returns:\n",
        "        predicted_class: 예측된 클래스 (예: 0 또는 1)\n",
        "        probs: 각 클래스의 확률 (numpy 배열)\n",
        "    \"\"\"\n",
        "    # 문장을 토큰화하고 encode_plus를 통해 [CLS], [SEP] 토큰을 추가하며, 패딩/자르기 적용\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,      # [CLS]와 [SEP] 추가\n",
        "        max_length=max_length,        # 최대 길이\n",
        "        padding=\"max_length\",         # 최대 길이에 맞게 패딩\n",
        "        truncation=True,              # 길면 자르기\n",
        "        return_tensors=\"pt\"           # PyTorch 텐서 반환\n",
        "    )\n",
        "\n",
        "    # GPU로 텐서를 전송\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # 모델을 평가 모드로 전환하고, 추론 시에는 기울기를 계산하지 않도록 설정\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]  # 모델의 출력은 튜플 형태로, 첫 번째 요소가 logits입니다.\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 사용\n",
        "sentence = \"This is an example sentence for inference.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PITHYJKhPHD",
        "outputId": "41695e34-05f2-4360-b53e-d65566616c1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: This is an example sentence for inference.\n",
            "예측된 클래스: 1\n",
            "예측 확률: [[0.47422963 0.52577037]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cordic 코드"
      ],
      "metadata": {
        "id": "oK0EdSFZjeh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def top_1200_input(attention_scores):\n",
        "    \"\"\"\n",
        "    1200개의 attention_scores를 120개의 리스트로 나누어 각 리스트를 top 함수에 전달\n",
        "    결과는 원본 형태 (1, 12, 10, 10)로 반환\n",
        "    \"\"\"\n",
        "    if isinstance(attention_scores, torch.Tensor):\n",
        "        attention_scores = attention_scores.detach().cpu().numpy()  # ✅ detach() 추가\n",
        "\n",
        "    batch_size, num_heads, seq_length, _ = attention_scores.shape\n",
        "    result_arrays = np.zeros((batch_size, num_heads, seq_length, seq_length))\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        for row in range(seq_length):\n",
        "            for col in range(seq_length // 10):\n",
        "                input_values = attention_scores[0, head, row, col * 10:(col + 1) * 10]\n",
        "                result = top(*input_values)\n",
        "                result_arrays[0, head, row, col * 10:(col + 1) * 10] = result\n",
        "\n",
        "    # ✅ numpy -> torch 변환할 때 `.to(device)` 추가\n",
        "    return torch.tensor(result_arrays, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def float_to_fixed_point(value, int_bits=7, frac_bits=13):\n",
        "    \"\"\"\n",
        "    실수를 32비트 Fixed-Point 바이너리로 변환하는 함수.\n",
        "    - 정수부는 7비트 (Signed, 2의 보수)\n",
        "    - 소수부는 13비트 (항상 양수)\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ PyTorch Tensor 처리\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        value = value.detach().cpu().numpy()\n",
        "\n",
        "    # ✅ NaN 또는 Inf 값 체크 후 예외 처리\n",
        "    if np.isnan(value) or np.isinf(value):\n",
        "        raise ValueError(f\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\")\n",
        "\n",
        "    # ✅ **최대/최소 값 제한 (7비트 표현 범위)**\n",
        "    value = max(min(value, 63), -64)\n",
        "\n",
        "    # ✅ 정수부와 소수부 분리\n",
        "    int_part = np.floor(value).astype(int)\n",
        "    frac_part = abs(value - int_part)  # 소수 부분\n",
        "\n",
        "    # ✅ 2의 보수 변환 (음수 처리)\n",
        "    if int_part < 0:\n",
        "        int_part = (1 << int_bits) + int_part\n",
        "\n",
        "    int_binary = format(int_part, f'0{int_bits}b')\n",
        "\n",
        "    # ✅ 12비트 0 추가 (BERT 출력 형식 유지)\n",
        "    int_binary = \"0\" * 12 + int_binary\n",
        "\n",
        "    # ✅ 소수부 변환 (13비트)\n",
        "    frac_binary = \"\"\n",
        "    for _ in range(frac_bits):\n",
        "        frac_part *= 2\n",
        "        if frac_part >= 1:\n",
        "            frac_binary += \"1\"\n",
        "            frac_part -= 1\n",
        "        else:\n",
        "            frac_binary += \"0\"\n",
        "\n",
        "    # ✅ 최종 32비트 바이너리 문자열 생성\n",
        "    binary_string = int_binary + frac_binary\n",
        "\n",
        "    # ✅ `binary_string`이 음수 값을 포함하는지 확인 후 처리\n",
        "    if \"-\" in binary_string:\n",
        "        raise ValueError(f\"[ERROR] 잘못된 바이너리 문자열 변환 감지: {binary_string}\")\n",
        "\n",
        "    # ✅ 20비트 정수 변환 (부호 처리)\n",
        "    fixed_binary = int(binary_string, 2)\n",
        "    if value < 0:\n",
        "        fixed_binary = (1 << 20) - fixed_binary  # 2의 보수 변환\n",
        "\n",
        "    lower_20_bits = fixed_binary & 0xFFFFF  # 20비트 마스킹\n",
        "\n",
        "    return lower_20_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def binary_to_decimal(binary_str):\n",
        "    # 16비트 중 앞 3비트는 정수부, 뒤 13비트는 소수부\n",
        "    int_part = int(binary_str[:3], 2)  # 정수부\n",
        "    frac_part = int(binary_str[3:], 2) / (2 ** 13)  # 소수부를 2^13으로 나눔\n",
        "    return int_part + frac_part  # 정수부와 소수부 합산\n",
        "\n",
        "def hex_to_binary(hex_str):\n",
        "    \"\"\"\n",
        "    16비트 헥스 값을 16비트 바이너리 문자열로 변환하는 함수\n",
        "\n",
        "    :param hex_str: 변환할 16비트 헥스 값 (예: \"1A2F\" 또는 0x1A2F)\n",
        "    :return: 16비트 바이너리 문자열 (예: \"0001101000101111\")\n",
        "    \"\"\"\n",
        "    # hex_str가 정수형일 경우 문자열로 변환\n",
        "    if isinstance(hex_str, int):\n",
        "        hex_str = format(hex_str, 'X')  # 16진수 대문자로 변환\n",
        "\n",
        "    # 16진수를 10진수로 변환 후, 16비트 이진수로 변환\n",
        "    binary_str = bin(int(hex_str, 16))[2:].zfill(16)\n",
        "    return binary_str\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "# 테스트 예제\n",
        "\n",
        "\n",
        "def top(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10) : #20bit data\n",
        "\n",
        "    data_11 = float_to_fixed_point(data_1)\n",
        "    data_22 = float_to_fixed_point(data_2)\n",
        "    data_33 = float_to_fixed_point(data_3)\n",
        "    data_44 = float_to_fixed_point(data_4)\n",
        "    data_55 = float_to_fixed_point(data_5)\n",
        "    data_66 = float_to_fixed_point(data_6)\n",
        "    data_77 = float_to_fixed_point(data_7)\n",
        "    data_88 = float_to_fixed_point(data_8)\n",
        "    data_99 = float_to_fixed_point(data_9)\n",
        "    data_1010 = float_to_fixed_point(data_10)\n",
        "\n",
        "    data_list = [data_11, data_22, data_33, data_44, data_55, data_66, data_77, data_88, data_99, data_1010]\n",
        "    #data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    int_list = [0] * 10 #exp_int input\n",
        "    exp_int_output = [0]*10 #exp_int output\n",
        "    fraction_list=[0]*10 #cordic_exu input\n",
        "    exp_fraction_output = [0]*10 #cordic_exu output\n",
        "    exp_whole=[0]*10 #cordic_exu +cordic_int\n",
        "    exp_trunc=[0]*10\n",
        "    exp_accum_input=[0]*10 # accumulator input  (exponential)\n",
        "    exp_accum_output=[0]*1\n",
        "    x_divider = [0] * 10 #cordic diu input\n",
        "    y_dividend = [0] * 10 #cordic diu input\n",
        "    data_out = [0] * 10\n",
        "    binary_values = [0] * 10\n",
        "    int_values = [0.0] * 10\n",
        "    for i in range(10):  # int part\n",
        "        int_list[i] = data_list[i] >> 13\n",
        "    exp_int_output[0], exp_int_output[1], exp_int_output[2], exp_int_output[3], exp_int_output[4], exp_int_output[5], exp_int_output[6], exp_int_output[7], exp_int_output[8], exp_int_output[9]=exp_int(int_list[0], int_list[1], int_list[2], int_list[3], int_list[4], int_list[5], int_list[6], int_list[7], int_list[8], int_list[9])\n",
        "\n",
        "    for i in range(10):  # fraction part\n",
        "        fraction_list[i] = (data_list[i] & 0x1FFF) << 1  # 하위 13비트를 가져오고, 뒤에 0을 추가\n",
        "    exp_fraction_output[0], exp_fraction_output[1], exp_fraction_output[2], exp_fraction_output[3], exp_fraction_output[4], exp_fraction_output[5], exp_fraction_output[6], exp_fraction_output[7], exp_fraction_output[8], exp_fraction_output[9] = cordic_exu(fraction_list[0], fraction_list[1], fraction_list[2], fraction_list[3], fraction_list[4], fraction_list[5], fraction_list[6], fraction_list[7], fraction_list[8], fraction_list[9])\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_whole[i] = exp_int_output[i] * exp_fraction_output[i]\n",
        "\n",
        "    for i in range(10):\n",
        "       exp_trunc[i] = (exp_whole[i] >> 14) & 0xFFFFF  # 33~14 비트를 추출 (20비트)\n",
        "    for i in range(10):\n",
        "       exp_accum_input[i] = (exp_trunc[i] >> 4) & 0xFFFF\n",
        "\n",
        "    exp_accum_output[0] = cordic_accumulator(exp_accum_input[0],exp_accum_input[1],exp_accum_input[2],exp_accum_input[3],exp_accum_input[4],exp_accum_input[5],exp_accum_input[6],exp_accum_input[7],exp_accum_input[8],exp_accum_input[9])\n",
        "\n",
        "\n",
        "\n",
        "    # denominator_sf[i]에서 18~4 비트 추출 후, 앞에 0을 붙여서 x_divider에 할당\n",
        "    x_divider[0] = (exp_accum_output[0] >> 4) & 0x7FFF  # 18~4 비트를 추출 (15비트)\n",
        "\n",
        "    for i in range(10):\n",
        "    # numerator_sf[i]에서 15~4 비트 추출 후, 앞에 4개의 0을 붙여서 y_dividend에 할당\n",
        "        y_dividend[i] = (exp_accum_input[i] >> 4)  # 15~4 비트를 추출 (12비트)\n",
        "\n",
        "    data_out[0],data_out[1],data_out[2],data_out[3],data_out[4],data_out[5],data_out[6],data_out[7],data_out[8],data_out[9], = cordic_diu (x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],x_divider[0],y_dividend[0],y_dividend[1],y_dividend[2],y_dividend[3],y_dividend[4],y_dividend[5],y_dividend[6],y_dividend[7],y_dividend[8],y_dividend[9])\n",
        "\n",
        "    for i in range(10):\n",
        "        binary_values[i] = hex_to_binary(data_out[i])\n",
        "       # print(binary_values[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        int_values[i] = binary_to_decimal(binary_values[i])\n",
        "\n",
        "    return int_values[0],int_values[1],int_values[2],int_values[3],int_values[4],int_values[5],int_values[6],int_values[7],int_values[8],int_values[9]\n",
        "\n",
        "def exp_int(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_to_int = [0]*10\n",
        "    int_output=[0]*10\n",
        "    for i in range(10):\n",
        "        # 이진수로 변환하여 signed_binary_to_int 함수에 전달\n",
        "        binary_str = bin(data_list[i])[2:].zfill(7)  # 20비트 이진수 문자열로 변환\n",
        "        data_to_int[i] = signed_binary_to_int(binary_str)\n",
        "\n",
        "    i_max = max(data_to_int)\n",
        "    offset=signed_binary_to_int(bin(0b0001011-i_max)[2:].zfill(7)) #offset\n",
        "\n",
        "    for i in range(10):\n",
        "        int_reg=data_to_int[i]+offset\n",
        "        #print(int_reg)\n",
        "        if int_reg==0 :\n",
        "            int_output[i]=0b0000_0000_0000_0001_00\n",
        "        elif int_reg==1 :\n",
        "            int_output[i]=0b0000_0000_0000_0010_11\n",
        "        elif int_reg==2 :\n",
        "            int_output[i]=0b0000_0000_0000_0111_10\n",
        "        elif int_reg==3 :\n",
        "            int_output[i]=0b0000_0000_0001_0100_00\n",
        "        elif int_reg==4 :\n",
        "            int_output[i]=0b0000_0000_0011_0110_10\n",
        "        elif int_reg==5 :\n",
        "            int_output[i]=0b0000_0000_1001_0100_10\n",
        "        elif int_reg==6 :\n",
        "            int_output[i]=0b0000_0001_1001_0011_10\n",
        "        elif int_reg==7 :\n",
        "            int_output[i]=0b0000_0100_0100_1000_11\n",
        "        elif int_reg==8 :\n",
        "            int_output[i]=0b0000_1011_1010_0101_00\n",
        "        elif int_reg==9 :\n",
        "            int_output[i]=0b0001_1111_1010_0111_00\n",
        "        elif int_reg==10 :\n",
        "            int_output[i]=0b0101_0110_0000_1010_10\n",
        "        elif int_reg==11 :\n",
        "            int_output[i]=0b1110_1001_1110_0010_01\n",
        "        else :\n",
        "            int_output[i]=0\n",
        "\n",
        "    return  int_output[0], int_output[1], int_output[2], int_output[3], int_output[4], int_output[5], int_output[6], int_output[7], int_output[8], int_output[9]\n",
        "\n",
        "\n",
        "def cordic_exu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_exu_pipe(data_list[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "def cordic_exu_pipe(z_in):\n",
        "    x_1, y_1, z_1=exp_unit(0b1001101010001111, 0, z_in, 0b0010001100100111,1)\n",
        "    x_2, y_2, z_2=exp_unit(x_1, y_1, z_1,0b0001000001011000,2)\n",
        "    x_3, y_3, z_3 = exp_unit(x_2, y_2, z_2, 0b0000100000001010, 3)\n",
        "    x_4, y_4, z_4 = exp_unit(x_3, y_3, z_3, 0b0000010000000001, 4)\n",
        "    x_5, y_5, z_5 = exp_unit(x_4, y_4, z_4, 0b0000001000000000, 5)\n",
        "    x_6, y_6, z_6 = exp_unit(x_5, y_5, z_5, 0b0000000100000000, 6)\n",
        "    x_7, y_7, z_7 = exp_unit(x_6, y_6, z_6, 0b0000000010000000, 7)\n",
        "    x_8, y_8, z_8 = exp_unit(x_7, y_7, z_7, 0b0000000001000000, 8)\n",
        "    x_9, y_9, z_9 = exp_unit(x_8, y_8, z_8, 0b0000000000100000, 9)\n",
        "    x_10, y_10, z_10 = exp_unit(x_9, y_9, z_9, 0b0000000000010000, 10)\n",
        "    x_11, y_11, z_11 = exp_unit(x_10, y_10, z_10, 0b0000000000001000, 11)\n",
        "    x_12, y_12, z_12 = exp_unit(x_11, y_11, z_11, 0b0000000000000100, 12)\n",
        "    x_13, y_13, z_13 = exp_unit(x_12, y_12, z_12, 0b0000000000000010, 13)\n",
        "    x_14, y_14, z_14 = exp_unit(x_13, y_13, z_13, 0b0000000000000010, 14)\n",
        "\n",
        "\n",
        "    exp_frac_reg=x_14+y_14\n",
        "    exp_frac = (exp_frac_reg >> 1) & 0xFFFF\n",
        "    #print(bin(exp_frac))\n",
        "    return exp_frac\n",
        "\n",
        "\n",
        "\n",
        "def exp_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "    x_cal = 0\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if z_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "        # 시계방향 (downward)\n",
        "        x_cal = x_in - (y_in >> pipe_num)\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "    else:\n",
        "        # 반시계방향 (upward)\n",
        "        x_cal = x_in + (y_in >> pipe_num)\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return x_cal, y_cal, z_cal\n",
        "\n",
        "\n",
        "def signed_binary_to_int(binary_str):\n",
        "    \"\"\"\n",
        "    2의 보수 표현된 이진수를 정수로 변환하는 함수.\n",
        "    \"\"\"\n",
        "    # ✅ \"0b\" 제거\n",
        "    binary_str = binary_str.replace(\"0b\", \"\")\n",
        "\n",
        "    # ✅ 이진수 길이 확인\n",
        "    n = len(binary_str)\n",
        "\n",
        "    # ✅ 부호 비트 확인 (최상위 비트가 1이면 음수)\n",
        "    if binary_str[0] == '1':\n",
        "        # 2의 보수 변환 (음수)\n",
        "        return int(binary_str, 2) - (1 << n)\n",
        "    else:\n",
        "        # 양수 변환\n",
        "        return int(binary_str, 2)\n",
        "\n",
        "\n",
        "def accumulator(sequence_in, exponential, state):\n",
        "    \"\"\"\n",
        "    입력값을 받아 sum1, sum2, denominator_sf 값을 업데이트하고 출력.\n",
        "    \"\"\"\n",
        "\n",
        "    # accumulator_SEL 토글\n",
        "    state[\"accumulator_SEL\"] = (state[\"accumulator_SEL\"] + sequence_in) & 1\n",
        "\n",
        "    # valid 시프트 레지스터 갱신\n",
        "    state[\"valid\"] = [sequence_in] + state[\"valid\"][:-1]\n",
        "\n",
        "    if state[\"accumulator_SEL\"]:  # accumulator 1 사용\n",
        "        state[\"sum1\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum2\"] = exponential  # sum2 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum1\"]  # denominator_sf 업데이트\n",
        "    else:  # accumulator 2 사용\n",
        "        state[\"sum2\"] += exponential\n",
        "        if sequence_in:\n",
        "            state[\"sum1\"] = exponential  # sum1 리셋\n",
        "        if state[\"valid\"][9]:\n",
        "            state[\"denominator_sf\"] = state[\"sum2\"]  # denominator_sf 업데이트\n",
        "\n",
        "    sequence_out = state[\"valid\"][0]\n",
        "\n",
        "    return  state[\"denominator_sf\"]\n",
        "\n",
        "\n",
        "def cordic_accumulator (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10):\n",
        "    data_list = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    state_accumulator = {\n",
        "    \"denominator_sf\": 0,\n",
        "    \"accumulator_SEL\": 0,\n",
        "    \"valid\": [0] * 11,\n",
        "    \"sum1\": 0,\n",
        "    \"sum2\": 0\n",
        "}\n",
        "\n",
        "    exp_fraction[0]=  accumulator(1,data_list[0],state_accumulator)\n",
        "    exp_fraction[1]=  accumulator( 0 ,data_list[1],state_accumulator)\n",
        "    exp_fraction[2]=  accumulator( 0 ,data_list[2],state_accumulator)\n",
        "    exp_fraction[3]=  accumulator( 0 ,data_list[3],state_accumulator)\n",
        "    exp_fraction[4]=  accumulator( 0 ,data_list[4],state_accumulator)\n",
        "    exp_fraction[5]=  accumulator( 0 ,data_list[5],state_accumulator)\n",
        "    exp_fraction[6]=  accumulator( 0 ,data_list[6],state_accumulator)\n",
        "    exp_fraction[7]=  accumulator( 0 ,data_list[7],state_accumulator)\n",
        "    exp_fraction[8]=  accumulator( 0 ,data_list[8],state_accumulator)\n",
        "    exp_fraction[9]=  accumulator( 0 ,data_list[9],state_accumulator)\n",
        "    #print(hex(exp_fraction[9]))\n",
        "\n",
        "    return  exp_fraction[9]\n",
        "\n",
        "\n",
        "\n",
        "def fifo(exponential, state):\n",
        "\n",
        "\n",
        "        # FIFO 시프트 (FIFO_reg[i] = FIFO_reg[i-1])\n",
        "    state[\"FIFO_reg\"] = [exponential] + state[\"FIFO_reg\"][:-1]\n",
        "\n",
        "    numerator_sf = state[\"FIFO_reg\"][9]\n",
        "\n",
        "\n",
        "    return numerator_sf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def div_unit(x_in, y_in, z_in, lut_val, pipe_num):\n",
        "    # 16-bit 입력 값\n",
        "\n",
        "    y_cal = 0\n",
        "    z_cal = 0\n",
        "\n",
        "    if y_in & (1 << 15):  # z_in[15] = MSB를 확인 (시계방향: z_in[15]가 1)\n",
        "\n",
        "\n",
        "        y_cal = y_in + (x_in >> pipe_num)\n",
        "        z_cal = z_in - lut_val\n",
        "    else:\n",
        "\n",
        "\n",
        "        y_cal = y_in - (x_in >> pipe_num)\n",
        "        z_cal = z_in + lut_val\n",
        "\n",
        "    if z_cal < 0:\n",
        "        # 음수일 경우 2의 보수를 구하고 16비트로 마스킹\n",
        "        z_cal = (z_cal + (1 << 16)) & 0xFFFF\n",
        "    else:\n",
        "        z_cal = z_cal & 0xFFFF\n",
        "    # 결과값 반환 (튜플로 출력)\n",
        "\n",
        "    return  y_cal, z_cal\n",
        "def cordic_diu_pipe(x_in, y_in):\n",
        "    y_1, z_1 = div_unit(x_in, y_in, 0, 0b0010000000000000, 0)\n",
        "    y_2, z_2 = div_unit(x_in, y_1, z_1, 0b0001000000000000, 1)\n",
        "    y_3, z_3 = div_unit(x_in, y_2, z_2, 0b0000100000000000, 2)\n",
        "    y_4, z_4 = div_unit(x_in, y_3, z_3, 0b0000010000000000, 3)\n",
        "    y_5, z_5 = div_unit(x_in, y_4, z_4, 0b0000001000000000, 4)\n",
        "    y_6, z_6 = div_unit(x_in, y_5, z_5, 0b0000000100000000, 5)\n",
        "    y_7, z_7 = div_unit(x_in, y_6, z_6, 0b0000000010000000, 6)\n",
        "    y_8, z_8 = div_unit(x_in, y_7, z_7, 0b0000000001000000, 7)\n",
        "    y_9, z_9 = div_unit(x_in, y_8, z_8, 0b0000000000100000, 8)\n",
        "    y_10, z_10 = div_unit(x_in, y_9, z_9, 0b0000000000010000, 9)\n",
        "    y_11, z_11 = div_unit(x_in, y_10, z_10, 0b0000000000001000, 10)\n",
        "    y_12, z_12 = div_unit(x_in, y_11, z_11, 0b0000000000000100, 11)\n",
        "    y_13, z_13 = div_unit(x_in, y_12, z_12, 0b0000000000000010, 12)\n",
        "    y_14, z_14 = div_unit(x_in, y_13, z_13, 0b0000000000000001, 13)\n",
        "    divided_val = z_14  # Final z value representing atan(y/x)\n",
        "    return divided_val\n",
        "\n",
        "def cordic_diu (data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10,data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20):\n",
        "    data_list_x = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
        "    data_list_y = [data_11, data_12, data_13, data_14, data_15, data_16, data_17, data_18, data_19, data_20]\n",
        "    exp_fraction =[0]*10\n",
        "\n",
        "    for i in range(10):\n",
        "        exp_fraction[i]=  cordic_diu_pipe(data_list_x[i],data_list_y[i])\n",
        "        #print(bin(data_list[i]))\n",
        "\n",
        "    return exp_fraction[0], exp_fraction[1], exp_fraction[2], exp_fraction[3], exp_fraction[4], exp_fraction[5], exp_fraction[6], exp_fraction[7], exp_fraction[8], exp_fraction[9]\n",
        "\n",
        "\n",
        "top(1.548621,\n",
        "2.154268,\n",
        "0.15548,\n",
        "1.358942,\n",
        "3.54985,\n",
        "2.1578712,\n",
        "-1.58952,\n",
        "-0.248623,\n",
        "1.6685123,\n",
        "-2.68856\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JuXVpD_Fjj7q",
        "outputId": "64c79585-e718-4f97-d2e0-7117fc3aa8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0528564453125,\n",
              " 0.0977783203125,\n",
              " 0.0128173828125,\n",
              " 0.0438232421875,\n",
              " 0.3963623046875,\n",
              " 0.0980224609375,\n",
              " 0.0552978515625,\n",
              " 0.0140380859375,\n",
              " 0.0599365234375,\n",
              " 0.1666259765625)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 수정 및 적용 코드"
      ],
      "metadata": {
        "id": "o19OXrVjjoaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "scMdkDy7tAqU",
        "outputId": "94a5a623-027d-4e54-beea-13ed430b7aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassificationModified(BertForSequenceClassification):\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Softmax 대신 CORDIC을 적용\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        return logits, probs\n"
      ],
      "metadata": {
        "id": "XySJ0nStsisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "\n",
        "        # 여기서 원래 softmax를 적용하는 대신 CORDIC 기반 함수를 사용합니다.\n",
        "        # 예를 들어, top_1200_input(attention_scores)를 사용하여 softmax 결과를 근사합니다.\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        # 만약 반환된 결과가 numpy 형태라면, torch.tensor로 변환해주어야 합니다.\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 원래 출력은 (context_layer, attention_probs) 또는 (context_layer,)인데,\n",
        "        # 필요에 따라 raw attention scores도 반환하도록 할 수 있습니다.\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "# 만약 학습된 가중치를 로드해야 한다면 로드합니다.\n",
        "# model.load_state_dict(torch.load(\"your_checkpoint.pt\"), strict=False)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# 이제 이 모델은 CoLA나 다른 영어 문장 분류 작업에 사용할 수 있습니다.\n",
        "# 예시 추론 함수:\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "def predict_sentence(model, tokenizer, sentence, max_length=10, device=\"cuda\"):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "    return predicted_class, probs.cpu().numpy()\n",
        "\n",
        "# 예시 문장으로 테스트\n",
        "sentence = \"This is a grammatically acceptable for sentence.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "print(\"입력 문장:\", sentence)\n",
        "print(\"예측된 클래스:\", pred_class)\n",
        "print(\"예측 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "itlqawMcjmJ3",
        "outputId": "284e5f8a-d05a-4a2e-92d3-7ebec63eaa6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10545\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 22688\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5433707936\n",
            "[TOP] exp_trunc[9] = 331647\n",
            "[TOP] exp_accum_input[9] = 20727\n",
            "[TOP] x_divider = 13294\n",
            "[TOP] y_dividend = 1295\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 18051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4323160347\n",
            "[TOP] exp_trunc[9] = 263864\n",
            "[TOP] exp_accum_input[9] = 16491\n",
            "[TOP] x_divider = 10986\n",
            "[TOP] y_dividend = 1030\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4733418708\n",
            "[TOP] exp_trunc[9] = 288904\n",
            "[TOP] exp_accum_input[9] = 18056\n",
            "[TOP] x_divider = 11030\n",
            "[TOP] y_dividend = 1128\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 36626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8771817122\n",
            "[TOP] exp_trunc[9] = 535389\n",
            "[TOP] exp_accum_input[9] = 33461\n",
            "[TOP] x_divider = 16307\n",
            "[TOP] y_dividend = 2091\n",
            "[TOP] data_out[9] = 1049\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11950\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 18976\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4544695072\n",
            "[TOP] exp_trunc[9] = 277386\n",
            "[TOP] exp_accum_input[9] = 17336\n",
            "[TOP] x_divider = 11789\n",
            "[TOP] y_dividend = 1083\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10760\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 18398\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4406265806\n",
            "[TOP] exp_trunc[9] = 268937\n",
            "[TOP] exp_accum_input[9] = 16808\n",
            "[TOP] x_divider = 10597\n",
            "[TOP] y_dividend = 1050\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 20790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4979142630\n",
            "[TOP] exp_trunc[9] = 303902\n",
            "[TOP] exp_accum_input[9] = 18993\n",
            "[TOP] x_divider = 12566\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10485\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 18111\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4337530167\n",
            "[TOP] exp_trunc[9] = 264741\n",
            "[TOP] exp_accum_input[9] = 16546\n",
            "[TOP] x_divider = 11099\n",
            "[TOP] y_dividend = 1034\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 24696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5914617912\n",
            "[TOP] exp_trunc[9] = 360999\n",
            "[TOP] exp_accum_input[9] = 22562\n",
            "[TOP] x_divider = 12992\n",
            "[TOP] y_dividend = 1410\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 17635\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4223529595\n",
            "[TOP] exp_trunc[9] = 257783\n",
            "[TOP] exp_accum_input[9] = 16111\n",
            "[TOP] x_divider = 11887\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4239336397\n",
            "[TOP] exp_trunc[9] = 258748\n",
            "[TOP] exp_accum_input[9] = 16171\n",
            "[TOP] x_divider = 10796\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 21152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5065840544\n",
            "[TOP] exp_trunc[9] = 309194\n",
            "[TOP] exp_accum_input[9] = 19324\n",
            "[TOP] x_divider = 13778\n",
            "[TOP] y_dividend = 1207\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 25950\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6214947150\n",
            "[TOP] exp_trunc[9] = 379330\n",
            "[TOP] exp_accum_input[9] = 23708\n",
            "[TOP] x_divider = 11736\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 1031\n",
            "[TOP] exp_fraction_output[9] = 21684\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5193252948\n",
            "[TOP] exp_trunc[9] = 316971\n",
            "[TOP] exp_accum_input[9] = 19810\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 33292\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2933224952\n",
            "[TOP] exp_trunc[9] = 179029\n",
            "[TOP] exp_accum_input[9] = 11189\n",
            "[TOP] x_divider = 6314\n",
            "[TOP] y_dividend = 699\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25942\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6213031174\n",
            "[TOP] exp_trunc[9] = 379213\n",
            "[TOP] exp_accum_input[9] = 23700\n",
            "[TOP] x_divider = 15370\n",
            "[TOP] y_dividend = 1481\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18622\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4459913134\n",
            "[TOP] exp_trunc[9] = 272211\n",
            "[TOP] exp_accum_input[9] = 17013\n",
            "[TOP] x_divider = 11129\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 32763\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7846640211\n",
            "[TOP] exp_trunc[9] = 478920\n",
            "[TOP] exp_accum_input[9] = 29932\n",
            "[TOP] x_divider = 15649\n",
            "[TOP] y_dividend = 1870\n",
            "[TOP] data_out[9] = 979\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 14685\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 581\n",
            "[TOP] exp_fraction_output[9] = 28649\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6861349553\n",
            "[TOP] exp_trunc[9] = 418783\n",
            "[TOP] exp_accum_input[9] = 26173\n",
            "[TOP] x_divider = 12575\n",
            "[TOP] y_dividend = 1635\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 15574\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 22161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5307493017\n",
            "[TOP] exp_trunc[9] = 323943\n",
            "[TOP] exp_accum_input[9] = 20246\n",
            "[TOP] x_divider = 13237\n",
            "[TOP] y_dividend = 1265\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 26216\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6278653352\n",
            "[TOP] exp_trunc[9] = 383218\n",
            "[TOP] exp_accum_input[9] = 23951\n",
            "[TOP] x_divider = 12267\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 42670\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10219336990\n",
            "[TOP] exp_trunc[9] = 623738\n",
            "[TOP] exp_accum_input[9] = 38983\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 2436\n",
            "[TOP] data_out[9] = 1213\n",
            "[TOP] exp_fraction_output[9] = 31661\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7582714517\n",
            "[TOP] exp_trunc[9] = 462812\n",
            "[TOP] exp_accum_input[9] = 28925\n",
            "[TOP] x_divider = 15707\n",
            "[TOP] y_dividend = 1807\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 28966\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6937270102\n",
            "[TOP] exp_trunc[9] = 423417\n",
            "[TOP] exp_accum_input[9] = 26463\n",
            "[TOP] x_divider = 14162\n",
            "[TOP] y_dividend = 1653\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 24792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5937609624\n",
            "[TOP] exp_trunc[9] = 362402\n",
            "[TOP] exp_accum_input[9] = 22650\n",
            "[TOP] x_divider = 11674\n",
            "[TOP] y_dividend = 1415\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 25534\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6115316398\n",
            "[TOP] exp_trunc[9] = 373249\n",
            "[TOP] exp_accum_input[9] = 23328\n",
            "[TOP] x_divider = 12824\n",
            "[TOP] y_dividend = 1458\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 21825\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5227022025\n",
            "[TOP] exp_trunc[9] = 319032\n",
            "[TOP] exp_accum_input[9] = 19939\n",
            "[TOP] x_divider = 12345\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 23341\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5590099477\n",
            "[TOP] exp_trunc[9] = 341192\n",
            "[TOP] exp_accum_input[9] = 21324\n",
            "[TOP] x_divider = 13618\n",
            "[TOP] y_dividend = 1332\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 32814\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7858854558\n",
            "[TOP] exp_trunc[9] = 479666\n",
            "[TOP] exp_accum_input[9] = 29979\n",
            "[TOP] x_divider = 16724\n",
            "[TOP] y_dividend = 1873\n",
            "[TOP] data_out[9] = 917\n",
            "[TOP] exp_fraction_output[9] = 18204\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4359803388\n",
            "[TOP] exp_trunc[9] = 266101\n",
            "[TOP] exp_accum_input[9] = 16631\n",
            "[TOP] x_divider = 11169\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 18150\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4346870550\n",
            "[TOP] exp_trunc[9] = 265311\n",
            "[TOP] exp_accum_input[9] = 16581\n",
            "[TOP] x_divider = 11011\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18631\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4462068607\n",
            "[TOP] exp_trunc[9] = 272343\n",
            "[TOP] exp_accum_input[9] = 17021\n",
            "[TOP] x_divider = 10920\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 19265\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4613909705\n",
            "[TOP] exp_trunc[9] = 281610\n",
            "[TOP] exp_accum_input[9] = 17600\n",
            "[TOP] x_divider = 10973\n",
            "[TOP] y_dividend = 1100\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10828\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 18917\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4530564749\n",
            "[TOP] exp_trunc[9] = 276523\n",
            "[TOP] exp_accum_input[9] = 17282\n",
            "[TOP] x_divider = 10840\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 19613\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4697254661\n",
            "[TOP] exp_trunc[9] = 286697\n",
            "[TOP] exp_accum_input[9] = 17918\n",
            "[TOP] x_divider = 11668\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20196\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836881412\n",
            "[TOP] exp_trunc[9] = 295219\n",
            "[TOP] exp_accum_input[9] = 18451\n",
            "[TOP] x_divider = 12199\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 25509\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6109328973\n",
            "[TOP] exp_trunc[9] = 372883\n",
            "[TOP] exp_accum_input[9] = 23305\n",
            "[TOP] x_divider = 13562\n",
            "[TOP] y_dividend = 1456\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4288912276\n",
            "[TOP] exp_trunc[9] = 261774\n",
            "[TOP] exp_accum_input[9] = 16360\n",
            "[TOP] x_divider = 11053\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 18189\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356210933\n",
            "[TOP] exp_trunc[9] = 265882\n",
            "[TOP] exp_accum_input[9] = 16617\n",
            "[TOP] x_divider = 10892\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 779\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 13359\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12605\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 18477\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4425186069\n",
            "[TOP] exp_trunc[9] = 270091\n",
            "[TOP] exp_accum_input[9] = 16880\n",
            "[TOP] x_divider = 14014\n",
            "[TOP] y_dividend = 1055\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 14893\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 20107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4815566179\n",
            "[TOP] exp_trunc[9] = 293918\n",
            "[TOP] exp_accum_input[9] = 18369\n",
            "[TOP] x_divider = 10863\n",
            "[TOP] y_dividend = 1148\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 22856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5473943432\n",
            "[TOP] exp_trunc[9] = 334102\n",
            "[TOP] exp_accum_input[9] = 20881\n",
            "[TOP] x_divider = 14529\n",
            "[TOP] y_dividend = 1305\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 583\n",
            "[TOP] exp_fraction_output[9] = 18548\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4442190356\n",
            "[TOP] exp_trunc[9] = 271129\n",
            "[TOP] exp_accum_input[9] = 16945\n",
            "[TOP] x_divider = 11061\n",
            "[TOP] y_dividend = 1059\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4679531883\n",
            "[TOP] exp_trunc[9] = 285615\n",
            "[TOP] exp_accum_input[9] = 17850\n",
            "[TOP] x_divider = 11685\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 20461\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4900348117\n",
            "[TOP] exp_trunc[9] = 299093\n",
            "[TOP] exp_accum_input[9] = 18693\n",
            "[TOP] x_divider = 11478\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 26894\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6441032318\n",
            "[TOP] exp_trunc[9] = 393129\n",
            "[TOP] exp_accum_input[9] = 24570\n",
            "[TOP] x_divider = 11410\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 19860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4756410420\n",
            "[TOP] exp_trunc[9] = 290308\n",
            "[TOP] exp_accum_input[9] = 18144\n",
            "[TOP] x_divider = 10536\n",
            "[TOP] y_dividend = 1134\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11262\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 839\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11701\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20444\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4896276668\n",
            "[TOP] exp_trunc[9] = 298845\n",
            "[TOP] exp_accum_input[9] = 18677\n",
            "[TOP] x_divider = 11715\n",
            "[TOP] y_dividend = 1167\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 21815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5224627055\n",
            "[TOP] exp_trunc[9] = 318885\n",
            "[TOP] exp_accum_input[9] = 19930\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 21527\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5155651919\n",
            "[TOP] exp_trunc[9] = 314676\n",
            "[TOP] exp_accum_input[9] = 19667\n",
            "[TOP] x_divider = 12489\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185449572\n",
            "[TOP] exp_trunc[9] = 255459\n",
            "[TOP] exp_accum_input[9] = 15966\n",
            "[TOP] x_divider = 11698\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4569123766\n",
            "[TOP] exp_trunc[9] = 278877\n",
            "[TOP] exp_accum_input[9] = 17429\n",
            "[TOP] x_divider = 12182\n",
            "[TOP] y_dividend = 1089\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 17543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4201495871\n",
            "[TOP] exp_trunc[9] = 256438\n",
            "[TOP] exp_accum_input[9] = 16027\n",
            "[TOP] x_divider = 12255\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17766\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254903702\n",
            "[TOP] exp_trunc[9] = 259698\n",
            "[TOP] exp_accum_input[9] = 16231\n",
            "[TOP] x_divider = 10276\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 809\n",
            "[TOP] exp_fraction_output[9] = 20666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949445002\n",
            "[TOP] exp_trunc[9] = 302090\n",
            "[TOP] exp_accum_input[9] = 18880\n",
            "[TOP] x_divider = 10877\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11281\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 26040\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6236501880\n",
            "[TOP] exp_trunc[9] = 380645\n",
            "[TOP] exp_accum_input[9] = 23790\n",
            "[TOP] x_divider = 11992\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 18538\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4439795386\n",
            "[TOP] exp_trunc[9] = 270983\n",
            "[TOP] exp_accum_input[9] = 16936\n",
            "[TOP] x_divider = 11386\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 25020\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5992214940\n",
            "[TOP] exp_trunc[9] = 365735\n",
            "[TOP] exp_accum_input[9] = 22858\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1428\n",
            "[TOP] data_out[9] = 969\n",
            "[TOP] exp_fraction_output[9] = 18292\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380879124\n",
            "[TOP] exp_trunc[9] = 267387\n",
            "[TOP] exp_accum_input[9] = 16711\n",
            "[TOP] x_divider = 11272\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 18294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4381358118\n",
            "[TOP] exp_trunc[9] = 267416\n",
            "[TOP] exp_accum_input[9] = 16713\n",
            "[TOP] x_divider = 11240\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 23762\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5690927714\n",
            "[TOP] exp_trunc[9] = 347346\n",
            "[TOP] exp_accum_input[9] = 21709\n",
            "[TOP] x_divider = 11857\n",
            "[TOP] y_dividend = 1356\n",
            "[TOP] data_out[9] = 937\n",
            "[TOP] exp_fraction_output[9] = 29990\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7182515030\n",
            "[TOP] exp_trunc[9] = 438385\n",
            "[TOP] exp_accum_input[9] = 27399\n",
            "[TOP] x_divider = 12830\n",
            "[TOP] y_dividend = 1712\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 20088\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4811015736\n",
            "[TOP] exp_trunc[9] = 293641\n",
            "[TOP] exp_accum_input[9] = 18352\n",
            "[TOP] x_divider = 11929\n",
            "[TOP] y_dividend = 1147\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 26891\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6440313827\n",
            "[TOP] exp_trunc[9] = 393085\n",
            "[TOP] exp_accum_input[9] = 24567\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 1535\n",
            "[TOP] data_out[9] = 1071\n",
            "[TOP] exp_fraction_output[9] = 19436\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4654863692\n",
            "[TOP] exp_trunc[9] = 284110\n",
            "[TOP] exp_accum_input[9] = 17756\n",
            "[TOP] x_divider = 11596\n",
            "[TOP] y_dividend = 1109\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 18565\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4446261805\n",
            "[TOP] exp_trunc[9] = 271378\n",
            "[TOP] exp_accum_input[9] = 16961\n",
            "[TOP] x_divider = 11838\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 11210\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 25440\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6092803680\n",
            "[TOP] exp_trunc[9] = 371875\n",
            "[TOP] exp_accum_input[9] = 23242\n",
            "[TOP] x_divider = 12640\n",
            "[TOP] y_dividend = 1452\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 27778\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6652747666\n",
            "[TOP] exp_trunc[9] = 406051\n",
            "[TOP] exp_accum_input[9] = 25378\n",
            "[TOP] x_divider = 12939\n",
            "[TOP] y_dividend = 1586\n",
            "[TOP] data_out[9] = 1003\n",
            "[TOP] exp_fraction_output[9] = 19294\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4620855118\n",
            "[TOP] exp_trunc[9] = 282034\n",
            "[TOP] exp_accum_input[9] = 17627\n",
            "[TOP] x_divider = 11779\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 19476\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4664443572\n",
            "[TOP] exp_trunc[9] = 284695\n",
            "[TOP] exp_accum_input[9] = 17793\n",
            "[TOP] x_divider = 12994\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17528\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4197903416\n",
            "[TOP] exp_trunc[9] = 256219\n",
            "[TOP] exp_accum_input[9] = 16013\n",
            "[TOP] x_divider = 12928\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 22511\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5391316967\n",
            "[TOP] exp_trunc[9] = 329059\n",
            "[TOP] exp_accum_input[9] = 20566\n",
            "[TOP] x_divider = 13605\n",
            "[TOP] y_dividend = 1285\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 19154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4587325538\n",
            "[TOP] exp_trunc[9] = 279988\n",
            "[TOP] exp_accum_input[9] = 17499\n",
            "[TOP] x_divider = 11381\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4191915991\n",
            "[TOP] exp_trunc[9] = 255854\n",
            "[TOP] exp_accum_input[9] = 15990\n",
            "[TOP] x_divider = 10757\n",
            "[TOP] y_dividend = 999\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 18288\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4379921136\n",
            "[TOP] exp_trunc[9] = 267329\n",
            "[TOP] exp_accum_input[9] = 16708\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 19604\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4695099188\n",
            "[TOP] exp_trunc[9] = 286566\n",
            "[TOP] exp_accum_input[9] = 17910\n",
            "[TOP] x_divider = 11040\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18929\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4533438713\n",
            "[TOP] exp_trunc[9] = 276699\n",
            "[TOP] exp_accum_input[9] = 17293\n",
            "[TOP] x_divider = 11820\n",
            "[TOP] y_dividend = 1080\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 19510\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4672586470\n",
            "[TOP] exp_trunc[9] = 285192\n",
            "[TOP] exp_accum_input[9] = 17824\n",
            "[TOP] x_divider = 10950\n",
            "[TOP] y_dividend = 1114\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 23850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5712003450\n",
            "[TOP] exp_trunc[9] = 348633\n",
            "[TOP] exp_accum_input[9] = 21789\n",
            "[TOP] x_divider = 12004\n",
            "[TOP] y_dividend = 1361\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 17605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4216344685\n",
            "[TOP] exp_trunc[9] = 257345\n",
            "[TOP] exp_accum_input[9] = 16084\n",
            "[TOP] x_divider = 10933\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4255861690\n",
            "[TOP] exp_trunc[9] = 259757\n",
            "[TOP] exp_accum_input[9] = 16234\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21731\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5204509307\n",
            "[TOP] exp_trunc[9] = 317658\n",
            "[TOP] exp_accum_input[9] = 19853\n",
            "[TOP] x_divider = 11488\n",
            "[TOP] y_dividend = 1240\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 17835\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271428995\n",
            "[TOP] exp_trunc[9] = 260707\n",
            "[TOP] exp_accum_input[9] = 16294\n",
            "[TOP] x_divider = 11475\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 18844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4513081468\n",
            "[TOP] exp_trunc[9] = 275456\n",
            "[TOP] exp_accum_input[9] = 17216\n",
            "[TOP] x_divider = 11140\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585170065\n",
            "[TOP] exp_trunc[9] = 279856\n",
            "[TOP] exp_accum_input[9] = 17491\n",
            "[TOP] x_divider = 12620\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11781\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12720\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10502\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 763\n",
            "[TOP] exp_fraction_output[9] = 26868\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6434805396\n",
            "[TOP] exp_trunc[9] = 392749\n",
            "[TOP] exp_accum_input[9] = 24546\n",
            "[TOP] x_divider = 16212\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 17779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4258017163\n",
            "[TOP] exp_trunc[9] = 259888\n",
            "[TOP] exp_accum_input[9] = 16243\n",
            "[TOP] x_divider = 11161\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 19338\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4631392986\n",
            "[TOP] exp_trunc[9] = 282677\n",
            "[TOP] exp_accum_input[9] = 17667\n",
            "[TOP] x_divider = 12405\n",
            "[TOP] y_dividend = 1104\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 24236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5804449292\n",
            "[TOP] exp_trunc[9] = 354275\n",
            "[TOP] exp_accum_input[9] = 22142\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 1383\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 23326\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586507022\n",
            "[TOP] exp_trunc[9] = 340973\n",
            "[TOP] exp_accum_input[9] = 21310\n",
            "[TOP] x_divider = 12019\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 907\n",
            "[TOP] exp_fraction_output[9] = 17198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4118869406\n",
            "[TOP] exp_trunc[9] = 251395\n",
            "[TOP] exp_accum_input[9] = 15712\n",
            "[TOP] x_divider = 12154\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 19165\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4589960005\n",
            "[TOP] exp_trunc[9] = 280148\n",
            "[TOP] exp_accum_input[9] = 17509\n",
            "[TOP] x_divider = 13570\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 19628\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4700847116\n",
            "[TOP] exp_trunc[9] = 286916\n",
            "[TOP] exp_accum_input[9] = 17932\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1120\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 19049\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562178353\n",
            "[TOP] exp_trunc[9] = 278453\n",
            "[TOP] exp_accum_input[9] = 17403\n",
            "[TOP] x_divider = 13609\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 23612\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5655003164\n",
            "[TOP] exp_trunc[9] = 345154\n",
            "[TOP] exp_accum_input[9] = 21572\n",
            "[TOP] x_divider = 11212\n",
            "[TOP] y_dividend = 1348\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 26603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6371338691\n",
            "[TOP] exp_trunc[9] = 388875\n",
            "[TOP] exp_accum_input[9] = 24304\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 1519\n",
            "[TOP] data_out[9] = 1093\n",
            "[TOP] exp_fraction_output[9] = 19570\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4686956290\n",
            "[TOP] exp_trunc[9] = 286069\n",
            "[TOP] exp_accum_input[9] = 17879\n",
            "[TOP] x_divider = 11083\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17699\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4238857403\n",
            "[TOP] exp_trunc[9] = 258719\n",
            "[TOP] exp_accum_input[9] = 16169\n",
            "[TOP] x_divider = 11586\n",
            "[TOP] y_dividend = 1010\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20194\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4836402418\n",
            "[TOP] exp_trunc[9] = 295190\n",
            "[TOP] exp_accum_input[9] = 18449\n",
            "[TOP] x_divider = 11269\n",
            "[TOP] y_dividend = 1153\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17539\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4200537883\n",
            "[TOP] exp_trunc[9] = 256380\n",
            "[TOP] exp_accum_input[9] = 16023\n",
            "[TOP] x_divider = 12494\n",
            "[TOP] y_dividend = 1001\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 24900\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963475300\n",
            "[TOP] exp_trunc[9] = 363981\n",
            "[TOP] exp_accum_input[9] = 22748\n",
            "[TOP] x_divider = 12659\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 25175\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6029336975\n",
            "[TOP] exp_trunc[9] = 368001\n",
            "[TOP] exp_accum_input[9] = 23000\n",
            "[TOP] x_divider = 13321\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 17880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4282206360\n",
            "[TOP] exp_trunc[9] = 261365\n",
            "[TOP] exp_accum_input[9] = 16335\n",
            "[TOP] x_divider = 11318\n",
            "[TOP] y_dividend = 1020\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 23240\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5565910280\n",
            "[TOP] exp_trunc[9] = 339716\n",
            "[TOP] exp_accum_input[9] = 21232\n",
            "[TOP] x_divider = 11637\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 29592\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7087195224\n",
            "[TOP] exp_trunc[9] = 432568\n",
            "[TOP] exp_accum_input[9] = 27035\n",
            "[TOP] x_divider = 14586\n",
            "[TOP] y_dividend = 1689\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 19549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4681926853\n",
            "[TOP] exp_trunc[9] = 285762\n",
            "[TOP] exp_accum_input[9] = 17860\n",
            "[TOP] x_divider = 12147\n",
            "[TOP] y_dividend = 1116\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4974592187\n",
            "[TOP] exp_trunc[9] = 303625\n",
            "[TOP] exp_accum_input[9] = 18976\n",
            "[TOP] x_divider = 13006\n",
            "[TOP] y_dividend = 1186\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12027\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 32790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7853106630\n",
            "[TOP] exp_trunc[9] = 479315\n",
            "[TOP] exp_accum_input[9] = 29957\n",
            "[TOP] x_divider = 15772\n",
            "[TOP] y_dividend = 1872\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 17904\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4287954288\n",
            "[TOP] exp_trunc[9] = 261715\n",
            "[TOP] exp_accum_input[9] = 16357\n",
            "[TOP] x_divider = 12088\n",
            "[TOP] y_dividend = 1022\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 18765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4494161205\n",
            "[TOP] exp_trunc[9] = 274301\n",
            "[TOP] exp_accum_input[9] = 17143\n",
            "[TOP] x_divider = 12011\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 14106\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 18408\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4408660776\n",
            "[TOP] exp_trunc[9] = 269083\n",
            "[TOP] exp_accum_input[9] = 16817\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 23801\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5700268097\n",
            "[TOP] exp_trunc[9] = 347916\n",
            "[TOP] exp_accum_input[9] = 21744\n",
            "[TOP] x_divider = 12166\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20608\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4935554176\n",
            "[TOP] exp_trunc[9] = 301242\n",
            "[TOP] exp_accum_input[9] = 18827\n",
            "[TOP] x_divider = 10942\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 17837\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4271907989\n",
            "[TOP] exp_trunc[9] = 260736\n",
            "[TOP] exp_accum_input[9] = 16296\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 18665\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4470211505\n",
            "[TOP] exp_trunc[9] = 272840\n",
            "[TOP] exp_accum_input[9] = 17052\n",
            "[TOP] x_divider = 13420\n",
            "[TOP] y_dividend = 1065\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12270\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 651\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10312\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 20152\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4826343544\n",
            "[TOP] exp_trunc[9] = 294576\n",
            "[TOP] exp_accum_input[9] = 18411\n",
            "[TOP] x_divider = 11172\n",
            "[TOP] y_dividend = 1150\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17318\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4147609046\n",
            "[TOP] exp_trunc[9] = 253149\n",
            "[TOP] exp_accum_input[9] = 15821\n",
            "[TOP] x_divider = 10474\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18540\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4440274380\n",
            "[TOP] exp_trunc[9] = 271012\n",
            "[TOP] exp_accum_input[9] = 16938\n",
            "[TOP] x_divider = 10600\n",
            "[TOP] y_dividend = 1058\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22703\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5437300391\n",
            "[TOP] exp_trunc[9] = 331866\n",
            "[TOP] exp_accum_input[9] = 20741\n",
            "[TOP] x_divider = 11384\n",
            "[TOP] y_dividend = 1296\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 18629\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4461589613\n",
            "[TOP] exp_trunc[9] = 272313\n",
            "[TOP] exp_accum_input[9] = 17019\n",
            "[TOP] x_divider = 11241\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 18758\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4492484726\n",
            "[TOP] exp_trunc[9] = 274199\n",
            "[TOP] exp_accum_input[9] = 17137\n",
            "[TOP] x_divider = 11123\n",
            "[TOP] y_dividend = 1071\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 17259\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4133478723\n",
            "[TOP] exp_trunc[9] = 252287\n",
            "[TOP] exp_accum_input[9] = 15767\n",
            "[TOP] x_divider = 11648\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 693\n",
            "[TOP] exp_fraction_output[9] = 20459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4899869123\n",
            "[TOP] exp_trunc[9] = 299064\n",
            "[TOP] exp_accum_input[9] = 18691\n",
            "[TOP] x_divider = 11372\n",
            "[TOP] y_dividend = 1168\n",
            "[TOP] data_out[9] = 841\n",
            "[TOP] exp_fraction_output[9] = 19391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4644086327\n",
            "[TOP] exp_trunc[9] = 283452\n",
            "[TOP] exp_accum_input[9] = 17715\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1107\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 601\n",
            "[TOP] exp_fraction_output[9] = 23924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5729726228\n",
            "[TOP] exp_trunc[9] = 349714\n",
            "[TOP] exp_accum_input[9] = 21857\n",
            "[TOP] x_divider = 15161\n",
            "[TOP] y_dividend = 1366\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18084\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4331063748\n",
            "[TOP] exp_trunc[9] = 264347\n",
            "[TOP] exp_accum_input[9] = 16521\n",
            "[TOP] x_divider = 11549\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 20389\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4883104333\n",
            "[TOP] exp_trunc[9] = 298041\n",
            "[TOP] exp_accum_input[9] = 18627\n",
            "[TOP] x_divider = 12473\n",
            "[TOP] y_dividend = 1164\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 11803\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12347\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 19005\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4551640485\n",
            "[TOP] exp_trunc[9] = 277810\n",
            "[TOP] exp_accum_input[9] = 17363\n",
            "[TOP] x_divider = 10852\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20587\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4930524739\n",
            "[TOP] exp_trunc[9] = 300935\n",
            "[TOP] exp_accum_input[9] = 18808\n",
            "[TOP] x_divider = 11912\n",
            "[TOP] y_dividend = 1175\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 18433\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4414648201\n",
            "[TOP] exp_trunc[9] = 269448\n",
            "[TOP] exp_accum_input[9] = 16840\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1052\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11200\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 715\n",
            "[TOP] exp_fraction_output[9] = 19320\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627082040\n",
            "[TOP] exp_trunc[9] = 282414\n",
            "[TOP] exp_accum_input[9] = 17650\n",
            "[TOP] x_divider = 10945\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 17662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229996014\n",
            "[TOP] exp_trunc[9] = 258178\n",
            "[TOP] exp_accum_input[9] = 16136\n",
            "[TOP] x_divider = 12042\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 17425\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173235225\n",
            "[TOP] exp_trunc[9] = 254714\n",
            "[TOP] exp_accum_input[9] = 15919\n",
            "[TOP] x_divider = 11365\n",
            "[TOP] y_dividend = 994\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 18411\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4409379267\n",
            "[TOP] exp_trunc[9] = 269127\n",
            "[TOP] exp_accum_input[9] = 16820\n",
            "[TOP] x_divider = 11147\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 21515\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5152777955\n",
            "[TOP] exp_trunc[9] = 314500\n",
            "[TOP] exp_accum_input[9] = 19656\n",
            "[TOP] x_divider = 11338\n",
            "[TOP] y_dividend = 1228\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 21738\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5206185786\n",
            "[TOP] exp_trunc[9] = 317760\n",
            "[TOP] exp_accum_input[9] = 19860\n",
            "[TOP] x_divider = 10818\n",
            "[TOP] y_dividend = 1241\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 27987\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6702802539\n",
            "[TOP] exp_trunc[9] = 409106\n",
            "[TOP] exp_accum_input[9] = 25569\n",
            "[TOP] x_divider = 12986\n",
            "[TOP] y_dividend = 1598\n",
            "[TOP] data_out[9] = 1009\n",
            "[TOP] exp_fraction_output[9] = 18858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4516434426\n",
            "[TOP] exp_trunc[9] = 275661\n",
            "[TOP] exp_accum_input[9] = 17228\n",
            "[TOP] x_divider = 10406\n",
            "[TOP] y_dividend = 1076\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12028\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 18833\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4510447001\n",
            "[TOP] exp_trunc[9] = 275295\n",
            "[TOP] exp_accum_input[9] = 17205\n",
            "[TOP] x_divider = 11231\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 19012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553316964\n",
            "[TOP] exp_trunc[9] = 277912\n",
            "[TOP] exp_accum_input[9] = 17369\n",
            "[TOP] x_divider = 11886\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11692\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 18965\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4542060605\n",
            "[TOP] exp_trunc[9] = 277225\n",
            "[TOP] exp_accum_input[9] = 17326\n",
            "[TOP] x_divider = 11516\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 19802\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4742519594\n",
            "[TOP] exp_trunc[9] = 289460\n",
            "[TOP] exp_accum_input[9] = 18091\n",
            "[TOP] x_divider = 10443\n",
            "[TOP] y_dividend = 1130\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 17212\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122222364\n",
            "[TOP] exp_trunc[9] = 251600\n",
            "[TOP] exp_accum_input[9] = 15725\n",
            "[TOP] x_divider = 10064\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13551\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 589\n",
            "[TOP] exp_fraction_output[9] = 21101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5053626197\n",
            "[TOP] exp_trunc[9] = 308448\n",
            "[TOP] exp_accum_input[9] = 19278\n",
            "[TOP] x_divider = 12093\n",
            "[TOP] y_dividend = 1204\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 19855\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4755212935\n",
            "[TOP] exp_trunc[9] = 290235\n",
            "[TOP] exp_accum_input[9] = 18139\n",
            "[TOP] x_divider = 11830\n",
            "[TOP] y_dividend = 1133\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10872\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914478440\n",
            "[TOP] exp_trunc[9] = 299955\n",
            "[TOP] exp_accum_input[9] = 18747\n",
            "[TOP] x_divider = 12614\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17681\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4234546457\n",
            "[TOP] exp_trunc[9] = 258456\n",
            "[TOP] exp_accum_input[9] = 16153\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13469\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 17618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4219458146\n",
            "[TOP] exp_trunc[9] = 257535\n",
            "[TOP] exp_accum_input[9] = 16095\n",
            "[TOP] x_divider = 10811\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 23311\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5582914567\n",
            "[TOP] exp_trunc[9] = 340754\n",
            "[TOP] exp_accum_input[9] = 21297\n",
            "[TOP] x_divider = 12055\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 903\n",
            "[TOP] exp_fraction_output[9] = 27549\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6597902853\n",
            "[TOP] exp_trunc[9] = 402704\n",
            "[TOP] exp_accum_input[9] = 25169\n",
            "[TOP] x_divider = 11877\n",
            "[TOP] y_dividend = 1573\n",
            "[TOP] data_out[9] = 1085\n",
            "[TOP] exp_fraction_output[9] = 23100\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5532380700\n",
            "[TOP] exp_trunc[9] = 337669\n",
            "[TOP] exp_accum_input[9] = 21104\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 11257\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12119\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11278\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12672\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 18290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4380400130\n",
            "[TOP] exp_trunc[9] = 267358\n",
            "[TOP] exp_accum_input[9] = 16709\n",
            "[TOP] x_divider = 15782\n",
            "[TOP] y_dividend = 1044\n",
            "[TOP] data_out[9] = 541\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 15434\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 519\n",
            "[TOP] exp_fraction_output[9] = 23491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626024027\n",
            "[TOP] exp_trunc[9] = 343385\n",
            "[TOP] exp_accum_input[9] = 21461\n",
            "[TOP] x_divider = 11613\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6140463583\n",
            "[TOP] exp_trunc[9] = 374784\n",
            "[TOP] exp_accum_input[9] = 23424\n",
            "[TOP] x_divider = 11406\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 1051\n",
            "[TOP] exp_fraction_output[9] = 27184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6510486448\n",
            "[TOP] exp_trunc[9] = 397368\n",
            "[TOP] exp_accum_input[9] = 24835\n",
            "[TOP] x_divider = 10939\n",
            "[TOP] y_dividend = 1552\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 38909\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 9318588773\n",
            "[TOP] exp_trunc[9] = 568761\n",
            "[TOP] exp_accum_input[9] = 35547\n",
            "[TOP] x_divider = 14650\n",
            "[TOP] y_dividend = 2221\n",
            "[TOP] data_out[9] = 1241\n",
            "[TOP] exp_fraction_output[9] = 18880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4521703360\n",
            "[TOP] exp_trunc[9] = 275982\n",
            "[TOP] exp_accum_input[9] = 17248\n",
            "[TOP] x_divider = 11160\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18898\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4526014306\n",
            "[TOP] exp_trunc[9] = 276245\n",
            "[TOP] exp_accum_input[9] = 17265\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11882\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 25180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6030534460\n",
            "[TOP] exp_trunc[9] = 368074\n",
            "[TOP] exp_accum_input[9] = 23004\n",
            "[TOP] x_divider = 11702\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 1007\n",
            "[TOP] exp_fraction_output[9] = 33438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8008300686\n",
            "[TOP] exp_trunc[9] = 488787\n",
            "[TOP] exp_accum_input[9] = 30549\n",
            "[TOP] x_divider = 13486\n",
            "[TOP] y_dividend = 1909\n",
            "[TOP] data_out[9] = 1159\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13624\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 26312\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6301645064\n",
            "[TOP] exp_trunc[9] = 384621\n",
            "[TOP] exp_accum_input[9] = 24038\n",
            "[TOP] x_divider = 13254\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 26719\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6399120343\n",
            "[TOP] exp_trunc[9] = 390571\n",
            "[TOP] exp_accum_input[9] = 24410\n",
            "[TOP] x_divider = 13958\n",
            "[TOP] y_dividend = 1525\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 21453\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5137929141\n",
            "[TOP] exp_trunc[9] = 313594\n",
            "[TOP] exp_accum_input[9] = 19599\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1224\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 20994\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5028000018\n",
            "[TOP] exp_trunc[9] = 306884\n",
            "[TOP] exp_accum_input[9] = 19180\n",
            "[TOP] x_divider = 12802\n",
            "[TOP] y_dividend = 1198\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 11437\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11430\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 701\n",
            "[TOP] exp_fraction_output[9] = 17831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4270471007\n",
            "[TOP] exp_trunc[9] = 260648\n",
            "[TOP] exp_accum_input[9] = 16290\n",
            "[TOP] x_divider = 10452\n",
            "[TOP] y_dividend = 1018\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 21132\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5061050604\n",
            "[TOP] exp_trunc[9] = 308902\n",
            "[TOP] exp_accum_input[9] = 19306\n",
            "[TOP] x_divider = 12565\n",
            "[TOP] y_dividend = 1206\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 20685\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4953995445\n",
            "[TOP] exp_trunc[9] = 302367\n",
            "[TOP] exp_accum_input[9] = 18897\n",
            "[TOP] x_divider = 11166\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 19007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4552119479\n",
            "[TOP] exp_trunc[9] = 277839\n",
            "[TOP] exp_accum_input[9] = 17364\n",
            "[TOP] x_divider = 10533\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 27234\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6522461298\n",
            "[TOP] exp_trunc[9] = 398099\n",
            "[TOP] exp_accum_input[9] = 24881\n",
            "[TOP] x_divider = 14731\n",
            "[TOP] y_dividend = 1555\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 18810\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4504938570\n",
            "[TOP] exp_trunc[9] = 274959\n",
            "[TOP] exp_accum_input[9] = 17184\n",
            "[TOP] x_divider = 12703\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 19595\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4692943715\n",
            "[TOP] exp_trunc[9] = 286434\n",
            "[TOP] exp_accum_input[9] = 17902\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1118\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 24646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5902643062\n",
            "[TOP] exp_trunc[9] = 360268\n",
            "[TOP] exp_accum_input[9] = 22516\n",
            "[TOP] x_divider = 11314\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 1021\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4217781667\n",
            "[TOP] exp_trunc[9] = 257432\n",
            "[TOP] exp_accum_input[9] = 16089\n",
            "[TOP] x_divider = 10966\n",
            "[TOP] y_dividend = 1005\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 13581\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 607\n",
            "[TOP] exp_fraction_output[9] = 19494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668754518\n",
            "[TOP] exp_trunc[9] = 284958\n",
            "[TOP] exp_accum_input[9] = 17809\n",
            "[TOP] x_divider = 12558\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 19147\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4585649059\n",
            "[TOP] exp_trunc[9] = 279885\n",
            "[TOP] exp_accum_input[9] = 17492\n",
            "[TOP] x_divider = 12142\n",
            "[TOP] y_dividend = 1093\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20012\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4792813964\n",
            "[TOP] exp_trunc[9] = 292530\n",
            "[TOP] exp_accum_input[9] = 18283\n",
            "[TOP] x_divider = 11969\n",
            "[TOP] y_dividend = 1142\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 18371\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4399799387\n",
            "[TOP] exp_trunc[9] = 268542\n",
            "[TOP] exp_accum_input[9] = 16783\n",
            "[TOP] x_divider = 10865\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19498\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4669712506\n",
            "[TOP] exp_trunc[9] = 285016\n",
            "[TOP] exp_accum_input[9] = 17813\n",
            "[TOP] x_divider = 12894\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 23607\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653805679\n",
            "[TOP] exp_trunc[9] = 345080\n",
            "[TOP] exp_accum_input[9] = 21567\n",
            "[TOP] x_divider = 12525\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 11684\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 41979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 10053844563\n",
            "[TOP] exp_trunc[9] = 613637\n",
            "[TOP] exp_accum_input[9] = 38352\n",
            "[TOP] x_divider = 19046\n",
            "[TOP] y_dividend = 2397\n",
            "[TOP] data_out[9] = 1029\n",
            "[TOP] exp_fraction_output[9] = 27869\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6674541893\n",
            "[TOP] exp_trunc[9] = 407381\n",
            "[TOP] exp_accum_input[9] = 25461\n",
            "[TOP] x_divider = 15936\n",
            "[TOP] y_dividend = 1591\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 18222\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4364114334\n",
            "[TOP] exp_trunc[9] = 266364\n",
            "[TOP] exp_accum_input[9] = 16647\n",
            "[TOP] x_divider = 12675\n",
            "[TOP] y_dividend = 1040\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 23031\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2029169286\n",
            "[TOP] exp_trunc[9] = 123850\n",
            "[TOP] exp_accum_input[9] = 7740\n",
            "[TOP] x_divider = 6434\n",
            "[TOP] y_dividend = 483\n",
            "[TOP] data_out[9] = 615\n",
            "[TOP] exp_fraction_output[9] = 28910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6923858270\n",
            "[TOP] exp_trunc[9] = 422598\n",
            "[TOP] exp_accum_input[9] = 26412\n",
            "[TOP] x_divider = 14346\n",
            "[TOP] y_dividend = 1650\n",
            "[TOP] data_out[9] = 943\n",
            "[TOP] exp_fraction_output[9] = 26208\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6276737376\n",
            "[TOP] exp_trunc[9] = 383101\n",
            "[TOP] exp_accum_input[9] = 23943\n",
            "[TOP] x_divider = 16608\n",
            "[TOP] y_dividend = 1496\n",
            "[TOP] data_out[9] = 739\n",
            "[TOP] exp_fraction_output[9] = 22906\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5485918282\n",
            "[TOP] exp_trunc[9] = 334833\n",
            "[TOP] exp_accum_input[9] = 20927\n",
            "[TOP] x_divider = 11539\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19317\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4626363549\n",
            "[TOP] exp_trunc[9] = 282370\n",
            "[TOP] exp_accum_input[9] = 17648\n",
            "[TOP] x_divider = 11482\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 22958\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5498372126\n",
            "[TOP] exp_trunc[9] = 335594\n",
            "[TOP] exp_accum_input[9] = 20974\n",
            "[TOP] x_divider = 13617\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 20526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4915915422\n",
            "[TOP] exp_trunc[9] = 300043\n",
            "[TOP] exp_accum_input[9] = 18752\n",
            "[TOP] x_divider = 11517\n",
            "[TOP] y_dividend = 1172\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 19491\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4668036027\n",
            "[TOP] exp_trunc[9] = 284914\n",
            "[TOP] exp_accum_input[9] = 17807\n",
            "[TOP] x_divider = 12407\n",
            "[TOP] y_dividend = 1112\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 21679\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5192055463\n",
            "[TOP] exp_trunc[9] = 316897\n",
            "[TOP] exp_accum_input[9] = 19806\n",
            "[TOP] x_divider = 12265\n",
            "[TOP] y_dividend = 1237\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 27746\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6645083762\n",
            "[TOP] exp_trunc[9] = 405583\n",
            "[TOP] exp_accum_input[9] = 25348\n",
            "[TOP] x_divider = 11529\n",
            "[TOP] y_dividend = 1584\n",
            "[TOP] data_out[9] = 1125\n",
            "[TOP] exp_fraction_output[9] = 26184\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6270989448\n",
            "[TOP] exp_trunc[9] = 382750\n",
            "[TOP] exp_accum_input[9] = 23921\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1069\n",
            "[TOP] exp_fraction_output[9] = 23029\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5515376413\n",
            "[TOP] exp_trunc[9] = 336631\n",
            "[TOP] exp_accum_input[9] = 21039\n",
            "[TOP] x_divider = 11261\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 17854\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275979438\n",
            "[TOP] exp_trunc[9] = 260985\n",
            "[TOP] exp_accum_input[9] = 16311\n",
            "[TOP] x_divider = 13483\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 17520\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195987440\n",
            "[TOP] exp_trunc[9] = 256102\n",
            "[TOP] exp_accum_input[9] = 16006\n",
            "[TOP] x_divider = 11555\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 28729\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6880509313\n",
            "[TOP] exp_trunc[9] = 419952\n",
            "[TOP] exp_accum_input[9] = 26247\n",
            "[TOP] x_divider = 13307\n",
            "[TOP] y_dividend = 1640\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10854\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 22146\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5303900562\n",
            "[TOP] exp_trunc[9] = 323724\n",
            "[TOP] exp_accum_input[9] = 20232\n",
            "[TOP] x_divider = 13327\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 19051\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4562657347\n",
            "[TOP] exp_trunc[9] = 278482\n",
            "[TOP] exp_accum_input[9] = 17405\n",
            "[TOP] x_divider = 12252\n",
            "[TOP] y_dividend = 1087\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 21537\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5158046889\n",
            "[TOP] exp_trunc[9] = 314822\n",
            "[TOP] exp_accum_input[9] = 19676\n",
            "[TOP] x_divider = 11224\n",
            "[TOP] y_dividend = 1229\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 21039\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5038777383\n",
            "[TOP] exp_trunc[9] = 307542\n",
            "[TOP] exp_accum_input[9] = 19221\n",
            "[TOP] x_divider = 13680\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4390937998\n",
            "[TOP] exp_trunc[9] = 268001\n",
            "[TOP] exp_accum_input[9] = 16750\n",
            "[TOP] x_divider = 11223\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12122\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 661\n",
            "[TOP] exp_fraction_output[9] = 17458\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4181138626\n",
            "[TOP] exp_trunc[9] = 255196\n",
            "[TOP] exp_accum_input[9] = 15949\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4247000301\n",
            "[TOP] exp_trunc[9] = 259216\n",
            "[TOP] exp_accum_input[9] = 16201\n",
            "[TOP] x_divider = 11681\n",
            "[TOP] y_dividend = 1012\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 17714\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4242449858\n",
            "[TOP] exp_trunc[9] = 258938\n",
            "[TOP] exp_accum_input[9] = 16183\n",
            "[TOP] x_divider = 11219\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 24467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5859773099\n",
            "[TOP] exp_trunc[9] = 357652\n",
            "[TOP] exp_accum_input[9] = 22353\n",
            "[TOP] x_divider = 13017\n",
            "[TOP] y_dividend = 1397\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 17936\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4295618192\n",
            "[TOP] exp_trunc[9] = 262183\n",
            "[TOP] exp_accum_input[9] = 16386\n",
            "[TOP] x_divider = 11293\n",
            "[TOP] y_dividend = 1024\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 20737\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4966449289\n",
            "[TOP] exp_trunc[9] = 303128\n",
            "[TOP] exp_accum_input[9] = 18945\n",
            "[TOP] x_divider = 11408\n",
            "[TOP] y_dividend = 1184\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 18908\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4528409276\n",
            "[TOP] exp_trunc[9] = 276392\n",
            "[TOP] exp_accum_input[9] = 17274\n",
            "[TOP] x_divider = 12646\n",
            "[TOP] y_dividend = 1079\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 12530\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 21331\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5108710507\n",
            "[TOP] exp_trunc[9] = 311810\n",
            "[TOP] exp_accum_input[9] = 19488\n",
            "[TOP] x_divider = 12179\n",
            "[TOP] y_dividend = 1218\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 26471\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6339725087\n",
            "[TOP] exp_trunc[9] = 386946\n",
            "[TOP] exp_accum_input[9] = 24184\n",
            "[TOP] x_divider = 14785\n",
            "[TOP] y_dividend = 1511\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 29843\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7147308971\n",
            "[TOP] exp_trunc[9] = 436237\n",
            "[TOP] exp_accum_input[9] = 27264\n",
            "[TOP] x_divider = 12719\n",
            "[TOP] y_dividend = 1704\n",
            "[TOP] data_out[9] = 1095\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 13229\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 32217\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2838511002\n",
            "[TOP] exp_trunc[9] = 173248\n",
            "[TOP] exp_accum_input[9] = 10828\n",
            "[TOP] x_divider = 7613\n",
            "[TOP] y_dividend = 676\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12065\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 22273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5334316681\n",
            "[TOP] exp_trunc[9] = 325580\n",
            "[TOP] exp_accum_input[9] = 20348\n",
            "[TOP] x_divider = 13500\n",
            "[TOP] y_dividend = 1271\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 17379\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4162218363\n",
            "[TOP] exp_trunc[9] = 254041\n",
            "[TOP] exp_accum_input[9] = 15877\n",
            "[TOP] x_divider = 11481\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5789121484\n",
            "[TOP] exp_trunc[9] = 353339\n",
            "[TOP] exp_accum_input[9] = 22083\n",
            "[TOP] x_divider = 16257\n",
            "[TOP] y_dividend = 1380\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17671\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4232151487\n",
            "[TOP] exp_trunc[9] = 258310\n",
            "[TOP] exp_accum_input[9] = 16144\n",
            "[TOP] x_divider = 11825\n",
            "[TOP] y_dividend = 1009\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 23637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5660990589\n",
            "[TOP] exp_trunc[9] = 345519\n",
            "[TOP] exp_accum_input[9] = 21594\n",
            "[TOP] x_divider = 12438\n",
            "[TOP] y_dividend = 1349\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19296\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4621334112\n",
            "[TOP] exp_trunc[9] = 282063\n",
            "[TOP] exp_accum_input[9] = 17628\n",
            "[TOP] x_divider = 12744\n",
            "[TOP] y_dividend = 1101\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21078\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5048117766\n",
            "[TOP] exp_trunc[9] = 308112\n",
            "[TOP] exp_accum_input[9] = 19257\n",
            "[TOP] x_divider = 13302\n",
            "[TOP] y_dividend = 1203\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 27363\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6553356411\n",
            "[TOP] exp_trunc[9] = 399985\n",
            "[TOP] exp_accum_input[9] = 24999\n",
            "[TOP] x_divider = 16020\n",
            "[TOP] y_dividend = 1562\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 22149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5304619053\n",
            "[TOP] exp_trunc[9] = 323768\n",
            "[TOP] exp_accum_input[9] = 20235\n",
            "[TOP] x_divider = 11791\n",
            "[TOP] y_dividend = 1264\n",
            "[TOP] data_out[9] = 877\n",
            "[TOP] exp_fraction_output[9] = 19322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4627561034\n",
            "[TOP] exp_trunc[9] = 282443\n",
            "[TOP] exp_accum_input[9] = 17652\n",
            "[TOP] x_divider = 11394\n",
            "[TOP] y_dividend = 1103\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11163\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 17856\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276458432\n",
            "[TOP] exp_trunc[9] = 261014\n",
            "[TOP] exp_accum_input[9] = 16313\n",
            "[TOP] x_divider = 12123\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 689\n",
            "[TOP] exp_fraction_output[9] = 18896\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4525535312\n",
            "[TOP] exp_trunc[9] = 276216\n",
            "[TOP] exp_accum_input[9] = 17263\n",
            "[TOP] x_divider = 11031\n",
            "[TOP] y_dividend = 1078\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 18569\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4447219793\n",
            "[TOP] exp_trunc[9] = 271436\n",
            "[TOP] exp_accum_input[9] = 16964\n",
            "[TOP] x_divider = 12091\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 19172\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4591636484\n",
            "[TOP] exp_trunc[9] = 280251\n",
            "[TOP] exp_accum_input[9] = 17515\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 1094\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 18519\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4435244943\n",
            "[TOP] exp_trunc[9] = 270705\n",
            "[TOP] exp_accum_input[9] = 16919\n",
            "[TOP] x_divider = 12249\n",
            "[TOP] y_dividend = 1057\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 26186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6271468442\n",
            "[TOP] exp_trunc[9] = 382780\n",
            "[TOP] exp_accum_input[9] = 23923\n",
            "[TOP] x_divider = 11360\n",
            "[TOP] y_dividend = 1495\n",
            "[TOP] data_out[9] = 1077\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10860\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 18776\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4496795672\n",
            "[TOP] exp_trunc[9] = 274462\n",
            "[TOP] exp_accum_input[9] = 17153\n",
            "[TOP] x_divider = 11153\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 18724\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4484341828\n",
            "[TOP] exp_trunc[9] = 273702\n",
            "[TOP] exp_accum_input[9] = 17106\n",
            "[TOP] x_divider = 11072\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11000\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 28198\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6753336406\n",
            "[TOP] exp_trunc[9] = 412190\n",
            "[TOP] exp_accum_input[9] = 25761\n",
            "[TOP] x_divider = 15249\n",
            "[TOP] y_dividend = 1610\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 28591\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6847458727\n",
            "[TOP] exp_trunc[9] = 417935\n",
            "[TOP] exp_accum_input[9] = 26120\n",
            "[TOP] x_divider = 13872\n",
            "[TOP] y_dividend = 1632\n",
            "[TOP] data_out[9] = 965\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 713\n",
            "[TOP] exp_fraction_output[9] = 20319\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4866339543\n",
            "[TOP] exp_trunc[9] = 297017\n",
            "[TOP] exp_accum_input[9] = 18563\n",
            "[TOP] x_divider = 11349\n",
            "[TOP] y_dividend = 1160\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22329\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5347728513\n",
            "[TOP] exp_trunc[9] = 326399\n",
            "[TOP] exp_accum_input[9] = 20399\n",
            "[TOP] x_divider = 13427\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 22601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412871697\n",
            "[TOP] exp_trunc[9] = 330375\n",
            "[TOP] exp_accum_input[9] = 20648\n",
            "[TOP] x_divider = 13130\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 37387\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8954074339\n",
            "[TOP] exp_trunc[9] = 546513\n",
            "[TOP] exp_accum_input[9] = 34157\n",
            "[TOP] x_divider = 16787\n",
            "[TOP] y_dividend = 2134\n",
            "[TOP] data_out[9] = 1039\n",
            "[TOP] exp_fraction_output[9] = 25840\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6188602480\n",
            "[TOP] exp_trunc[9] = 377722\n",
            "[TOP] exp_accum_input[9] = 23607\n",
            "[TOP] x_divider = 12098\n",
            "[TOP] y_dividend = 1475\n",
            "[TOP] data_out[9] = 999\n",
            "[TOP] exp_fraction_output[9] = 35844\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8584530468\n",
            "[TOP] exp_trunc[9] = 523958\n",
            "[TOP] exp_accum_input[9] = 32747\n",
            "[TOP] x_divider = 13716\n",
            "[TOP] y_dividend = 2046\n",
            "[TOP] data_out[9] = 1221\n",
            "[TOP] exp_fraction_output[9] = 25658\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6145014026\n",
            "[TOP] exp_trunc[9] = 375061\n",
            "[TOP] exp_accum_input[9] = 23441\n",
            "[TOP] x_divider = 13429\n",
            "[TOP] y_dividend = 1465\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20183\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4833767951\n",
            "[TOP] exp_trunc[9] = 295029\n",
            "[TOP] exp_accum_input[9] = 18439\n",
            "[TOP] x_divider = 10613\n",
            "[TOP] y_dividend = 1152\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 36993\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8859712521\n",
            "[TOP] exp_trunc[9] = 540753\n",
            "[TOP] exp_accum_input[9] = 33797\n",
            "[TOP] x_divider = 18570\n",
            "[TOP] y_dividend = 2112\n",
            "[TOP] data_out[9] = 931\n",
            "[TOP] exp_fraction_output[9] = 43948\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 3872082488\n",
            "[TOP] exp_trunc[9] = 236333\n",
            "[TOP] exp_accum_input[9] = 14770\n",
            "[TOP] x_divider = 7486\n",
            "[TOP] y_dividend = 923\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 24121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5776907137\n",
            "[TOP] exp_trunc[9] = 352594\n",
            "[TOP] exp_accum_input[9] = 22037\n",
            "[TOP] x_divider = 13245\n",
            "[TOP] y_dividend = 1377\n",
            "[TOP] data_out[9] = 851\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 7897\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 1017\n",
            "[TOP] exp_fraction_output[9] = 32030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7671088910\n",
            "[TOP] exp_trunc[9] = 468206\n",
            "[TOP] exp_accum_input[9] = 29262\n",
            "[TOP] x_divider = 12925\n",
            "[TOP] y_dividend = 1828\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24213\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5798940861\n",
            "[TOP] exp_trunc[9] = 353939\n",
            "[TOP] exp_accum_input[9] = 22121\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1382\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 10858\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 27273\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6531801681\n",
            "[TOP] exp_trunc[9] = 398669\n",
            "[TOP] exp_accum_input[9] = 24916\n",
            "[TOP] x_divider = 12112\n",
            "[TOP] y_dividend = 1557\n",
            "[TOP] data_out[9] = 1053\n",
            "[TOP] exp_fraction_output[9] = 21581\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5168584757\n",
            "[TOP] exp_trunc[9] = 315465\n",
            "[TOP] exp_accum_input[9] = 19716\n",
            "[TOP] x_divider = 11560\n",
            "[TOP] y_dividend = 1232\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6104778530\n",
            "[TOP] exp_trunc[9] = 372606\n",
            "[TOP] exp_accum_input[9] = 23287\n",
            "[TOP] x_divider = 11984\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 995\n",
            "[TOP] exp_fraction_output[9] = 25647\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6142379559\n",
            "[TOP] exp_trunc[9] = 374901\n",
            "[TOP] exp_accum_input[9] = 23431\n",
            "[TOP] x_divider = 15670\n",
            "[TOP] y_dividend = 1464\n",
            "[TOP] data_out[9] = 767\n",
            "[TOP] exp_fraction_output[9] = 31798\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7615525606\n",
            "[TOP] exp_trunc[9] = 464814\n",
            "[TOP] exp_accum_input[9] = 29050\n",
            "[TOP] x_divider = 12708\n",
            "[TOP] y_dividend = 1815\n",
            "[TOP] data_out[9] = 1169\n",
            "[TOP] exp_fraction_output[9] = 25853\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6191715941\n",
            "[TOP] exp_trunc[9] = 377912\n",
            "[TOP] exp_accum_input[9] = 23619\n",
            "[TOP] x_divider = 13472\n",
            "[TOP] y_dividend = 1476\n",
            "[TOP] data_out[9] = 897\n",
            "[TOP] exp_fraction_output[9] = 28599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6849374703\n",
            "[TOP] exp_trunc[9] = 418052\n",
            "[TOP] exp_accum_input[9] = 26128\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1633\n",
            "[TOP] data_out[9] = 1011\n",
            "[TOP] exp_fraction_output[9] = 18328\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4389501016\n",
            "[TOP] exp_trunc[9] = 267913\n",
            "[TOP] exp_accum_input[9] = 16744\n",
            "[TOP] x_divider = 11117\n",
            "[TOP] y_dividend = 1046\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 22925\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5490468725\n",
            "[TOP] exp_trunc[9] = 335111\n",
            "[TOP] exp_accum_input[9] = 20944\n",
            "[TOP] x_divider = 12102\n",
            "[TOP] y_dividend = 1309\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4411774237\n",
            "[TOP] exp_trunc[9] = 269273\n",
            "[TOP] exp_accum_input[9] = 16829\n",
            "[TOP] x_divider = 12442\n",
            "[TOP] y_dividend = 1051\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 28633\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6857517601\n",
            "[TOP] exp_trunc[9] = 418549\n",
            "[TOP] exp_accum_input[9] = 26159\n",
            "[TOP] x_divider = 15349\n",
            "[TOP] y_dividend = 1634\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 33114\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7930703658\n",
            "[TOP] exp_trunc[9] = 484051\n",
            "[TOP] exp_accum_input[9] = 30253\n",
            "[TOP] x_divider = 16206\n",
            "[TOP] y_dividend = 1890\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21768\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213370696\n",
            "[TOP] exp_trunc[9] = 318198\n",
            "[TOP] exp_accum_input[9] = 19887\n",
            "[TOP] x_divider = 15445\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 32959\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7893581623\n",
            "[TOP] exp_trunc[9] = 481785\n",
            "[TOP] exp_accum_input[9] = 30111\n",
            "[TOP] x_divider = 16141\n",
            "[TOP] y_dividend = 1881\n",
            "[TOP] data_out[9] = 955\n",
            "[TOP] exp_fraction_output[9] = 33686\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8067695942\n",
            "[TOP] exp_trunc[9] = 492413\n",
            "[TOP] exp_accum_input[9] = 30775\n",
            "[TOP] x_divider = 16475\n",
            "[TOP] y_dividend = 1923\n",
            "[TOP] data_out[9] = 957\n",
            "[TOP] exp_fraction_output[9] = 21217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5081407849\n",
            "[TOP] exp_trunc[9] = 310144\n",
            "[TOP] exp_accum_input[9] = 19384\n",
            "[TOP] x_divider = 12340\n",
            "[TOP] y_dividend = 1211\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5497174641\n",
            "[TOP] exp_trunc[9] = 335520\n",
            "[TOP] exp_accum_input[9] = 20970\n",
            "[TOP] x_divider = 13633\n",
            "[TOP] y_dividend = 1310\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 24526\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5873903422\n",
            "[TOP] exp_trunc[9] = 358514\n",
            "[TOP] exp_accum_input[9] = 22407\n",
            "[TOP] x_divider = 12220\n",
            "[TOP] y_dividend = 1400\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 20601\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4933877697\n",
            "[TOP] exp_trunc[9] = 301139\n",
            "[TOP] exp_accum_input[9] = 18821\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 1176\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 25485\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6103581045\n",
            "[TOP] exp_trunc[9] = 372533\n",
            "[TOP] exp_accum_input[9] = 23283\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1455\n",
            "[TOP] data_out[9] = 835\n",
            "[TOP] exp_fraction_output[9] = 20494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4908251518\n",
            "[TOP] exp_trunc[9] = 299575\n",
            "[TOP] exp_accum_input[9] = 18723\n",
            "[TOP] x_divider = 12187\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 25333\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6067177501\n",
            "[TOP] exp_trunc[9] = 370311\n",
            "[TOP] exp_accum_input[9] = 23144\n",
            "[TOP] x_divider = 15042\n",
            "[TOP] y_dividend = 1446\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6027420999\n",
            "[TOP] exp_trunc[9] = 367884\n",
            "[TOP] exp_accum_input[9] = 22992\n",
            "[TOP] x_divider = 14310\n",
            "[TOP] y_dividend = 1437\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 18305\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4383992585\n",
            "[TOP] exp_trunc[9] = 267577\n",
            "[TOP] exp_accum_input[9] = 16723\n",
            "[TOP] x_divider = 11863\n",
            "[TOP] y_dividend = 1045\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 23952\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2110314912\n",
            "[TOP] exp_trunc[9] = 128803\n",
            "[TOP] exp_accum_input[9] = 8050\n",
            "[TOP] x_divider = 5423\n",
            "[TOP] y_dividend = 503\n",
            "[TOP] data_out[9] = 757\n",
            "[TOP] exp_fraction_output[9] = 21903\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5245702791\n",
            "[TOP] exp_trunc[9] = 320172\n",
            "[TOP] exp_accum_input[9] = 20010\n",
            "[TOP] x_divider = 11453\n",
            "[TOP] y_dividend = 1250\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 20171\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4830893987\n",
            "[TOP] exp_trunc[9] = 294854\n",
            "[TOP] exp_accum_input[9] = 18428\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 825\n",
            "[TOP] exp_fraction_output[9] = 23420\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5609019740\n",
            "[TOP] exp_trunc[9] = 342347\n",
            "[TOP] exp_accum_input[9] = 21396\n",
            "[TOP] x_divider = 13078\n",
            "[TOP] y_dividend = 1337\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 14252\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 567\n",
            "[TOP] exp_fraction_output[9] = 24856\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2189962736\n",
            "[TOP] exp_trunc[9] = 133664\n",
            "[TOP] exp_accum_input[9] = 8354\n",
            "[TOP] x_divider = 7201\n",
            "[TOP] y_dividend = 522\n",
            "[TOP] data_out[9] = 593\n",
            "[TOP] exp_fraction_output[9] = 18889\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1664234234\n",
            "[TOP] exp_trunc[9] = 101576\n",
            "[TOP] exp_accum_input[9] = 6348\n",
            "[TOP] x_divider = 5850\n",
            "[TOP] y_dividend = 396\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 17772\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4256340684\n",
            "[TOP] exp_trunc[9] = 259786\n",
            "[TOP] exp_accum_input[9] = 16236\n",
            "[TOP] x_divider = 11906\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12728\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 631\n",
            "[TOP] exp_fraction_output[9] = 17468\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4183533596\n",
            "[TOP] exp_trunc[9] = 255342\n",
            "[TOP] exp_accum_input[9] = 15958\n",
            "[TOP] x_divider = 13071\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 13266\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 24616\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5895458152\n",
            "[TOP] exp_trunc[9] = 359830\n",
            "[TOP] exp_accum_input[9] = 22489\n",
            "[TOP] x_divider = 17693\n",
            "[TOP] y_dividend = 1405\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 20926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5011714222\n",
            "[TOP] exp_trunc[9] = 305890\n",
            "[TOP] exp_accum_input[9] = 19118\n",
            "[TOP] x_divider = 11780\n",
            "[TOP] y_dividend = 1194\n",
            "[TOP] data_out[9] = 831\n",
            "[TOP] exp_fraction_output[9] = 18754\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4491526738\n",
            "[TOP] exp_trunc[9] = 274141\n",
            "[TOP] exp_accum_input[9] = 17133\n",
            "[TOP] x_divider = 12039\n",
            "[TOP] y_dividend = 1070\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17214\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4122701358\n",
            "[TOP] exp_trunc[9] = 251629\n",
            "[TOP] exp_accum_input[9] = 15726\n",
            "[TOP] x_divider = 10739\n",
            "[TOP] y_dividend = 982\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 17226\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4125575322\n",
            "[TOP] exp_trunc[9] = 251805\n",
            "[TOP] exp_accum_input[9] = 15737\n",
            "[TOP] x_divider = 10355\n",
            "[TOP] y_dividend = 983\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 21770\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5213849690\n",
            "[TOP] exp_trunc[9] = 318228\n",
            "[TOP] exp_accum_input[9] = 19889\n",
            "[TOP] x_divider = 11002\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 18831\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4509968007\n",
            "[TOP] exp_trunc[9] = 275266\n",
            "[TOP] exp_accum_input[9] = 17204\n",
            "[TOP] x_divider = 12274\n",
            "[TOP] y_dividend = 1075\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12152\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 659\n",
            "[TOP] exp_fraction_output[9] = 18953\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4539186641\n",
            "[TOP] exp_trunc[9] = 277049\n",
            "[TOP] exp_accum_input[9] = 17315\n",
            "[TOP] x_divider = 10512\n",
            "[TOP] y_dividend = 1082\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17518\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4195508446\n",
            "[TOP] exp_trunc[9] = 256073\n",
            "[TOP] exp_accum_input[9] = 16004\n",
            "[TOP] x_divider = 11380\n",
            "[TOP] y_dividend = 1000\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 19217\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4602413849\n",
            "[TOP] exp_trunc[9] = 280909\n",
            "[TOP] exp_accum_input[9] = 17556\n",
            "[TOP] x_divider = 10585\n",
            "[TOP] y_dividend = 1097\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 20644\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4944176068\n",
            "[TOP] exp_trunc[9] = 301768\n",
            "[TOP] exp_accum_input[9] = 18860\n",
            "[TOP] x_divider = 10861\n",
            "[TOP] y_dividend = 1178\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 19199\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4598102903\n",
            "[TOP] exp_trunc[9] = 280645\n",
            "[TOP] exp_accum_input[9] = 17540\n",
            "[TOP] x_divider = 10850\n",
            "[TOP] y_dividend = 1096\n",
            "[TOP] data_out[9] = 827\n",
            "[TOP] exp_fraction_output[9] = 25224\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6041072328\n",
            "[TOP] exp_trunc[9] = 368717\n",
            "[TOP] exp_accum_input[9] = 23044\n",
            "[TOP] x_divider = 11252\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 1047\n",
            "[TOP] exp_fraction_output[9] = 23170\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5549145490\n",
            "[TOP] exp_trunc[9] = 338692\n",
            "[TOP] exp_accum_input[9] = 21168\n",
            "[TOP] x_divider = 11840\n",
            "[TOP] y_dividend = 1323\n",
            "[TOP] data_out[9] = 915\n",
            "[TOP] exp_fraction_output[9] = 21353\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5113979441\n",
            "[TOP] exp_trunc[9] = 312132\n",
            "[TOP] exp_accum_input[9] = 19508\n",
            "[TOP] x_divider = 11460\n",
            "[TOP] y_dividend = 1219\n",
            "[TOP] data_out[9] = 871\n",
            "[TOP] exp_fraction_output[9] = 21785\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5217442145\n",
            "[TOP] exp_trunc[9] = 318447\n",
            "[TOP] exp_accum_input[9] = 19902\n",
            "[TOP] x_divider = 10963\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 23107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5534057179\n",
            "[TOP] exp_trunc[9] = 337772\n",
            "[TOP] exp_accum_input[9] = 21110\n",
            "[TOP] x_divider = 11337\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 953\n",
            "[TOP] exp_fraction_output[9] = 22112\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5295757664\n",
            "[TOP] exp_trunc[9] = 323227\n",
            "[TOP] exp_accum_input[9] = 20201\n",
            "[TOP] x_divider = 12541\n",
            "[TOP] y_dividend = 1262\n",
            "[TOP] data_out[9] = 823\n",
            "[TOP] exp_fraction_output[9] = 21790\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5218639630\n",
            "[TOP] exp_trunc[9] = 318520\n",
            "[TOP] exp_accum_input[9] = 19907\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1244\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 25459\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6097354123\n",
            "[TOP] exp_trunc[9] = 372152\n",
            "[TOP] exp_accum_input[9] = 23259\n",
            "[TOP] x_divider = 12078\n",
            "[TOP] y_dividend = 1453\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 28295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6776567615\n",
            "[TOP] exp_trunc[9] = 413608\n",
            "[TOP] exp_accum_input[9] = 25850\n",
            "[TOP] x_divider = 13416\n",
            "[TOP] y_dividend = 1615\n",
            "[TOP] data_out[9] = 987\n",
            "[TOP] exp_fraction_output[9] = 20430\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4892923710\n",
            "[TOP] exp_trunc[9] = 298640\n",
            "[TOP] exp_accum_input[9] = 18665\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 1166\n",
            "[TOP] data_out[9] = 891\n",
            "[TOP] exp_fraction_output[9] = 27047\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6477675359\n",
            "[TOP] exp_trunc[9] = 395365\n",
            "[TOP] exp_accum_input[9] = 24710\n",
            "[TOP] x_divider = 16395\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 771\n",
            "[TOP] exp_fraction_output[9] = 22813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5463645061\n",
            "[TOP] exp_trunc[9] = 333474\n",
            "[TOP] exp_accum_input[9] = 20842\n",
            "[TOP] x_divider = 13514\n",
            "[TOP] y_dividend = 1302\n",
            "[TOP] data_out[9] = 787\n",
            "[TOP] exp_fraction_output[9] = 25706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6156509882\n",
            "[TOP] exp_trunc[9] = 375763\n",
            "[TOP] exp_accum_input[9] = 23485\n",
            "[TOP] x_divider = 13073\n",
            "[TOP] y_dividend = 1467\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 17850\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4275021450\n",
            "[TOP] exp_trunc[9] = 260926\n",
            "[TOP] exp_accum_input[9] = 16307\n",
            "[TOP] x_divider = 11368\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17443\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4177546171\n",
            "[TOP] exp_trunc[9] = 254977\n",
            "[TOP] exp_accum_input[9] = 15936\n",
            "[TOP] x_divider = 11737\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10931\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 799\n",
            "[TOP] exp_fraction_output[9] = 29143\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6979661071\n",
            "[TOP] exp_trunc[9] = 426004\n",
            "[TOP] exp_accum_input[9] = 26625\n",
            "[TOP] x_divider = 14588\n",
            "[TOP] y_dividend = 1664\n",
            "[TOP] data_out[9] = 935\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12761\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 629\n",
            "[TOP] exp_fraction_output[9] = 30236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7241431292\n",
            "[TOP] exp_trunc[9] = 441981\n",
            "[TOP] exp_accum_input[9] = 27623\n",
            "[TOP] x_divider = 15270\n",
            "[TOP] y_dividend = 1726\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 20490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907293530\n",
            "[TOP] exp_trunc[9] = 299517\n",
            "[TOP] exp_accum_input[9] = 18719\n",
            "[TOP] x_divider = 13310\n",
            "[TOP] y_dividend = 1169\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 18792\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4500627624\n",
            "[TOP] exp_trunc[9] = 274696\n",
            "[TOP] exp_accum_input[9] = 17168\n",
            "[TOP] x_divider = 11444\n",
            "[TOP] y_dividend = 1073\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 27416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6566049752\n",
            "[TOP] exp_trunc[9] = 400759\n",
            "[TOP] exp_accum_input[9] = 25047\n",
            "[TOP] x_divider = 11998\n",
            "[TOP] y_dividend = 1565\n",
            "[TOP] data_out[9] = 1067\n",
            "[TOP] exp_fraction_output[9] = 28860\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6911883420\n",
            "[TOP] exp_trunc[9] = 421867\n",
            "[TOP] exp_accum_input[9] = 26366\n",
            "[TOP] x_divider = 13191\n",
            "[TOP] y_dividend = 1647\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 29779\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7131981163\n",
            "[TOP] exp_trunc[9] = 435301\n",
            "[TOP] exp_accum_input[9] = 27206\n",
            "[TOP] x_divider = 13079\n",
            "[TOP] y_dividend = 1700\n",
            "[TOP] data_out[9] = 1063\n",
            "[TOP] exp_fraction_output[9] = 17366\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4159104902\n",
            "[TOP] exp_trunc[9] = 253851\n",
            "[TOP] exp_accum_input[9] = 15865\n",
            "[TOP] x_divider = 11259\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 26536\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6355292392\n",
            "[TOP] exp_trunc[9] = 387896\n",
            "[TOP] exp_accum_input[9] = 24243\n",
            "[TOP] x_divider = 11466\n",
            "[TOP] y_dividend = 1515\n",
            "[TOP] data_out[9] = 1081\n",
            "[TOP] exp_fraction_output[9] = 23193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5554653921\n",
            "[TOP] exp_trunc[9] = 339029\n",
            "[TOP] exp_accum_input[9] = 21189\n",
            "[TOP] x_divider = 11126\n",
            "[TOP] y_dividend = 1324\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23577\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5646620769\n",
            "[TOP] exp_trunc[9] = 344642\n",
            "[TOP] exp_accum_input[9] = 21540\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 1346\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 31609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7570260673\n",
            "[TOP] exp_trunc[9] = 462052\n",
            "[TOP] exp_accum_input[9] = 28878\n",
            "[TOP] x_divider = 13385\n",
            "[TOP] y_dividend = 1804\n",
            "[TOP] data_out[9] = 1103\n",
            "[TOP] exp_fraction_output[9] = 22186\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5313480442\n",
            "[TOP] exp_trunc[9] = 324309\n",
            "[TOP] exp_accum_input[9] = 20269\n",
            "[TOP] x_divider = 12076\n",
            "[TOP] y_dividend = 1266\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 17474\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4184970578\n",
            "[TOP] exp_trunc[9] = 255430\n",
            "[TOP] exp_accum_input[9] = 15964\n",
            "[TOP] x_divider = 10496\n",
            "[TOP] y_dividend = 997\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4312622479\n",
            "[TOP] exp_trunc[9] = 263221\n",
            "[TOP] exp_accum_input[9] = 16451\n",
            "[TOP] x_divider = 11097\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 761\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11796\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 679\n",
            "[TOP] exp_fraction_output[9] = 17161\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4110008017\n",
            "[TOP] exp_trunc[9] = 250854\n",
            "[TOP] exp_accum_input[9] = 15678\n",
            "[TOP] x_divider = 12331\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11544\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 18015\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4314538455\n",
            "[TOP] exp_trunc[9] = 263338\n",
            "[TOP] exp_accum_input[9] = 16458\n",
            "[TOP] x_divider = 10713\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17275\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4137310675\n",
            "[TOP] exp_trunc[9] = 252521\n",
            "[TOP] exp_accum_input[9] = 15782\n",
            "[TOP] x_divider = 11108\n",
            "[TOP] y_dividend = 986\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 17552\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4203651344\n",
            "[TOP] exp_trunc[9] = 256570\n",
            "[TOP] exp_accum_input[9] = 16035\n",
            "[TOP] x_divider = 10350\n",
            "[TOP] y_dividend = 1002\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 22462\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5379581614\n",
            "[TOP] exp_trunc[9] = 328343\n",
            "[TOP] exp_accum_input[9] = 20521\n",
            "[TOP] x_divider = 11290\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 929\n",
            "[TOP] exp_fraction_output[9] = 18009\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4313101473\n",
            "[TOP] exp_trunc[9] = 263250\n",
            "[TOP] exp_accum_input[9] = 16453\n",
            "[TOP] x_divider = 10814\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 18206\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4360282382\n",
            "[TOP] exp_trunc[9] = 266130\n",
            "[TOP] exp_accum_input[9] = 16633\n",
            "[TOP] x_divider = 11258\n",
            "[TOP] y_dividend = 1039\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4148567034\n",
            "[TOP] exp_trunc[9] = 253208\n",
            "[TOP] exp_accum_input[9] = 15825\n",
            "[TOP] x_divider = 10461\n",
            "[TOP] y_dividend = 989\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11719\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 683\n",
            "[TOP] exp_fraction_output[9] = 21032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5037100904\n",
            "[TOP] exp_trunc[9] = 307440\n",
            "[TOP] exp_accum_input[9] = 19215\n",
            "[TOP] x_divider = 11626\n",
            "[TOP] y_dividend = 1200\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 23243\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5566628771\n",
            "[TOP] exp_trunc[9] = 339760\n",
            "[TOP] exp_accum_input[9] = 21235\n",
            "[TOP] x_divider = 12278\n",
            "[TOP] y_dividend = 1327\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 18821\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4507573037\n",
            "[TOP] exp_trunc[9] = 275120\n",
            "[TOP] exp_accum_input[9] = 17195\n",
            "[TOP] x_divider = 13368\n",
            "[TOP] y_dividend = 1074\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 22599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5412392703\n",
            "[TOP] exp_trunc[9] = 330346\n",
            "[TOP] exp_accum_input[9] = 20646\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 1290\n",
            "[TOP] data_out[9] = 959\n",
            "[TOP] exp_fraction_output[9] = 26391\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6320565327\n",
            "[TOP] exp_trunc[9] = 385776\n",
            "[TOP] exp_accum_input[9] = 24111\n",
            "[TOP] x_divider = 14123\n",
            "[TOP] y_dividend = 1506\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 25188\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6032450436\n",
            "[TOP] exp_trunc[9] = 368191\n",
            "[TOP] exp_accum_input[9] = 23011\n",
            "[TOP] x_divider = 14432\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 21416\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5129067752\n",
            "[TOP] exp_trunc[9] = 313053\n",
            "[TOP] exp_accum_input[9] = 19565\n",
            "[TOP] x_divider = 12398\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 807\n",
            "[TOP] exp_fraction_output[9] = 29409\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7043367273\n",
            "[TOP] exp_trunc[9] = 429893\n",
            "[TOP] exp_accum_input[9] = 26868\n",
            "[TOP] x_divider = 14547\n",
            "[TOP] y_dividend = 1679\n",
            "[TOP] data_out[9] = 945\n",
            "[TOP] exp_fraction_output[9] = 25837\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 2276394722\n",
            "[TOP] exp_trunc[9] = 138940\n",
            "[TOP] exp_accum_input[9] = 8683\n",
            "[TOP] x_divider = 6291\n",
            "[TOP] y_dividend = 542\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 24815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5943118055\n",
            "[TOP] exp_trunc[9] = 362739\n",
            "[TOP] exp_accum_input[9] = 22671\n",
            "[TOP] x_divider = 14134\n",
            "[TOP] y_dividend = 1416\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 23324\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5586028028\n",
            "[TOP] exp_trunc[9] = 340944\n",
            "[TOP] exp_accum_input[9] = 21309\n",
            "[TOP] x_divider = 11095\n",
            "[TOP] y_dividend = 1331\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 11195\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 24045\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5758705365\n",
            "[TOP] exp_trunc[9] = 351483\n",
            "[TOP] exp_accum_input[9] = 21967\n",
            "[TOP] x_divider = 13000\n",
            "[TOP] y_dividend = 1372\n",
            "[TOP] data_out[9] = 865\n",
            "[TOP] exp_fraction_output[9] = 29003\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6946131491\n",
            "[TOP] exp_trunc[9] = 423958\n",
            "[TOP] exp_accum_input[9] = 26497\n",
            "[TOP] x_divider = 15963\n",
            "[TOP] y_dividend = 1656\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 27073\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6483902281\n",
            "[TOP] exp_trunc[9] = 395745\n",
            "[TOP] exp_accum_input[9] = 24734\n",
            "[TOP] x_divider = 14354\n",
            "[TOP] y_dividend = 1545\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 27050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6478393850\n",
            "[TOP] exp_trunc[9] = 395409\n",
            "[TOP] exp_accum_input[9] = 24713\n",
            "[TOP] x_divider = 12938\n",
            "[TOP] y_dividend = 1544\n",
            "[TOP] data_out[9] = 977\n",
            "[TOP] exp_fraction_output[9] = 22122\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5298152634\n",
            "[TOP] exp_trunc[9] = 323373\n",
            "[TOP] exp_accum_input[9] = 20210\n",
            "[TOP] x_divider = 13012\n",
            "[TOP] y_dividend = 1263\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 21822\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5226303534\n",
            "[TOP] exp_trunc[9] = 318988\n",
            "[TOP] exp_accum_input[9] = 19936\n",
            "[TOP] x_divider = 12487\n",
            "[TOP] y_dividend = 1246\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 20281\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4857238657\n",
            "[TOP] exp_trunc[9] = 296462\n",
            "[TOP] exp_accum_input[9] = 18528\n",
            "[TOP] x_divider = 12393\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17660\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4229517020\n",
            "[TOP] exp_trunc[9] = 258149\n",
            "[TOP] exp_accum_input[9] = 16134\n",
            "[TOP] x_divider = 12694\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 649\n",
            "[TOP] exp_fraction_output[9] = 23205\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5557527885\n",
            "[TOP] exp_trunc[9] = 339204\n",
            "[TOP] exp_accum_input[9] = 21200\n",
            "[TOP] x_divider = 12836\n",
            "[TOP] y_dividend = 1325\n",
            "[TOP] data_out[9] = 845\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 14111\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14478\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 921\n",
            "[TOP] exp_fraction_output[9] = 17643\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4225445571\n",
            "[TOP] exp_trunc[9] = 257900\n",
            "[TOP] exp_accum_input[9] = 16118\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 18624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4460392128\n",
            "[TOP] exp_trunc[9] = 272240\n",
            "[TOP] exp_accum_input[9] = 17015\n",
            "[TOP] x_divider = 11611\n",
            "[TOP] y_dividend = 1063\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 11091\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18191\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4356689927\n",
            "[TOP] exp_trunc[9] = 265911\n",
            "[TOP] exp_accum_input[9] = 16619\n",
            "[TOP] x_divider = 11707\n",
            "[TOP] y_dividend = 1038\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 26258\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6288712226\n",
            "[TOP] exp_trunc[9] = 383832\n",
            "[TOP] exp_accum_input[9] = 23989\n",
            "[TOP] x_divider = 12630\n",
            "[TOP] y_dividend = 1499\n",
            "[TOP] data_out[9] = 973\n",
            "[TOP] exp_fraction_output[9] = 22637\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5421493589\n",
            "[TOP] exp_trunc[9] = 330901\n",
            "[TOP] exp_accum_input[9] = 20681\n",
            "[TOP] x_divider = 11913\n",
            "[TOP] y_dividend = 1292\n",
            "[TOP] data_out[9] = 889\n",
            "[TOP] exp_fraction_output[9] = 22007\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5270610479\n",
            "[TOP] exp_trunc[9] = 321692\n",
            "[TOP] exp_accum_input[9] = 20105\n",
            "[TOP] x_divider = 11733\n",
            "[TOP] y_dividend = 1256\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 21421\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5130265237\n",
            "[TOP] exp_trunc[9] = 313126\n",
            "[TOP] exp_accum_input[9] = 19570\n",
            "[TOP] x_divider = 11387\n",
            "[TOP] y_dividend = 1223\n",
            "[TOP] data_out[9] = 879\n",
            "[TOP] exp_fraction_output[9] = 20618\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4937949146\n",
            "[TOP] exp_trunc[9] = 301388\n",
            "[TOP] exp_accum_input[9] = 18836\n",
            "[TOP] x_divider = 12339\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 23709\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5678234373\n",
            "[TOP] exp_trunc[9] = 346571\n",
            "[TOP] exp_accum_input[9] = 21660\n",
            "[TOP] x_divider = 12533\n",
            "[TOP] y_dividend = 1353\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 18351\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4395009447\n",
            "[TOP] exp_trunc[9] = 268250\n",
            "[TOP] exp_accum_input[9] = 16765\n",
            "[TOP] x_divider = 11976\n",
            "[TOP] y_dividend = 1047\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 27286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6534915142\n",
            "[TOP] exp_trunc[9] = 398859\n",
            "[TOP] exp_accum_input[9] = 24928\n",
            "[TOP] x_divider = 12974\n",
            "[TOP] y_dividend = 1558\n",
            "[TOP] data_out[9] = 985\n",
            "[TOP] exp_fraction_output[9] = 21316\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5105118052\n",
            "[TOP] exp_trunc[9] = 311591\n",
            "[TOP] exp_accum_input[9] = 19474\n",
            "[TOP] x_divider = 12248\n",
            "[TOP] y_dividend = 1217\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 23018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5512741946\n",
            "[TOP] exp_trunc[9] = 336471\n",
            "[TOP] exp_accum_input[9] = 21029\n",
            "[TOP] x_divider = 11794\n",
            "[TOP] y_dividend = 1314\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 23817\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5704100049\n",
            "[TOP] exp_trunc[9] = 348150\n",
            "[TOP] exp_accum_input[9] = 21759\n",
            "[TOP] x_divider = 11722\n",
            "[TOP] y_dividend = 1359\n",
            "[TOP] data_out[9] = 951\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 11659\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 22490\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5386287530\n",
            "[TOP] exp_trunc[9] = 328752\n",
            "[TOP] exp_accum_input[9] = 20547\n",
            "[TOP] x_divider = 10790\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 975\n",
            "[TOP] exp_fraction_output[9] = 23880\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5719188360\n",
            "[TOP] exp_trunc[9] = 349071\n",
            "[TOP] exp_accum_input[9] = 21816\n",
            "[TOP] x_divider = 11254\n",
            "[TOP] y_dividend = 1363\n",
            "[TOP] data_out[9] = 993\n",
            "[TOP] exp_fraction_output[9] = 19652\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4706595044\n",
            "[TOP] exp_trunc[9] = 287267\n",
            "[TOP] exp_accum_input[9] = 17954\n",
            "[TOP] x_divider = 11006\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 833\n",
            "[TOP] exp_fraction_output[9] = 18736\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4487215792\n",
            "[TOP] exp_trunc[9] = 273877\n",
            "[TOP] exp_accum_input[9] = 17117\n",
            "[TOP] x_divider = 10769\n",
            "[TOP] y_dividend = 1069\n",
            "[TOP] data_out[9] = 813\n",
            "[TOP] exp_fraction_output[9] = 17397\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4166529309\n",
            "[TOP] exp_trunc[9] = 254304\n",
            "[TOP] exp_accum_input[9] = 15894\n",
            "[TOP] x_divider = 11350\n",
            "[TOP] y_dividend = 993\n",
            "[TOP] data_out[9] = 717\n",
            "[TOP] exp_fraction_output[9] = 20235\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4846221795\n",
            "[TOP] exp_trunc[9] = 295789\n",
            "[TOP] exp_accum_input[9] = 18486\n",
            "[TOP] x_divider = 11572\n",
            "[TOP] y_dividend = 1155\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 22901\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5484720797\n",
            "[TOP] exp_trunc[9] = 334760\n",
            "[TOP] exp_accum_input[9] = 20922\n",
            "[TOP] x_divider = 10916\n",
            "[TOP] y_dividend = 1307\n",
            "[TOP] data_out[9] = 983\n",
            "[TOP] exp_fraction_output[9] = 23824\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5705776528\n",
            "[TOP] exp_trunc[9] = 348252\n",
            "[TOP] exp_accum_input[9] = 21765\n",
            "[TOP] x_divider = 11600\n",
            "[TOP] y_dividend = 1360\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17979\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4305916563\n",
            "[TOP] exp_trunc[9] = 262812\n",
            "[TOP] exp_accum_input[9] = 16425\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1026\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 22322\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5346052034\n",
            "[TOP] exp_trunc[9] = 326297\n",
            "[TOP] exp_accum_input[9] = 20393\n",
            "[TOP] x_divider = 12478\n",
            "[TOP] y_dividend = 1274\n",
            "[TOP] data_out[9] = 837\n",
            "[TOP] exp_fraction_output[9] = 22922\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5489750234\n",
            "[TOP] exp_trunc[9] = 335067\n",
            "[TOP] exp_accum_input[9] = 20941\n",
            "[TOP] x_divider = 14237\n",
            "[TOP] y_dividend = 1308\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17815\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4266639055\n",
            "[TOP] exp_trunc[9] = 260414\n",
            "[TOP] exp_accum_input[9] = 16275\n",
            "[TOP] x_divider = 10837\n",
            "[TOP] y_dividend = 1017\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10951\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 731\n",
            "[TOP] exp_fraction_output[9] = 18715\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4482186355\n",
            "[TOP] exp_trunc[9] = 273570\n",
            "[TOP] exp_accum_input[9] = 17098\n",
            "[TOP] x_divider = 13642\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 641\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 10978\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 17427\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4173714219\n",
            "[TOP] exp_trunc[9] = 254743\n",
            "[TOP] exp_accum_input[9] = 15921\n",
            "[TOP] x_divider = 11654\n",
            "[TOP] y_dividend = 995\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 20290\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4859394130\n",
            "[TOP] exp_trunc[9] = 296593\n",
            "[TOP] exp_accum_input[9] = 18537\n",
            "[TOP] x_divider = 11075\n",
            "[TOP] y_dividend = 1158\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 28032\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6713579904\n",
            "[TOP] exp_trunc[9] = 409764\n",
            "[TOP] exp_accum_input[9] = 25610\n",
            "[TOP] x_divider = 17852\n",
            "[TOP] y_dividend = 1600\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 18082\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4330584754\n",
            "[TOP] exp_trunc[9] = 264317\n",
            "[TOP] exp_accum_input[9] = 16519\n",
            "[TOP] x_divider = 11156\n",
            "[TOP] y_dividend = 1032\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13246\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 605\n",
            "[TOP] exp_fraction_output[9] = 36600\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8765590200\n",
            "[TOP] exp_trunc[9] = 535009\n",
            "[TOP] exp_accum_input[9] = 33438\n",
            "[TOP] x_divider = 19558\n",
            "[TOP] y_dividend = 2089\n",
            "[TOP] data_out[9] = 875\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11546\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 21696\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5196126912\n",
            "[TOP] exp_trunc[9] = 317146\n",
            "[TOP] exp_accum_input[9] = 19821\n",
            "[TOP] x_divider = 10919\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 24254\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5808760238\n",
            "[TOP] exp_trunc[9] = 354538\n",
            "[TOP] exp_accum_input[9] = 22158\n",
            "[TOP] x_divider = 12597\n",
            "[TOP] y_dividend = 1384\n",
            "[TOP] data_out[9] = 899\n",
            "[TOP] exp_fraction_output[9] = 25193\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6033647921\n",
            "[TOP] exp_trunc[9] = 368264\n",
            "[TOP] exp_accum_input[9] = 23016\n",
            "[TOP] x_divider = 13186\n",
            "[TOP] y_dividend = 1438\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 23121\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5537410137\n",
            "[TOP] exp_trunc[9] = 337976\n",
            "[TOP] exp_accum_input[9] = 21123\n",
            "[TOP] x_divider = 13934\n",
            "[TOP] y_dividend = 1320\n",
            "[TOP] data_out[9] = 775\n",
            "[TOP] exp_fraction_output[9] = 18167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4350941999\n",
            "[TOP] exp_trunc[9] = 265560\n",
            "[TOP] exp_accum_input[9] = 16597\n",
            "[TOP] x_divider = 11412\n",
            "[TOP] y_dividend = 1037\n",
            "[TOP] data_out[9] = 745\n",
            "[TOP] exp_fraction_output[9] = 24070\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5764692790\n",
            "[TOP] exp_trunc[9] = 351848\n",
            "[TOP] exp_accum_input[9] = 21990\n",
            "[TOP] x_divider = 14412\n",
            "[TOP] y_dividend = 1374\n",
            "[TOP] data_out[9] = 781\n",
            "[TOP] exp_fraction_output[9] = 17346\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4154314962\n",
            "[TOP] exp_trunc[9] = 253559\n",
            "[TOP] exp_accum_input[9] = 15847\n",
            "[TOP] x_divider = 10781\n",
            "[TOP] y_dividend = 990\n",
            "[TOP] data_out[9] = 751\n",
            "[TOP] exp_fraction_output[9] = 19609\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4696296673\n",
            "[TOP] exp_trunc[9] = 286639\n",
            "[TOP] exp_accum_input[9] = 17914\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1119\n",
            "[TOP] data_out[9] = 817\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11493\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17639\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4224487583\n",
            "[TOP] exp_trunc[9] = 257842\n",
            "[TOP] exp_accum_input[9] = 16115\n",
            "[TOP] x_divider = 11369\n",
            "[TOP] y_dividend = 1007\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 21242\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5087395274\n",
            "[TOP] exp_trunc[9] = 310509\n",
            "[TOP] exp_accum_input[9] = 19406\n",
            "[TOP] x_divider = 11535\n",
            "[TOP] y_dividend = 1212\n",
            "[TOP] data_out[9] = 859\n",
            "[TOP] exp_fraction_output[9] = 19945\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4776767665\n",
            "[TOP] exp_trunc[9] = 291550\n",
            "[TOP] exp_accum_input[9] = 18221\n",
            "[TOP] x_divider = 11845\n",
            "[TOP] y_dividend = 1138\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 18787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4499430139\n",
            "[TOP] exp_trunc[9] = 274623\n",
            "[TOP] exp_accum_input[9] = 17163\n",
            "[TOP] x_divider = 11035\n",
            "[TOP] y_dividend = 1072\n",
            "[TOP] data_out[9] = 795\n",
            "[TOP] exp_fraction_output[9] = 17706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4240533882\n",
            "[TOP] exp_trunc[9] = 258821\n",
            "[TOP] exp_accum_input[9] = 16176\n",
            "[TOP] x_divider = 11067\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 24642\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5901685074\n",
            "[TOP] exp_trunc[9] = 360210\n",
            "[TOP] exp_accum_input[9] = 22513\n",
            "[TOP] x_divider = 12545\n",
            "[TOP] y_dividend = 1407\n",
            "[TOP] data_out[9] = 919\n",
            "[TOP] exp_fraction_output[9] = 30984\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7420575048\n",
            "[TOP] exp_trunc[9] = 452915\n",
            "[TOP] exp_accum_input[9] = 28307\n",
            "[TOP] x_divider = 12499\n",
            "[TOP] y_dividend = 1769\n",
            "[TOP] data_out[9] = 1157\n",
            "[TOP] exp_fraction_output[9] = 24348\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5831272956\n",
            "[TOP] exp_trunc[9] = 355912\n",
            "[TOP] exp_accum_input[9] = 22244\n",
            "[TOP] x_divider = 12721\n",
            "[TOP] y_dividend = 1390\n",
            "[TOP] data_out[9] = 895\n",
            "[TOP] exp_fraction_output[9] = 28716\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6877395852\n",
            "[TOP] exp_trunc[9] = 419762\n",
            "[TOP] exp_accum_input[9] = 26235\n",
            "[TOP] x_divider = 12710\n",
            "[TOP] y_dividend = 1639\n",
            "[TOP] data_out[9] = 1055\n",
            "[TOP] exp_fraction_output[9] = 25229\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6042269813\n",
            "[TOP] exp_trunc[9] = 368790\n",
            "[TOP] exp_accum_input[9] = 23049\n",
            "[TOP] x_divider = 12639\n",
            "[TOP] y_dividend = 1440\n",
            "[TOP] data_out[9] = 933\n",
            "[TOP] exp_fraction_output[9] = 23605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5653326685\n",
            "[TOP] exp_trunc[9] = 345051\n",
            "[TOP] exp_accum_input[9] = 21565\n",
            "[TOP] x_divider = 10784\n",
            "[TOP] y_dividend = 1347\n",
            "[TOP] data_out[9] = 1023\n",
            "[TOP] exp_fraction_output[9] = 17590\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4212752230\n",
            "[TOP] exp_trunc[9] = 257125\n",
            "[TOP] exp_accum_input[9] = 16070\n",
            "[TOP] x_divider = 10622\n",
            "[TOP] y_dividend = 1004\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 27720\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6638856840\n",
            "[TOP] exp_trunc[9] = 405203\n",
            "[TOP] exp_accum_input[9] = 25325\n",
            "[TOP] x_divider = 13212\n",
            "[TOP] y_dividend = 1582\n",
            "[TOP] data_out[9] = 981\n",
            "[TOP] exp_fraction_output[9] = 22973\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5501964581\n",
            "[TOP] exp_trunc[9] = 335813\n",
            "[TOP] exp_accum_input[9] = 20988\n",
            "[TOP] x_divider = 11058\n",
            "[TOP] y_dividend = 1311\n",
            "[TOP] data_out[9] = 971\n",
            "[TOP] exp_fraction_output[9] = 22503\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5389400991\n",
            "[TOP] exp_trunc[9] = 328942\n",
            "[TOP] exp_accum_input[9] = 20558\n",
            "[TOP] x_divider = 11890\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 885\n",
            "[TOP] exp_fraction_output[9] = 26910\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6444864270\n",
            "[TOP] exp_trunc[9] = 393363\n",
            "[TOP] exp_accum_input[9] = 24585\n",
            "[TOP] x_divider = 13004\n",
            "[TOP] y_dividend = 1536\n",
            "[TOP] data_out[9] = 967\n",
            "[TOP] exp_fraction_output[9] = 21042\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5039495874\n",
            "[TOP] exp_trunc[9] = 307586\n",
            "[TOP] exp_accum_input[9] = 19224\n",
            "[TOP] x_divider = 15418\n",
            "[TOP] y_dividend = 1201\n",
            "[TOP] data_out[9] = 639\n",
            "[TOP] exp_fraction_output[9] = 17611\n",
            "[TOP] exp_int_output[9] = 88106\n",
            "[TOP] exp_whole[9] = 1551634766\n",
            "[TOP] exp_trunc[9] = 94704\n",
            "[TOP] exp_accum_input[9] = 5919\n",
            "[TOP] x_divider = 6579\n",
            "[TOP] y_dividend = 369\n",
            "[TOP] data_out[9] = 457\n",
            "[TOP] exp_fraction_output[9] = 19103\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4575111191\n",
            "[TOP] exp_trunc[9] = 279242\n",
            "[TOP] exp_accum_input[9] = 17452\n",
            "[TOP] x_divider = 14043\n",
            "[TOP] y_dividend = 1090\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12901\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 619\n",
            "[TOP] exp_fraction_output[9] = 21849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5232769953\n",
            "[TOP] exp_trunc[9] = 319382\n",
            "[TOP] exp_accum_input[9] = 19961\n",
            "[TOP] x_divider = 12718\n",
            "[TOP] y_dividend = 1247\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 16437\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 493\n",
            "[TOP] exp_fraction_output[9] = 17358\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4157188926\n",
            "[TOP] exp_trunc[9] = 253734\n",
            "[TOP] exp_accum_input[9] = 15858\n",
            "[TOP] x_divider = 14744\n",
            "[TOP] y_dividend = 991\n",
            "[TOP] data_out[9] = 551\n",
            "[TOP] exp_fraction_output[9] = 22095\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5291686215\n",
            "[TOP] exp_trunc[9] = 322978\n",
            "[TOP] exp_accum_input[9] = 20186\n",
            "[TOP] x_divider = 14721\n",
            "[TOP] y_dividend = 1261\n",
            "[TOP] data_out[9] = 703\n",
            "[TOP] exp_fraction_output[9] = 26061\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6241531317\n",
            "[TOP] exp_trunc[9] = 380952\n",
            "[TOP] exp_accum_input[9] = 23809\n",
            "[TOP] x_divider = 16576\n",
            "[TOP] y_dividend = 1488\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 26286\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6295418142\n",
            "[TOP] exp_trunc[9] = 384241\n",
            "[TOP] exp_accum_input[9] = 24015\n",
            "[TOP] x_divider = 15505\n",
            "[TOP] y_dividend = 1500\n",
            "[TOP] data_out[9] = 793\n",
            "[TOP] exp_fraction_output[9] = 18251\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4371059747\n",
            "[TOP] exp_trunc[9] = 266788\n",
            "[TOP] exp_accum_input[9] = 16674\n",
            "[TOP] x_divider = 11178\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 765\n",
            "[TOP] exp_fraction_output[9] = 17314\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4146651058\n",
            "[TOP] exp_trunc[9] = 253091\n",
            "[TOP] exp_accum_input[9] = 15818\n",
            "[TOP] x_divider = 11015\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 17155\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4108571035\n",
            "[TOP] exp_trunc[9] = 250767\n",
            "[TOP] exp_accum_input[9] = 15672\n",
            "[TOP] x_divider = 10723\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 749\n",
            "[TOP] exp_fraction_output[9] = 21783\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5216963151\n",
            "[TOP] exp_trunc[9] = 318418\n",
            "[TOP] exp_accum_input[9] = 19901\n",
            "[TOP] x_divider = 13081\n",
            "[TOP] y_dividend = 1243\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 24438\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5852827686\n",
            "[TOP] exp_trunc[9] = 357228\n",
            "[TOP] exp_accum_input[9] = 22326\n",
            "[TOP] x_divider = 12876\n",
            "[TOP] y_dividend = 1395\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 22203\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5317551891\n",
            "[TOP] exp_trunc[9] = 324557\n",
            "[TOP] exp_accum_input[9] = 20284\n",
            "[TOP] x_divider = 11374\n",
            "[TOP] y_dividend = 1267\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 20522\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4914957434\n",
            "[TOP] exp_trunc[9] = 299985\n",
            "[TOP] exp_accum_input[9] = 18749\n",
            "[TOP] x_divider = 12385\n",
            "[TOP] y_dividend = 1171\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 24902\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5963954294\n",
            "[TOP] exp_trunc[9] = 364010\n",
            "[TOP] exp_accum_input[9] = 22750\n",
            "[TOP] x_divider = 12596\n",
            "[TOP] y_dividend = 1421\n",
            "[TOP] data_out[9] = 925\n",
            "[TOP] exp_fraction_output[9] = 21406\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5126672782\n",
            "[TOP] exp_trunc[9] = 312907\n",
            "[TOP] exp_accum_input[9] = 19556\n",
            "[TOP] x_divider = 11043\n",
            "[TOP] y_dividend = 1222\n",
            "[TOP] data_out[9] = 905\n",
            "[TOP] exp_fraction_output[9] = 19576\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4688393272\n",
            "[TOP] exp_trunc[9] = 286156\n",
            "[TOP] exp_accum_input[9] = 17884\n",
            "[TOP] x_divider = 11828\n",
            "[TOP] y_dividend = 1117\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 35963\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8613030611\n",
            "[TOP] exp_trunc[9] = 525697\n",
            "[TOP] exp_accum_input[9] = 32856\n",
            "[TOP] x_divider = 15390\n",
            "[TOP] y_dividend = 2053\n",
            "[TOP] data_out[9] = 1091\n",
            "[TOP] exp_fraction_output[9] = 32277\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7730244669\n",
            "[TOP] exp_trunc[9] = 471816\n",
            "[TOP] exp_accum_input[9] = 29488\n",
            "[TOP] x_divider = 12203\n",
            "[TOP] y_dividend = 1843\n",
            "[TOP] data_out[9] = 1237\n",
            "[TOP] exp_fraction_output[9] = 23105\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5533578185\n",
            "[TOP] exp_trunc[9] = 337742\n",
            "[TOP] exp_accum_input[9] = 21108\n",
            "[TOP] x_divider = 10676\n",
            "[TOP] y_dividend = 1319\n",
            "[TOP] data_out[9] = 1015\n",
            "[TOP] exp_fraction_output[9] = 30295\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7255561615\n",
            "[TOP] exp_trunc[9] = 442844\n",
            "[TOP] exp_accum_input[9] = 27677\n",
            "[TOP] x_divider = 11700\n",
            "[TOP] y_dividend = 1729\n",
            "[TOP] data_out[9] = 1211\n",
            "[TOP] exp_fraction_output[9] = 22467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5380779099\n",
            "[TOP] exp_trunc[9] = 328416\n",
            "[TOP] exp_accum_input[9] = 20526\n",
            "[TOP] x_divider = 11189\n",
            "[TOP] y_dividend = 1282\n",
            "[TOP] data_out[9] = 939\n",
            "[TOP] exp_fraction_output[9] = 26878\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6437200366\n",
            "[TOP] exp_trunc[9] = 392895\n",
            "[TOP] exp_accum_input[9] = 24555\n",
            "[TOP] x_divider = 10882\n",
            "[TOP] y_dividend = 1534\n",
            "[TOP] data_out[9] = 1153\n",
            "[TOP] exp_fraction_output[9] = 22050\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5280908850\n",
            "[TOP] exp_trunc[9] = 322321\n",
            "[TOP] exp_accum_input[9] = 20145\n",
            "[TOP] x_divider = 11313\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 913\n",
            "[TOP] exp_fraction_output[9] = 30634\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7336751098\n",
            "[TOP] exp_trunc[9] = 447799\n",
            "[TOP] exp_accum_input[9] = 27987\n",
            "[TOP] x_divider = 12590\n",
            "[TOP] y_dividend = 1749\n",
            "[TOP] data_out[9] = 1137\n",
            "[TOP] exp_fraction_output[9] = 35180\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 8425504460\n",
            "[TOP] exp_trunc[9] = 514251\n",
            "[TOP] exp_accum_input[9] = 32140\n",
            "[TOP] x_divider = 12922\n",
            "[TOP] y_dividend = 2008\n",
            "[TOP] data_out[9] = 1273\n",
            "[TOP] exp_fraction_output[9] = 17157\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4109050029\n",
            "[TOP] exp_trunc[9] = 250796\n",
            "[TOP] exp_accum_input[9] = 15674\n",
            "[TOP] x_divider = 10220\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19405\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4647439285\n",
            "[TOP] exp_trunc[9] = 283657\n",
            "[TOP] exp_accum_input[9] = 17728\n",
            "[TOP] x_divider = 11390\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10802\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 20492\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4907772524\n",
            "[TOP] exp_trunc[9] = 299546\n",
            "[TOP] exp_accum_input[9] = 18721\n",
            "[TOP] x_divider = 11221\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 855\n",
            "[TOP] exp_fraction_output[9] = 17255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4132520735\n",
            "[TOP] exp_trunc[9] = 252229\n",
            "[TOP] exp_accum_input[9] = 15764\n",
            "[TOP] x_divider = 10370\n",
            "[TOP] y_dividend = 985\n",
            "[TOP] data_out[9] = 777\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10791\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 741\n",
            "[TOP] exp_fraction_output[9] = 18706\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4480030882\n",
            "[TOP] exp_trunc[9] = 273439\n",
            "[TOP] exp_accum_input[9] = 17089\n",
            "[TOP] x_divider = 11918\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 735\n",
            "[TOP] exp_fraction_output[9] = 22063\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5284022311\n",
            "[TOP] exp_trunc[9] = 322511\n",
            "[TOP] exp_accum_input[9] = 20156\n",
            "[TOP] x_divider = 11646\n",
            "[TOP] y_dividend = 1259\n",
            "[TOP] data_out[9] = 887\n",
            "[TOP] exp_fraction_output[9] = 20899\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5005247803\n",
            "[TOP] exp_trunc[9] = 305496\n",
            "[TOP] exp_accum_input[9] = 19093\n",
            "[TOP] x_divider = 11405\n",
            "[TOP] y_dividend = 1193\n",
            "[TOP] data_out[9] = 857\n",
            "[TOP] exp_fraction_output[9] = 18669\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4471169493\n",
            "[TOP] exp_trunc[9] = 272898\n",
            "[TOP] exp_accum_input[9] = 17056\n",
            "[TOP] x_divider = 10642\n",
            "[TOP] y_dividend = 1066\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10875\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17654\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4228080038\n",
            "[TOP] exp_trunc[9] = 258061\n",
            "[TOP] exp_accum_input[9] = 16128\n",
            "[TOP] x_divider = 10935\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 21467\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5141282099\n",
            "[TOP] exp_trunc[9] = 313798\n",
            "[TOP] exp_accum_input[9] = 19612\n",
            "[TOP] x_divider = 12696\n",
            "[TOP] y_dividend = 1225\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19676\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4712342972\n",
            "[TOP] exp_trunc[9] = 287618\n",
            "[TOP] exp_accum_input[9] = 17976\n",
            "[TOP] x_divider = 10868\n",
            "[TOP] y_dividend = 1123\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 19307\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4623968579\n",
            "[TOP] exp_trunc[9] = 282224\n",
            "[TOP] exp_accum_input[9] = 17639\n",
            "[TOP] x_divider = 12526\n",
            "[TOP] y_dividend = 1102\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 18101\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4335135197\n",
            "[TOP] exp_trunc[9] = 264595\n",
            "[TOP] exp_accum_input[9] = 16537\n",
            "[TOP] x_divider = 13797\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 613\n",
            "[TOP] exp_fraction_output[9] = 26035\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6235304395\n",
            "[TOP] exp_trunc[9] = 380572\n",
            "[TOP] exp_accum_input[9] = 23785\n",
            "[TOP] x_divider = 15724\n",
            "[TOP] y_dividend = 1486\n",
            "[TOP] data_out[9] = 773\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12401\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25004\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5988382988\n",
            "[TOP] exp_trunc[9] = 365501\n",
            "[TOP] exp_accum_input[9] = 22843\n",
            "[TOP] x_divider = 13231\n",
            "[TOP] y_dividend = 1427\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 26321\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6303800537\n",
            "[TOP] exp_trunc[9] = 384753\n",
            "[TOP] exp_accum_input[9] = 24047\n",
            "[TOP] x_divider = 14992\n",
            "[TOP] y_dividend = 1502\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 20624\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4939386128\n",
            "[TOP] exp_trunc[9] = 301476\n",
            "[TOP] exp_accum_input[9] = 18842\n",
            "[TOP] x_divider = 11761\n",
            "[TOP] y_dividend = 1177\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 20166\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4829696502\n",
            "[TOP] exp_trunc[9] = 294781\n",
            "[TOP] exp_accum_input[9] = 18423\n",
            "[TOP] x_divider = 10880\n",
            "[TOP] y_dividend = 1151\n",
            "[TOP] data_out[9] = 867\n",
            "[TOP] exp_fraction_output[9] = 23156\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5545792532\n",
            "[TOP] exp_trunc[9] = 338488\n",
            "[TOP] exp_accum_input[9] = 21155\n",
            "[TOP] x_divider = 11724\n",
            "[TOP] y_dividend = 1322\n",
            "[TOP] data_out[9] = 923\n",
            "[TOP] exp_fraction_output[9] = 23385\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5600637345\n",
            "[TOP] exp_trunc[9] = 341835\n",
            "[TOP] exp_accum_input[9] = 21364\n",
            "[TOP] x_divider = 11817\n",
            "[TOP] y_dividend = 1335\n",
            "[TOP] data_out[9] = 927\n",
            "[TOP] exp_fraction_output[9] = 19014\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4553795958\n",
            "[TOP] exp_trunc[9] = 277941\n",
            "[TOP] exp_accum_input[9] = 17371\n",
            "[TOP] x_divider = 11092\n",
            "[TOP] y_dividend = 1085\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 17708\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4241012876\n",
            "[TOP] exp_trunc[9] = 258850\n",
            "[TOP] exp_accum_input[9] = 16178\n",
            "[TOP] x_divider = 11523\n",
            "[TOP] y_dividend = 1011\n",
            "[TOP] data_out[9] = 719\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11296\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 709\n",
            "[TOP] exp_fraction_output[9] = 18255\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4372017735\n",
            "[TOP] exp_trunc[9] = 266846\n",
            "[TOP] exp_accum_input[9] = 16677\n",
            "[TOP] x_divider = 10387\n",
            "[TOP] y_dividend = 1042\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 19926\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4772217222\n",
            "[TOP] exp_trunc[9] = 291273\n",
            "[TOP] exp_accum_input[9] = 18204\n",
            "[TOP] x_divider = 11048\n",
            "[TOP] y_dividend = 1137\n",
            "[TOP] data_out[9] = 843\n",
            "[TOP] exp_fraction_output[9] = 17804\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4264004588\n",
            "[TOP] exp_trunc[9] = 260254\n",
            "[TOP] exp_accum_input[9] = 16265\n",
            "[TOP] x_divider = 12129\n",
            "[TOP] y_dividend = 1016\n",
            "[TOP] data_out[9] = 687\n",
            "[TOP] exp_fraction_output[9] = 19811\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4744675067\n",
            "[TOP] exp_trunc[9] = 289591\n",
            "[TOP] exp_accum_input[9] = 18099\n",
            "[TOP] x_divider = 11417\n",
            "[TOP] y_dividend = 1131\n",
            "[TOP] data_out[9] = 811\n",
            "[TOP] exp_fraction_output[9] = 19949\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4777725653\n",
            "[TOP] exp_trunc[9] = 291609\n",
            "[TOP] exp_accum_input[9] = 18225\n",
            "[TOP] x_divider = 10601\n",
            "[TOP] y_dividend = 1139\n",
            "[TOP] data_out[9] = 881\n",
            "[TOP] exp_fraction_output[9] = 22496\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5387724512\n",
            "[TOP] exp_trunc[9] = 328840\n",
            "[TOP] exp_accum_input[9] = 20552\n",
            "[TOP] x_divider = 11908\n",
            "[TOP] y_dividend = 1284\n",
            "[TOP] data_out[9] = 883\n",
            "[TOP] exp_fraction_output[9] = 18107\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4336572179\n",
            "[TOP] exp_trunc[9] = 264683\n",
            "[TOP] exp_accum_input[9] = 16542\n",
            "[TOP] x_divider = 12238\n",
            "[TOP] y_dividend = 1033\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10968\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 733\n",
            "[TOP] exp_fraction_output[9] = 24557\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5881327829\n",
            "[TOP] exp_trunc[9] = 358967\n",
            "[TOP] exp_accum_input[9] = 22435\n",
            "[TOP] x_divider = 12595\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 911\n",
            "[TOP] exp_fraction_output[9] = 18136\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4343517592\n",
            "[TOP] exp_trunc[9] = 265107\n",
            "[TOP] exp_accum_input[9] = 16569\n",
            "[TOP] x_divider = 11755\n",
            "[TOP] y_dividend = 1035\n",
            "[TOP] data_out[9] = 721\n",
            "[TOP] exp_fraction_output[9] = 17764\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4254424708\n",
            "[TOP] exp_trunc[9] = 259669\n",
            "[TOP] exp_accum_input[9] = 16229\n",
            "[TOP] x_divider = 11186\n",
            "[TOP] y_dividend = 1014\n",
            "[TOP] data_out[9] = 743\n",
            "[TOP] exp_fraction_output[9] = 18154\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4347828538\n",
            "[TOP] exp_trunc[9] = 265370\n",
            "[TOP] exp_accum_input[9] = 16585\n",
            "[TOP] x_divider = 10758\n",
            "[TOP] y_dividend = 1036\n",
            "[TOP] data_out[9] = 789\n",
            "[TOP] exp_fraction_output[9] = 18704\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4479551888\n",
            "[TOP] exp_trunc[9] = 273410\n",
            "[TOP] exp_accum_input[9] = 17088\n",
            "[TOP] x_divider = 10911\n",
            "[TOP] y_dividend = 1068\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 18378\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4401475866\n",
            "[TOP] exp_trunc[9] = 268644\n",
            "[TOP] exp_accum_input[9] = 16790\n",
            "[TOP] x_divider = 10707\n",
            "[TOP] y_dividend = 1049\n",
            "[TOP] data_out[9] = 801\n",
            "[TOP] exp_fraction_output[9] = 23493\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5626503021\n",
            "[TOP] exp_trunc[9] = 343414\n",
            "[TOP] exp_accum_input[9] = 21463\n",
            "[TOP] x_divider = 15737\n",
            "[TOP] y_dividend = 1341\n",
            "[TOP] data_out[9] = 699\n",
            "[TOP] exp_fraction_output[9] = 18876\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4520745372\n",
            "[TOP] exp_trunc[9] = 275924\n",
            "[TOP] exp_accum_input[9] = 17245\n",
            "[TOP] x_divider = 13422\n",
            "[TOP] y_dividend = 1077\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20033\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4797843401\n",
            "[TOP] exp_trunc[9] = 292837\n",
            "[TOP] exp_accum_input[9] = 18302\n",
            "[TOP] x_divider = 15293\n",
            "[TOP] y_dividend = 1143\n",
            "[TOP] data_out[9] = 611\n",
            "[TOP] exp_fraction_output[9] = 21812\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5223908564\n",
            "[TOP] exp_trunc[9] = 318842\n",
            "[TOP] exp_accum_input[9] = 19927\n",
            "[TOP] x_divider = 14613\n",
            "[TOP] y_dividend = 1245\n",
            "[TOP] data_out[9] = 697\n",
            "[TOP] exp_fraction_output[9] = 17478\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4185928566\n",
            "[TOP] exp_trunc[9] = 255488\n",
            "[TOP] exp_accum_input[9] = 15968\n",
            "[TOP] x_divider = 12454\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 655\n",
            "[TOP] exp_fraction_output[9] = 17486\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4187844542\n",
            "[TOP] exp_trunc[9] = 255605\n",
            "[TOP] exp_accum_input[9] = 15975\n",
            "[TOP] x_divider = 12297\n",
            "[TOP] y_dividend = 998\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 20813\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4984651061\n",
            "[TOP] exp_trunc[9] = 304238\n",
            "[TOP] exp_accum_input[9] = 19014\n",
            "[TOP] x_divider = 12933\n",
            "[TOP] y_dividend = 1188\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 20849\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4993272953\n",
            "[TOP] exp_trunc[9] = 304765\n",
            "[TOP] exp_accum_input[9] = 19047\n",
            "[TOP] x_divider = 12914\n",
            "[TOP] y_dividend = 1190\n",
            "[TOP] data_out[9] = 755\n",
            "[TOP] exp_fraction_output[9] = 17167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4111444999\n",
            "[TOP] exp_trunc[9] = 250942\n",
            "[TOP] exp_accum_input[9] = 15683\n",
            "[TOP] x_divider = 11978\n",
            "[TOP] y_dividend = 980\n",
            "[TOP] data_out[9] = 671\n",
            "[TOP] exp_fraction_output[9] = 19967\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4782036599\n",
            "[TOP] exp_trunc[9] = 291872\n",
            "[TOP] exp_accum_input[9] = 18242\n",
            "[TOP] x_divider = 12874\n",
            "[TOP] y_dividend = 1140\n",
            "[TOP] data_out[9] = 725\n",
            "[TOP] exp_fraction_output[9] = 18599\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4454404703\n",
            "[TOP] exp_trunc[9] = 271875\n",
            "[TOP] exp_accum_input[9] = 16992\n",
            "[TOP] x_divider = 13205\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 657\n",
            "[TOP] exp_fraction_output[9] = 20372\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4879032884\n",
            "[TOP] exp_trunc[9] = 297792\n",
            "[TOP] exp_accum_input[9] = 18612\n",
            "[TOP] x_divider = 10928\n",
            "[TOP] y_dividend = 1163\n",
            "[TOP] data_out[9] = 873\n",
            "[TOP] exp_fraction_output[9] = 24270\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5812592190\n",
            "[TOP] exp_trunc[9] = 354772\n",
            "[TOP] exp_accum_input[9] = 22173\n",
            "[TOP] x_divider = 11951\n",
            "[TOP] y_dividend = 1385\n",
            "[TOP] data_out[9] = 949\n",
            "[TOP] exp_fraction_output[9] = 18605\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455841685\n",
            "[TOP] exp_trunc[9] = 271962\n",
            "[TOP] exp_accum_input[9] = 16997\n",
            "[TOP] x_divider = 11656\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 20502\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4910167494\n",
            "[TOP] exp_trunc[9] = 299692\n",
            "[TOP] exp_accum_input[9] = 18730\n",
            "[TOP] x_divider = 11903\n",
            "[TOP] y_dividend = 1170\n",
            "[TOP] data_out[9] = 803\n",
            "[TOP] exp_fraction_output[9] = 17309\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4145453573\n",
            "[TOP] exp_trunc[9] = 253018\n",
            "[TOP] exp_accum_input[9] = 15813\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 988\n",
            "[TOP] data_out[9] = 695\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 10881\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 20733\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4965491301\n",
            "[TOP] exp_trunc[9] = 303069\n",
            "[TOP] exp_accum_input[9] = 18941\n",
            "[TOP] x_divider = 11822\n",
            "[TOP] y_dividend = 1183\n",
            "[TOP] data_out[9] = 819\n",
            "[TOP] exp_fraction_output[9] = 17456\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4180659632\n",
            "[TOP] exp_trunc[9] = 255167\n",
            "[TOP] exp_accum_input[9] = 15947\n",
            "[TOP] x_divider = 11571\n",
            "[TOP] y_dividend = 996\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 18357\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396446429\n",
            "[TOP] exp_trunc[9] = 268337\n",
            "[TOP] exp_accum_input[9] = 16771\n",
            "[TOP] x_divider = 10651\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 17666\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4230954002\n",
            "[TOP] exp_trunc[9] = 258236\n",
            "[TOP] exp_accum_input[9] = 16139\n",
            "[TOP] x_divider = 11657\n",
            "[TOP] y_dividend = 1008\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 21689\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5194450433\n",
            "[TOP] exp_trunc[9] = 317044\n",
            "[TOP] exp_accum_input[9] = 19815\n",
            "[TOP] x_divider = 12828\n",
            "[TOP] y_dividend = 1238\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 19543\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4680489871\n",
            "[TOP] exp_trunc[9] = 285674\n",
            "[TOP] exp_accum_input[9] = 17854\n",
            "[TOP] x_divider = 11206\n",
            "[TOP] y_dividend = 1115\n",
            "[TOP] data_out[9] = 815\n",
            "[TOP] exp_fraction_output[9] = 18359\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4396925423\n",
            "[TOP] exp_trunc[9] = 268367\n",
            "[TOP] exp_accum_input[9] = 16772\n",
            "[TOP] x_divider = 10786\n",
            "[TOP] y_dividend = 1048\n",
            "[TOP] data_out[9] = 797\n",
            "[TOP] exp_fraction_output[9] = 28572\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6842908284\n",
            "[TOP] exp_trunc[9] = 417657\n",
            "[TOP] exp_accum_input[9] = 26103\n",
            "[TOP] x_divider = 14967\n",
            "[TOP] y_dividend = 1631\n",
            "[TOP] data_out[9] = 893\n",
            "[TOP] exp_fraction_output[9] = 24567\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5883722799\n",
            "[TOP] exp_trunc[9] = 359113\n",
            "[TOP] exp_accum_input[9] = 22444\n",
            "[TOP] x_divider = 15260\n",
            "[TOP] y_dividend = 1402\n",
            "[TOP] data_out[9] = 753\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11047\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 723\n",
            "[TOP] exp_fraction_output[9] = 17301\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4143537597\n",
            "[TOP] exp_trunc[9] = 252901\n",
            "[TOP] exp_accum_input[9] = 15806\n",
            "[TOP] x_divider = 12146\n",
            "[TOP] y_dividend = 987\n",
            "[TOP] data_out[9] = 665\n",
            "[TOP] exp_fraction_output[9] = 17383\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4163176351\n",
            "[TOP] exp_trunc[9] = 254100\n",
            "[TOP] exp_accum_input[9] = 15881\n",
            "[TOP] x_divider = 12600\n",
            "[TOP] y_dividend = 992\n",
            "[TOP] data_out[9] = 645\n",
            "[TOP] exp_fraction_output[9] = 25924\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 6208720228\n",
            "[TOP] exp_trunc[9] = 378950\n",
            "[TOP] exp_accum_input[9] = 23684\n",
            "[TOP] x_divider = 16238\n",
            "[TOP] y_dividend = 1480\n",
            "[TOP] data_out[9] = 747\n",
            "[TOP] exp_fraction_output[9] = 19062\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4565291814\n",
            "[TOP] exp_trunc[9] = 278643\n",
            "[TOP] exp_accum_input[9] = 17415\n",
            "[TOP] x_divider = 11347\n",
            "[TOP] y_dividend = 1088\n",
            "[TOP] data_out[9] = 785\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11019\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 727\n",
            "[TOP] exp_fraction_output[9] = 20415\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4889331255\n",
            "[TOP] exp_trunc[9] = 298421\n",
            "[TOP] exp_accum_input[9] = 18651\n",
            "[TOP] x_divider = 11077\n",
            "[TOP] y_dividend = 1165\n",
            "[TOP] data_out[9] = 861\n",
            "[TOP] exp_fraction_output[9] = 18636\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4463266092\n",
            "[TOP] exp_trunc[9] = 272416\n",
            "[TOP] exp_accum_input[9] = 17026\n",
            "[TOP] x_divider = 12609\n",
            "[TOP] y_dividend = 1064\n",
            "[TOP] data_out[9] = 691\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11264\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 711\n",
            "[TOP] exp_fraction_output[9] = 19370\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4639056890\n",
            "[TOP] exp_trunc[9] = 283145\n",
            "[TOP] exp_accum_input[9] = 17696\n",
            "[TOP] x_divider = 11443\n",
            "[TOP] y_dividend = 1106\n",
            "[TOP] data_out[9] = 791\n",
            "[TOP] exp_fraction_output[9] = 18018\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4315256946\n",
            "[TOP] exp_trunc[9] = 263382\n",
            "[TOP] exp_accum_input[9] = 16461\n",
            "[TOP] x_divider = 11415\n",
            "[TOP] y_dividend = 1028\n",
            "[TOP] data_out[9] = 737\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12660\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 633\n",
            "[TOP] exp_fraction_output[9] = 19701\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4718330397\n",
            "[TOP] exp_trunc[9] = 287984\n",
            "[TOP] exp_accum_input[9] = 17999\n",
            "[TOP] x_divider = 10867\n",
            "[TOP] y_dividend = 1124\n",
            "[TOP] data_out[9] = 847\n",
            "[TOP] exp_fraction_output[9] = 21765\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5212652205\n",
            "[TOP] exp_trunc[9] = 318155\n",
            "[TOP] exp_accum_input[9] = 19884\n",
            "[TOP] x_divider = 13401\n",
            "[TOP] y_dividend = 1242\n",
            "[TOP] data_out[9] = 759\n",
            "[TOP] exp_fraction_output[9] = 22248\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5328329256\n",
            "[TOP] exp_trunc[9] = 325215\n",
            "[TOP] exp_accum_input[9] = 20325\n",
            "[TOP] x_divider = 15274\n",
            "[TOP] y_dividend = 1270\n",
            "[TOP] data_out[9] = 681\n",
            "[TOP] exp_fraction_output[9] = 20653\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4946331541\n",
            "[TOP] exp_trunc[9] = 301900\n",
            "[TOP] exp_accum_input[9] = 18868\n",
            "[TOP] x_divider = 14548\n",
            "[TOP] y_dividend = 1179\n",
            "[TOP] data_out[9] = 663\n",
            "[TOP] exp_fraction_output[9] = 17236\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4127970292\n",
            "[TOP] exp_trunc[9] = 251951\n",
            "[TOP] exp_accum_input[9] = 15746\n",
            "[TOP] x_divider = 12702\n",
            "[TOP] y_dividend = 984\n",
            "[TOP] data_out[9] = 635\n",
            "[TOP] exp_fraction_output[9] = 20799\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4981298103\n",
            "[TOP] exp_trunc[9] = 304034\n",
            "[TOP] exp_accum_input[9] = 19002\n",
            "[TOP] x_divider = 13789\n",
            "[TOP] y_dividend = 1187\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19506\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4671628482\n",
            "[TOP] exp_trunc[9] = 285133\n",
            "[TOP] exp_accum_input[9] = 17820\n",
            "[TOP] x_divider = 11303\n",
            "[TOP] y_dividend = 1113\n",
            "[TOP] data_out[9] = 805\n",
            "[TOP] exp_fraction_output[9] = 22068\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5285219796\n",
            "[TOP] exp_trunc[9] = 322584\n",
            "[TOP] exp_accum_input[9] = 20161\n",
            "[TOP] x_divider = 14163\n",
            "[TOP] y_dividend = 1260\n",
            "[TOP] data_out[9] = 729\n",
            "[TOP] exp_fraction_output[9] = 19646\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4705158062\n",
            "[TOP] exp_trunc[9] = 287180\n",
            "[TOP] exp_accum_input[9] = 17948\n",
            "[TOP] x_divider = 14401\n",
            "[TOP] y_dividend = 1121\n",
            "[TOP] data_out[9] = 637\n",
            "[TOP] exp_fraction_output[9] = 24771\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5932580187\n",
            "[TOP] exp_trunc[9] = 362095\n",
            "[TOP] exp_accum_input[9] = 22630\n",
            "[TOP] x_divider = 18996\n",
            "[TOP] y_dividend = 1414\n",
            "[TOP] data_out[9] = 609\n",
            "[TOP] exp_fraction_output[9] = 17149\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4107134053\n",
            "[TOP] exp_trunc[9] = 250679\n",
            "[TOP] exp_accum_input[9] = 15667\n",
            "[TOP] x_divider = 12837\n",
            "[TOP] y_dividend = 979\n",
            "[TOP] data_out[9] = 623\n",
            "[TOP] exp_fraction_output[9] = 29382\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 7036900854\n",
            "[TOP] exp_trunc[9] = 429498\n",
            "[TOP] exp_accum_input[9] = 26843\n",
            "[TOP] x_divider = 20285\n",
            "[TOP] y_dividend = 1677\n",
            "[TOP] data_out[9] = 677\n",
            "[TOP] exp_fraction_output[9] = 20694\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4956150918\n",
            "[TOP] exp_trunc[9] = 302499\n",
            "[TOP] exp_accum_input[9] = 18906\n",
            "[TOP] x_divider = 14997\n",
            "[TOP] y_dividend = 1181\n",
            "[TOP] data_out[9] = 643\n",
            "[TOP] exp_fraction_output[9] = 19662\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4708990014\n",
            "[TOP] exp_trunc[9] = 287413\n",
            "[TOP] exp_accum_input[9] = 17963\n",
            "[TOP] x_divider = 15698\n",
            "[TOP] y_dividend = 1122\n",
            "[TOP] data_out[9] = 585\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 12362\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 647\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 14012\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 13550\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 591\n",
            "[TOP] exp_fraction_output[9] = 21167\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5069432999\n",
            "[TOP] exp_trunc[9] = 309413\n",
            "[TOP] exp_accum_input[9] = 19338\n",
            "[TOP] x_divider = 14782\n",
            "[TOP] y_dividend = 1208\n",
            "[TOP] data_out[9] = 669\n",
            "[TOP] exp_fraction_output[9] = 17787\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4259933139\n",
            "[TOP] exp_trunc[9] = 260005\n",
            "[TOP] exp_accum_input[9] = 16250\n",
            "[TOP] x_divider = 14554\n",
            "[TOP] y_dividend = 1015\n",
            "[TOP] data_out[9] = 571\n",
            "[TOP] exp_fraction_output[9] = 20968\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5021773096\n",
            "[TOP] exp_trunc[9] = 306504\n",
            "[TOP] exp_accum_input[9] = 19156\n",
            "[TOP] x_divider = 14328\n",
            "[TOP] y_dividend = 1197\n",
            "[TOP] data_out[9] = 685\n",
            "[TOP] exp_fraction_output[9] = 20668\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4949923996\n",
            "[TOP] exp_trunc[9] = 302119\n",
            "[TOP] exp_accum_input[9] = 18882\n",
            "[TOP] x_divider = 16874\n",
            "[TOP] y_dividend = 1180\n",
            "[TOP] data_out[9] = 573\n",
            "[TOP] exp_fraction_output[9] = 18603\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4455362691\n",
            "[TOP] exp_trunc[9] = 271933\n",
            "[TOP] exp_accum_input[9] = 16995\n",
            "[TOP] x_divider = 13297\n",
            "[TOP] y_dividend = 1062\n",
            "[TOP] data_out[9] = 653\n",
            "[TOP] exp_fraction_output[9] = 18579\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4449614763\n",
            "[TOP] exp_trunc[9] = 271582\n",
            "[TOP] exp_accum_input[9] = 16973\n",
            "[TOP] x_divider = 10571\n",
            "[TOP] y_dividend = 1060\n",
            "[TOP] data_out[9] = 821\n",
            "[TOP] exp_fraction_output[9] = 17858\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4276937426\n",
            "[TOP] exp_trunc[9] = 261043\n",
            "[TOP] exp_accum_input[9] = 16315\n",
            "[TOP] x_divider = 10815\n",
            "[TOP] y_dividend = 1019\n",
            "[TOP] data_out[9] = 769\n",
            "[TOP] exp_fraction_output[9] = 23562\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5643028314\n",
            "[TOP] exp_trunc[9] = 344423\n",
            "[TOP] exp_accum_input[9] = 21526\n",
            "[TOP] x_divider = 12919\n",
            "[TOP] y_dividend = 1345\n",
            "[TOP] data_out[9] = 853\n",
            "[TOP] exp_fraction_output[9] = 18064\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4326273808\n",
            "[TOP] exp_trunc[9] = 264054\n",
            "[TOP] exp_accum_input[9] = 16503\n",
            "[TOP] x_divider = 10774\n",
            "[TOP] y_dividend = 1031\n",
            "[TOP] data_out[9] = 783\n",
            "[TOP] exp_fraction_output[9] = 19030\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4557627910\n",
            "[TOP] exp_trunc[9] = 278175\n",
            "[TOP] exp_accum_input[9] = 17385\n",
            "[TOP] x_divider = 12583\n",
            "[TOP] y_dividend = 1086\n",
            "[TOP] data_out[9] = 705\n",
            "[TOP] exp_fraction_output[9] = 19412\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4649115764\n",
            "[TOP] exp_trunc[9] = 283759\n",
            "[TOP] exp_accum_input[9] = 17734\n",
            "[TOP] x_divider = 10685\n",
            "[TOP] y_dividend = 1108\n",
            "[TOP] data_out[9] = 849\n",
            "[TOP] exp_fraction_output[9] = 22334\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5348925998\n",
            "[TOP] exp_trunc[9] = 326472\n",
            "[TOP] exp_accum_input[9] = 20404\n",
            "[TOP] x_divider = 10855\n",
            "[TOP] y_dividend = 1275\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17145\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4106176065\n",
            "[TOP] exp_trunc[9] = 250621\n",
            "[TOP] exp_accum_input[9] = 15663\n",
            "[TOP] x_divider = 11324\n",
            "[TOP] y_dividend = 978\n",
            "[TOP] data_out[9] = 707\n",
            "[TOP] exp_fraction_output[9] = 24494\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 5866239518\n",
            "[TOP] exp_trunc[9] = 358046\n",
            "[TOP] exp_accum_input[9] = 22377\n",
            "[TOP] x_divider = 11899\n",
            "[TOP] y_dividend = 1398\n",
            "[TOP] data_out[9] = 961\n",
            "[TOP] exp_fraction_output[9] = 17626\n",
            "[TOP] exp_int_output[9] = 239497\n",
            "[TOP] exp_whole[9] = 4221374122\n",
            "[TOP] exp_trunc[9] = 257652\n",
            "[TOP] exp_accum_input[9] = 16103\n",
            "[TOP] x_divider = 11279\n",
            "[TOP] y_dividend = 1006\n",
            "[TOP] data_out[9] = 729\n",
            "입력 문장: This is a grammatically acceptable for sentence.\n",
            "예측된 클래스: 0\n",
            "예측 확률: [[0.56147647 0.43852356]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 encoder 내 각 레이어의 self-attention 모듈 타입을 출력하여\n",
        "# 수정된 BertSelfAttentionModified가 적용되었는지 확인합니다.\n",
        "print(\"수정된 Attention Layers 확인:\")\n",
        "for i, layer in enumerate(model.bert.encoder.layer):\n",
        "    attn_layer = layer.attention.self\n",
        "    print(f\"Layer {i} self-attention layer type: {type(attn_layer)}\")\n"
      ],
      "metadata": {
        "id": "8WUtGKLuoW2z",
        "outputId": "5fbb725e-8939-4578-ea68-dc042ec45fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수정된 Attention Layers 확인:\n",
            "Layer 0 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 1 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 2 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 3 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 4 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 5 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 6 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 7 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 8 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 9 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 10 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n",
            "Layer 11 self-attention layer type: <class '__main__.BertSelfAttentionModified'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 디버깅용 잡동사니코드"
      ],
      "metadata": {
        "id": "H5iP5hhyJuf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\"[TOP] exp_fraction_output[{i}] = {exp_fraction_output[i]}\")\n",
        "    print(f\"[TOP] exp_int_output[{i}] = {exp_int_output[i]}\")\n",
        "    print(f\"[TOP] exp_whole[{i}] = {exp_whole[i]}\")\n",
        "    print(f\"[TOP] exp_trunc[{i}] = {exp_trunc[i]}\")\n",
        "    print(f\"[TOP] exp_accum_input[{i}] = {exp_accum_input[i]}\")\n",
        "    print(f\"[TOP] x_divider = {x_divider[0]}\")\n",
        "    print(f\"[TOP] y_dividend = {y_dividend[i]}\")\n",
        "    print(f\"[TOP] data_out[{i}] = {data_out[i]}\")"
      ],
      "metadata": {
        "id": "rAamcoXWSjnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # ✅ 디버깅용 Q, K 체크\n",
        "        with torch.no_grad():\n",
        "            h = hidden_states.detach().cpu().numpy()\n",
        "            print(\"=== Hidden States 디버깅 ===\")\n",
        "            print(\"min:\", np.min(h), \"max:\", np.max(h))\n",
        "            print(\"NaN 수:\", np.isnan(h).sum(), \"Inf 수:\", np.isinf(h).sum())\n",
        "            q = query_layer.detach().cpu().numpy()\n",
        "            k = key_layer.detach().cpu().numpy()\n",
        "            print(\"=== Q, K 디버깅 ===\")\n",
        "            print(\"Q min/max:\", np.min(q), np.max(q))\n",
        "            print(\"K min/max:\", np.min(k), np.max(k))\n",
        "            print(\"Q NaN 수:\", np.isnan(q).sum(), \"K NaN 수:\", np.isnan(k).sum())\n",
        "            print(\"Q Inf 수:\", np.isinf(q).sum(), \"K Inf 수:\", np.isinf(k).sum())\n",
        "\n",
        "        # attention score 계산 후 clamp\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 계산 직후 ===\")\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item(), \"Inf 수:\", torch.isinf(attention_scores).sum().item())\n",
        "        attention_scores = torch.clamp(attention_scores, min=-10.0, max=10.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            print(\"=== [DEBUG] attention_scores 예시 ===\")\n",
        "            print(attention_scores[0, 0, 0, :10])\n",
        "            print(\"min:\", attention_scores.min().item(), \"max:\", attention_scores.max().item())\n",
        "            print(\"NaN 수:\", torch.isnan(attention_scores).sum().item())\n",
        "        # CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\n",
        "        cordic_attention = top_1200_input(attention_scores)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "\n",
        "# 2. BertLayerWithNaNCheck: layer 0 내부 모듈 NaN 추적\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n",
        "\n",
        "\n",
        "# 3. BertEncoderModified\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "        self.layer[0] = BertLayerWithNaNCheck(self.layer[0])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_values=None, use_cache=False, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=True):\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            with torch.no_grad():\n",
        "                hs = hidden_states.detach().cpu().numpy()\n",
        "                if np.isnan(hs).sum() > 0:\n",
        "                    print(f\"[NaN DETECTED] ❌ in hidden_states BEFORE layer {i}\")\n",
        "                else:\n",
        "                    print(f\"[OK] ✅ hidden_states BEFORE layer {i}\")\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                head_mask[i] if head_mask is not None else None,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_values[i] if past_key_values is not None else None,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        return (hidden_states,)\n",
        "\n",
        "\n",
        "# 4. BertWithModifiedAttentionForClassification\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 5. Load config and model\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "dTD-BGlHKAh5",
        "outputId": "383af56c-e173-4f09-db40-610c833f616d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayerWithNaNCheck(\n",
              "          (layer): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttentionModified(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-11): 11 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def predict_sentence(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_class = np.argmax(probs)\n",
        "\n",
        "    return pred_class, probs\n"
      ],
      "metadata": {
        "id": "sgc6czs9DqMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_nan(module, input, output):\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        if torch.isnan(output).any():\n",
        "            print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "    elif isinstance(output, (tuple, list)):\n",
        "        for o in output:\n",
        "            if torch.is_tensor(o) and torch.isnan(o).any():\n",
        "                print(f\"[NaN DETECTED] in {module.__class__.__name__}\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    module.register_forward_hook(check_for_nan)\n"
      ],
      "metadata": {
        "id": "2QHFLakIFV8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class BertLayerWithNaNCheck(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        print(\"=== [Layer 0] BEFORE ===\")\n",
        "        print(\"min:\", hidden_states.min().item(), \"max:\", hidden_states.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(hidden_states).sum().item(), \"Inf 수:\", torch.isinf(hidden_states).sum().item())\n",
        "\n",
        "        # Attention\n",
        "        hidden_states, *_ = self.layer.attention(hidden_states, *args, **kwargs)\n",
        "        if torch.isnan(hidden_states).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Attention in Layer 0\")\n",
        "\n",
        "        # Intermediate\n",
        "        intermediate_output = self.layer.intermediate(hidden_states)\n",
        "        if torch.isnan(intermediate_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Intermediate in Layer 0\")\n",
        "\n",
        "        # Output\n",
        "        layer_output = self.layer.output(intermediate_output, hidden_states)\n",
        "        if torch.isnan(layer_output).any():\n",
        "            print(\"[NaN DETECTED] ❌ after Output in Layer 0\")\n",
        "\n",
        "        print(\"=== [Layer 0] AFTER ===\")\n",
        "        print(\"min:\", layer_output.min().item(), \"max:\", layer_output.max().item())\n",
        "        print(\"NaN 수:\", torch.isnan(layer_output).sum().item(), \"Inf 수:\", torch.isinf(layer_output).sum().item())\n",
        "\n",
        "        return (layer_output,)\n"
      ],
      "metadata": {
        "id": "gAA7SyorIaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentence = \"The cat is sitting on the mat.\"\n",
        "pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "\n",
        "print(\"문장:\", sentence)\n",
        "print(\"예측 클래스:\", pred_class)\n",
        "print(\"클래스별 확률:\", pred_probs)\n"
      ],
      "metadata": {
        "id": "yv0VeRUlDpLp",
        "outputId": "1cd74ea5-ca18-4524-d57c-bed12d1b5f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] ✅ hidden_states BEFORE layer 0\n",
            "=== [Layer 0] BEFORE ===\n",
            "min: -4.571415901184082 max: 3.8990025520324707\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Hidden States 디버깅 ===\n",
            "min: -4.571416 max: 3.8990026\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: -2.1867194 2.275151\n",
            "K min/max: -2.4105966 2.2897592\n",
            "Q NaN 수: 0 K NaN 수: 0\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([ 0.4173, -0.1602, -0.0771,  0.3595,  0.1112, -0.1229, -0.2256, -0.0283,\n",
            "        -0.1147, -0.0984], device='cuda:0')\n",
            "min: -1.3686672449111938 max: 1.3171285390853882\n",
            "NaN 수: 0\n",
            "[NaN DETECTED] ❌ after Attention in Layer 0\n",
            "[NaN DETECTED] ❌ after Intermediate in Layer 0\n",
            "[NaN DETECTED] ❌ after Output in Layer 0\n",
            "=== [Layer 0] AFTER ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "[NaN DETECTED] ❌ in hidden_states BEFORE layer 1\n",
            "=== Hidden States 디버깅 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== Q, K 디버깅 ===\n",
            "Q min/max: nan nan\n",
            "K min/max: nan nan\n",
            "Q NaN 수: 49152 K NaN 수: 49152\n",
            "Q Inf 수: 0 K Inf 수: 0\n",
            "=== [DEBUG] attention_scores 계산 직후 ===\n",
            "min: nan max: nan\n",
            "NaN 수: 49152 Inf 수: 0\n",
            "=== [DEBUG] attention_scores 예시 ===\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
            "min: nan max: nan\n",
            "NaN 수: 49152\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "float_to_fixed_point() - NaN 또는 무한대 값 감지: nan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-8b7cffed0231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The cat is sitting on the mat.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"문장:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-c78350183b4c>\u001b[0m in \u001b[0;36mpredict_sentence\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[OK] ✅ hidden_states BEFORE layer {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-79756fe349fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN 수:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# CORDIC 방식으로 softmax 대체 (임시로 softmax로 출력 확인)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcordic_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_1200_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop_1200_input\u001b[0;34m(attention_scores)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mresult_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#20bit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdata_22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-584fdc9bac46>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# ✅ NaN 또는 Inf 값 체크 후 예외 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# ✅ **최대/최소 값 제한 (7비트 표현 범위)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: float_to_fixed_point() - NaN 또는 무한대 값 감지: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.modeling_bert import BertSelfAttention, BertEncoder, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# 1. BertSelfAttentionModified: 기존 softmax 대신 CORDIC 연산을 적용\n",
        "class BertSelfAttentionModified(BertSelfAttention):\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
        "                encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        batch_size, num_heads, seq_len, _ = attention_scores.shape\n",
        "        for b in range(batch_size):\n",
        "          for h in range(num_heads):\n",
        "            for row in range(seq_len):\n",
        "              float_row = attention_scores[b, h, row, :10].detach().cpu().numpy().tolist()\n",
        "\n",
        "              if any(np.isnan(f) for f in float_row):\n",
        "                print(f\"[NaN] b={b}, head={h}, row={row} - 입력에 NaN 있음! ❌\")\n",
        "                continue  # 이건 skip하고 다음으로\n",
        "              try:\n",
        "                cordic_attention = top(*float_row)\n",
        "              except Exception as e:\n",
        "                print(f\"[ERROR] top() 실패: {e}\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          sample_row = attention_scores[0, 0, 0, :10]  # [10개 float]\n",
        "          float_row = sample_row.detach().cpu().numpy().tolist()\n",
        "          print(\"[DEBUG] top 입력값:\", float_row)\n",
        "\n",
        "          try:\n",
        "            top_result = top(*float_row)\n",
        "            print(\"[DEBUG] top 출력값:\", top_result)\n",
        "          except Exception as e:\n",
        "            print(\"[ERROR] top에서 예외 발생:\", e)\n",
        "        # CORDIC 방식으로 softmax 대체\n",
        "        cordic_attention = top(*float_row)\n",
        "\n",
        "        if any(np.isnan(c) for c in cordic_attention):\n",
        "          print(f\"[NaN DETECTED] ❌ top() 결과에 NaN 존재! input: {float_row}\")\n",
        "\n",
        "        attention_probs = torch.as_tensor(cordic_attention, dtype=torch.float32, device=hidden_states.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_probs = attention_probs + attention_mask\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # ⚠️ 먼저 너의 원본 문장 리스트가 있어야 해!\n",
        "# 예: dataset_sentences = [\"문장1\", \"문장2\", ..., \"문장N\"]\n",
        "        # 🧪 validation_dataloader에서 원문 문장 추출 (예시)\n",
        "        dataset_sentences = []\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "          input_ids = batch[0]\n",
        "          for ids in input_ids:\n",
        "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            dataset_sentences.append(text)\n",
        "\n",
        "        for i, sentence in enumerate(dataset_sentences):\n",
        "          try:\n",
        "            pred_class, pred_probs = predict_sentence(model, tokenizer, sentence)\n",
        "          except Exception as e:\n",
        "            print(f\"[❌ NaN 발생] 문장 index = {i}\")\n",
        "            print(\"문장 내용:\", sentence)\n",
        "            print(\"에러 메시지:\", e)\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs, attention_scores)\n",
        "        else:\n",
        "            return (context_layer,)\n",
        "\n",
        "# 2. BertEncoderModified: 모든 레이어의 self-attention을 수정된 클래스로 교체\n",
        "class BertEncoderModified(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i in range(config.num_hidden_layers):\n",
        "            self.layer[i].attention.self = BertSelfAttentionModified(config)\n",
        "\n",
        "# 3. BertWithModifiedAttentionForClassification: 최종 모델에서 커스텀 encoder 사용\n",
        "class BertWithModifiedAttentionForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        config.num_labels = 2  # CoLA는 이진 분류 문제\n",
        "        super().__init__(config)\n",
        "        self.bert.encoder = BertEncoderModified(config)\n",
        "\n",
        "# 4. 모델 생성 및 학습/추론\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertWithModifiedAttentionForClassification(config)\n",
        "model.cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "_dPquKTIGbOE",
        "outputId": "54615d57-7130-49f1-b469-d5b229ca4e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertWithModifiedAttentionForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoderModified(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionModified(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 적용 모델 validation"
      ],
      "metadata": {
        "id": "JSRL23_d7voR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "metadata": {
        "id": "h4m08g6HohMF",
        "outputId": "1644ccf7-52ee-466d-92bd-1fbd4b207b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "float_to_fixed_point() - NaN 또는 무한대 값 감지: nan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e22c35b96d8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# The documentation for this `model` function is here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         outputs = model(b_input_ids,\n\u001b[0m\u001b[1;32m     41\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         attention_mask=b_input_mask)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6e3b41572d66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# CORDIC 방식으로 softmax 대체\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcordic_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_1200_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcordic_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mtop_1200_input\u001b[0;34m(attention_scores)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mresult_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mtop\u001b[0;34m(data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#20bit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdata_22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mdata_33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_to_fixed_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-584fdc9bac46>\u001b[0m in \u001b[0;36mfloat_to_fixed_point\u001b[0;34m(value, int_bits, frac_bits)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# ✅ NaN 또는 Inf 값 체크 후 예외 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"float_to_fixed_point() - NaN 또는 무한대 값 감지: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# ✅ **최대/최소 값 제한 (7비트 표현 범위)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: float_to_fixed_point() - NaN 또는 무한대 값 감지: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 저장 및 로드 코드.\n",
        "path = '/content/model/'\n",
        "torch.save(model.state_dict(), path+\"CoLA.pt\")\n",
        "model.load_state_dict(torch.load(path+\"CoLA.pt\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6f3ddQs0x7V-",
        "outputId": "eeb472a5-b074-4bbe-a958-e6eafda42707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}